{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nLzqlc0wOXCO",
    "outputId": "d14d769a-ce59-4f88-9fae-cd80d37e806c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: scikit-learn\n",
      "Version: 1.6.0\n",
      "Summary: A set of python modules for machine learning and data mining\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: BSD 3-Clause License\n",
      "         \n",
      "         Copyright (c) 2007-2024 The scikit-learn developers.\n",
      "         All rights reserved.\n",
      "         \n",
      "         Redistribution and use in source and binary forms, with or without\n",
      "         modification, are permitted provided that the following conditions are met:\n",
      "         \n",
      "         * Redistributions of source code must retain the above copyright notice, this\n",
      "           list of conditions and the following disclaimer.\n",
      "         \n",
      "         * Redistributions in binary form must reproduce the above copyright notice,\n",
      "           this list of conditions and the following disclaimer in the documentation\n",
      "           and/or other materials provided with the distribution.\n",
      "         \n",
      "         * Neither the name of the copyright holder nor the names of its\n",
      "           contributors may be used to endorse or promote products derived from\n",
      "           this software without specific prior written permission.\n",
      "         \n",
      "         THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      "         AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      "         IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      "         DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      "         FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      "         DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      "         SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      "         CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      "         OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "         OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "         \n",
      "         ----\n",
      "         \n",
      "         This binary distribution of scikit-learn also bundles the following software:\n",
      "         \n",
      "         ----\n",
      "         \n",
      "         Name: Microsoft Visual C++ Runtime Files\n",
      "         Files: sklearn\\.libs\\*.dll\n",
      "         Availability: https://learn.microsoft.com/en-us/visualstudio/releases/2015/2015-redistribution-vs\n",
      "         \n",
      "         Subject to the License Terms for the software, you may copy and distribute with your\n",
      "         program any of the files within the followng folder and its subfolders except as noted\n",
      "         below. You may not modify these files.\n",
      "         \n",
      "         C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\redist\n",
      "         \n",
      "         You may not distribute the contents of the following folders:\n",
      "         \n",
      "         C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\redist\\debug_nonredist\n",
      "         C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\redist\\onecore\\debug_nonredist\n",
      "         \n",
      "         Subject to the License Terms for the software, you may copy and distribute the following\n",
      "         files with your program in your program’s application local folder or by deploying them\n",
      "         into the Global Assembly Cache (GAC):\n",
      "         \n",
      "         VC\\atlmfc\\lib\\mfcmifc80.dll\n",
      "         VC\\atlmfc\\lib\\amd64\\mfcmifc80.dll\n",
      "         \n",
      "Location: c:\\users\\vedashree\\anaconda3\\lib\\site-packages\n",
      "Requires: joblib, numpy, scipy, threadpoolctl\n",
      "Required-by: scikit-learn-intelex\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uvkYoMYFO0tQ"
   },
   "outputs": [],
   "source": [
    "# import opendatasets as od\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cqJ8YTHUPFzl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vedashree\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aYdfiMe6PMdm",
    "outputId": "eaf20941-b5bf-477e-8e35-e22693152e3d"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'od' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19476\\3756406337.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m od.download(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \"https://www.kaggle.com/datasets/spsayakpaul/arxiv-paper-abstracts/data\")\n",
      "\u001b[1;31mNameError\u001b[0m: name 'od' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "od.download(\n",
    "    \"https://www.kaggle.com/datasets/spsayakpaul/arxiv-paper-abstracts/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "M1knbyG9PfRV"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C://Users//Vedashree//Downloads//arxiv_data//data2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "id": "O-V7onNIQDrW",
    "outputId": "1afe6ade-9fb5-4177-a095-85322773edb7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Multi-Level Attention Pooling for Graph Neural...</td>\n",
       "      <td>Graph neural networks (GNNs) have been widely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['cs.LG', 'cs.AI']</td>\n",
       "      <td>Decision Forests vs. Deep Networks: Conceptual...</td>\n",
       "      <td>Deep networks and decision forests (such as ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['cs.LG', 'cs.CR', 'stat.ML']</td>\n",
       "      <td>Power up! Robust Graph Convolutional Network v...</td>\n",
       "      <td>Graph convolutional networks (GCNs) are powerf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['cs.LG', 'cs.CR']</td>\n",
       "      <td>Releasing Graph Neural Networks with Different...</td>\n",
       "      <td>With the increasing popularity of Graph Neural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Recurrence-Aware Long-Term Cognitive Network f...</td>\n",
       "      <td>Machine learning solutions for pattern classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56176</th>\n",
       "      <td>['cs.CV', 'cs.IR']</td>\n",
       "      <td>Mining Spatio-temporal Data on Industrializati...</td>\n",
       "      <td>Despite the growing availability of big data i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56177</th>\n",
       "      <td>['cs.LG', 'cs.AI', 'cs.CL', 'I.2.6; I.2.7']</td>\n",
       "      <td>Wav2Letter: an End-to-End ConvNet-based Speech...</td>\n",
       "      <td>This paper presents a simple end-to-end model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56178</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Deep Reinforcement Learning with Double Q-lear...</td>\n",
       "      <td>The popular Q-learning algorithm is known to o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56179</th>\n",
       "      <td>['stat.ML', 'cs.LG', 'math.OC']</td>\n",
       "      <td>Generalized Low Rank Models</td>\n",
       "      <td>Principal components analysis (PCA) is a well-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56180</th>\n",
       "      <td>['cs.LG', 'cs.AI', 'stat.ML']</td>\n",
       "      <td>Chi-square Tests Driven Method for Learning th...</td>\n",
       "      <td>SDYNA is a general framework designed to addre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56181 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             terms  \\\n",
       "0                                        ['cs.LG']   \n",
       "1                               ['cs.LG', 'cs.AI']   \n",
       "2                    ['cs.LG', 'cs.CR', 'stat.ML']   \n",
       "3                               ['cs.LG', 'cs.CR']   \n",
       "4                                        ['cs.LG']   \n",
       "...                                            ...   \n",
       "56176                           ['cs.CV', 'cs.IR']   \n",
       "56177  ['cs.LG', 'cs.AI', 'cs.CL', 'I.2.6; I.2.7']   \n",
       "56178                                    ['cs.LG']   \n",
       "56179              ['stat.ML', 'cs.LG', 'math.OC']   \n",
       "56180                ['cs.LG', 'cs.AI', 'stat.ML']   \n",
       "\n",
       "                                                  titles  \\\n",
       "0      Multi-Level Attention Pooling for Graph Neural...   \n",
       "1      Decision Forests vs. Deep Networks: Conceptual...   \n",
       "2      Power up! Robust Graph Convolutional Network v...   \n",
       "3      Releasing Graph Neural Networks with Different...   \n",
       "4      Recurrence-Aware Long-Term Cognitive Network f...   \n",
       "...                                                  ...   \n",
       "56176  Mining Spatio-temporal Data on Industrializati...   \n",
       "56177  Wav2Letter: an End-to-End ConvNet-based Speech...   \n",
       "56178  Deep Reinforcement Learning with Double Q-lear...   \n",
       "56179                        Generalized Low Rank Models   \n",
       "56180  Chi-square Tests Driven Method for Learning th...   \n",
       "\n",
       "                                               abstracts  \n",
       "0      Graph neural networks (GNNs) have been widely ...  \n",
       "1      Deep networks and decision forests (such as ra...  \n",
       "2      Graph convolutional networks (GCNs) are powerf...  \n",
       "3      With the increasing popularity of Graph Neural...  \n",
       "4      Machine learning solutions for pattern classif...  \n",
       "...                                                  ...  \n",
       "56176  Despite the growing availability of big data i...  \n",
       "56177  This paper presents a simple end-to-end model ...  \n",
       "56178  The popular Q-learning algorithm is known to o...  \n",
       "56179  Principal components analysis (PCA) is a well-...  \n",
       "56180  SDYNA is a general framework designed to addre...  \n",
       "\n",
       "[56181 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ik-QBnQB3ctZ"
   },
   "outputs": [],
   "source": [
    "# Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Cwed4GEZ3cqH"
   },
   "outputs": [],
   "source": [
    "df2=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "ArQyYd_j3cng",
    "outputId": "af3bd537-e7ac-4488-db45-91378b06639c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "terms        0\n",
       "titles       0\n",
       "abstracts    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8hKn-EFK3ck4",
    "outputId": "69d1e337-00d1-4295-a706-5cd4456854fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15054"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XqKn6WCyM8H7"
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Wp2LCWYR3ciK"
   },
   "outputs": [],
   "source": [
    "labels_cols=df2.terms.apply(literal_eval)\n",
    "labels=labels_cols.explode().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-T1WvNKR3cRa",
    "outputId": "adab35df-c92f-4890-a9e8-2d2c5003a0d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:  ['cs.LG' 'cs.AI' 'cs.CR' ... 'D.1.3; G.4; I.2.8; I.2.11; I.5.3; J.3'\n",
      " '68T07, 68T45, 68T10, 68T50, 68U35' 'I.2.0; G.3']\n",
      "1177\n"
     ]
    }
   ],
   "source": [
    "print('labels: ',labels)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "19YzyI4d3cOB"
   },
   "outputs": [],
   "source": [
    "df2=df2[~df2['titles'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZU_aDKO5nnQ",
    "outputId": "2f38ab11-e0bf-49ff-a5e4-7068ab52c681"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41105, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GDIPLAos5nju",
    "outputId": "7ba8c4da-8048-4169-bdc2-3a85fac99deb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2503\n",
      "3401\n"
     ]
    }
   ],
   "source": [
    "print(sum(df2.terms.value_counts()==1))\n",
    "print(df2['terms'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lxiwCrha5ng7"
   },
   "outputs": [],
   "source": [
    "df_filtered=df2.groupby('terms').filter(lambda x: len(x)>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "deo5o7T75nVy",
    "outputId": "97e85b90-d017-4f91-9e82-338be92effcf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38602, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rMtxK9GnCuxe",
    "outputId": "3d624527-991b-4275-e8a3-85e5342ff8eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>    38602\n",
      "Name: terms, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_filtered['terms'].apply(type).value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9qzmuejD809"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oeIKW81h7CR8",
    "outputId": "664b792d-9b8c-4ea0-9ba5-8199a6821d54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['cs.LG']), list(['cs.LG', 'cs.AI']),\n",
       "       list(['cs.LG', 'cs.CR', 'stat.ML'])], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered['terms']=df_filtered['terms'].apply(lambda x: literal_eval(x))\n",
    "df_filtered['terms'].values[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "8n-_b6kX7COf",
    "outputId": "344c4ee1-7b68-4f15-e723-c992a6a4b659"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                          ['cs.LG']\n",
       "1                                 ['cs.LG', 'cs.AI']\n",
       "2                      ['cs.LG', 'cs.CR', 'stat.ML']\n",
       "3                                 ['cs.LG', 'cs.CR']\n",
       "4                                          ['cs.LG']\n",
       "                            ...                     \n",
       "56173                                    ['stat.ML']\n",
       "56175                           ['stat.ML', 'cs.LG']\n",
       "56176                             ['cs.CV', 'cs.IR']\n",
       "56177    ['cs.LG', 'cs.AI', 'cs.CL', 'I.2.6; I.2.7']\n",
       "56179                ['stat.ML', 'cs.LG', 'math.OC']\n",
       "Name: terms, Length: 41105, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "TZmIwARI7CMF"
   },
   "outputs": [],
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "zjols7Ln7CKG"
   },
   "outputs": [],
   "source": [
    "train_df, test_df=train_test_split(df_filtered, test_size=0.1, stratify=df_filtered['terms'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Ge_CpuZL7CH2"
   },
   "outputs": [],
   "source": [
    "val_df=test_df.sample(frac=0.5)\n",
    "test_df.drop(val_df.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEwpSlqU7CFF",
    "outputId": "674fbdb8-9710-4c67-d2da-06992499f7ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1930, 3), (1931, 3))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "id": "CXzF4frq7CCm",
    "outputId": "8b31de15-29ec-44c1-d05f-5fbf2a382c16"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>Video Description: A Survey of Methods, Datase...</td>\n",
       "      <td>Video description is the automatic generation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45676</th>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>Index Network</td>\n",
       "      <td>We show that existing upsampling operators can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30620</th>\n",
       "      <td>[cs.LG, cs.PL, cs.SE]</td>\n",
       "      <td>WheaCha: A Method for Explaining the Predictio...</td>\n",
       "      <td>The last decade has witnessed a rapid advance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35139</th>\n",
       "      <td>[cs.LG, cs.AI, stat.ML]</td>\n",
       "      <td>A Theoretical Analysis of Catastrophic Forgett...</td>\n",
       "      <td>Continual learning (CL) is a setting in which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7671</th>\n",
       "      <td>[cs.LG, q-bio.NC]</td>\n",
       "      <td>Deep Reinforcement Learning Models Predict Vis...</td>\n",
       "      <td>Supervised deep convolutional neural networks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45513</th>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>Progressive Depth Learning for Single Image De...</td>\n",
       "      <td>The formulation of the hazy image is mainly do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8449</th>\n",
       "      <td>[cs.LG, cs.CR]</td>\n",
       "      <td>Query-based Targeted Action-Space Adversarial ...</td>\n",
       "      <td>Advances in computing resources have resulted ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28868</th>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>Bias Remediation in Driver Drowsiness Detectio...</td>\n",
       "      <td>Datasets are crucial when training a deep neur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45716</th>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>5D Light Field Synthesis from a Monocular Video</td>\n",
       "      <td>Commercially available light field cameras hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27385</th>\n",
       "      <td>[cs.LG, stat.ML]</td>\n",
       "      <td>Neural Random Forest Imitation</td>\n",
       "      <td>We present Neural Random Forest Imitation - a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34741 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         terms  \\\n",
       "1313                   [cs.CV]   \n",
       "45676                  [cs.CV]   \n",
       "30620    [cs.LG, cs.PL, cs.SE]   \n",
       "35139  [cs.LG, cs.AI, stat.ML]   \n",
       "7671         [cs.LG, q-bio.NC]   \n",
       "...                        ...   \n",
       "45513                  [cs.CV]   \n",
       "8449            [cs.LG, cs.CR]   \n",
       "28868                  [cs.CV]   \n",
       "45716                  [cs.CV]   \n",
       "27385         [cs.LG, stat.ML]   \n",
       "\n",
       "                                                  titles  \\\n",
       "1313   Video Description: A Survey of Methods, Datase...   \n",
       "45676                                      Index Network   \n",
       "30620  WheaCha: A Method for Explaining the Predictio...   \n",
       "35139  A Theoretical Analysis of Catastrophic Forgett...   \n",
       "7671   Deep Reinforcement Learning Models Predict Vis...   \n",
       "...                                                  ...   \n",
       "45513  Progressive Depth Learning for Single Image De...   \n",
       "8449   Query-based Targeted Action-Space Adversarial ...   \n",
       "28868  Bias Remediation in Driver Drowsiness Detectio...   \n",
       "45716    5D Light Field Synthesis from a Monocular Video   \n",
       "27385                     Neural Random Forest Imitation   \n",
       "\n",
       "                                               abstracts  \n",
       "1313   Video description is the automatic generation ...  \n",
       "45676  We show that existing upsampling operators can...  \n",
       "30620  The last decade has witnessed a rapid advance ...  \n",
       "35139  Continual learning (CL) is a setting in which ...  \n",
       "7671   Supervised deep convolutional neural networks ...  \n",
       "...                                                  ...  \n",
       "45513  The formulation of the hazy image is mainly do...  \n",
       "8449   Advances in computing resources have resulted ...  \n",
       "28868  Datasets are crucial when training a deep neur...  \n",
       "45716  Commercially available light field cameras hav...  \n",
       "27385  We present Neural Random Forest Imitation - a ...  \n",
       "\n",
       "[34741 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "1CX8k4Dl9dM6"
   },
   "outputs": [],
   "source": [
    "terms=tf.ragged.constant(train_df.terms)\n",
    "lookup=tf.keras.layers.StringLookup(output_mode='multi_hot')\n",
    "lookup.adapt(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Ye3twlEE9dJe"
   },
   "outputs": [],
   "source": [
    "vocab=lookup.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xy24qRFo9dG3",
    "outputId": "355388a1-be44-40ac-8d1e-ff591379ce55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cs.CV']\n",
      "tf.Tensor(\n",
      "[[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 165), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "sample_label=train_df['terms'].iloc[0]\n",
    "print(sample_label)\n",
    "label_binarized=lookup([sample_label])\n",
    "print(label_binarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "2E1IRmNc9dEc"
   },
   "outputs": [],
   "source": [
    "max_len=150\n",
    "batch_size=128\n",
    "padding_token=\"<pad>\"\n",
    "auto =tf.data.AUTOTUNE\n",
    "\n",
    "def make_dataset(dataframe, is_train=True):\n",
    "    labels=tf.ragged.constant(dataframe.terms.values)\n",
    "    label_binarized=lookup(labels).numpy()\n",
    "    dataset=tf.data.Dataset.from_tensor_slices((dataframe['abstracts'].values, label_binarized))\n",
    "    dataset= dataset.shuffle(batch_size*10) if is_train else dataset\n",
    "    # only that data is taken where it is training data\n",
    "\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "train_dataset=make_dataset(train_df, is_train=True)\n",
    "test_dataset=make_dataset(test_df, is_train=False)\n",
    "val_dataset=make_dataset(val_df, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 165), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gkmrFVpnGbIu",
    "outputId": "2b183501-922b-4fc9-a3d7-b34b86a20e98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Vertical federated learning (VFL) attracts increasing attention due to the\\nemerging demands of multi-party collaborative modeling and concerns of privacy\\nleakage. In the real VFL applications, usually only one or partial parties hold\\nlabels, which makes it challenging for all parties to collaboratively learn the\\nmodel without privacy leakage. Meanwhile, most existing VFL algorithms are\\ntrapped in the synchronous computations, which leads to inefficiency in their\\nreal-world applications. To address these challenging problems, we propose a\\nnovel {\\\\bf VF}L framework integrated with new {\\\\bf b}ackward updating mechanism\\nand {\\\\bf b}ilevel asynchronous parallel architecture (VF{${\\\\textbf{B}}^2$}),\\nunder which three new algorithms, including VF{${\\\\textbf{B}}^2$}-SGD, -SVRG,\\nand -SAGA, are proposed. We derive the theoretical results of the convergence\\nrates of these three algorithms under both strongly convex and nonconvex\\nconditions. We also prove the security of VF{${\\\\textbf{B}}^2$} under\\nsemi-honest threat models. Extensive experiments on benchmark datasets\\ndemonstrate that our algorithms are efficient, scalable and lossless.', shape=(), dtype=string)\n",
      "['cs.LG']\n",
      "tf.Tensor(b'We explore the efficiency of the CRF inference beyond image level semantic\\nsegmentation and perform joint inference in video frames. The key idea is to\\ncombine best of two worlds: semantic co-labeling and more expressive models.\\nOur formulation enables us to perform inference over ten thousand images within\\nseconds and makes the system amenable to perform video semantic segmentation\\nmost effectively. On CamVid dataset, with TextonBoost unaries, our proposed\\nmethod achieves up to 8% improvement in accuracy over individual semantic image\\nsegmentation without additional time overhead. The source code is available at\\nhttps://github.com/subtri/video_inference', shape=(), dtype=string)\n",
      "['cs.CV']\n",
      "tf.Tensor(b'We define disentanglement as how far class-different data points from each\\nother are, relative to the distances among class-similar data points. When\\nmaximizing disentanglement during representation learning, we obtain a\\ntransformed feature representation where the class memberships of the data\\npoints are preserved. If the class memberships of the data points are\\npreserved, we would have a feature representation space in which a nearest\\nneighbour classifier or a clustering algorithm would perform well. We take\\nadvantage of this method to learn better natural language representation, and\\nemploy it on text classification and text clustering tasks. Through\\ndisentanglement, we obtain text representations with better-defined clusters\\nand improve text classification performance. Our approach had a test\\nclassification accuracy of as high as 90.11% and test clustering accuracy of\\n88% on the AG News dataset, outperforming our baseline models -- without any\\nother training tricks or regularization.', shape=(), dtype=string)\n",
      "['cs.LG' 'cs.CL' 'cs.NE']\n",
      "tf.Tensor(b'We propose a Multi-Instance-Learning (MIL) approach for weakly-supervised\\nlearning problems, where a training set is formed by bags (sets of feature\\nvectors or instances) and only labels at bag-level are provided. Specifically,\\nwe consider the Multi-Instance Dynamic-Ordinal-Regression (MI-DOR) setting,\\nwhere the instance labels are naturally represented as ordinal variables and\\nbags are structured as temporal sequences. To this end, we propose\\nMulti-Instance Dynamic Ordinal Random Fields (MI-DORF). In this framework, we\\ntreat instance-labels as temporally-dependent latent variables in an Undirected\\nGraphical Model. Different MIL assumptions are modelled via newly introduced\\nhigh-order potentials relating bag and instance-labels within the energy\\nfunction of the model. We also extend our framework to address the\\nPartially-Observed MI-DOR problems, where a subset of instance labels are\\navailable during training. We show on the tasks of weakly-supervised facial\\nbehavior analysis, Facial Action Unit (DISFA dataset) and Pain (UNBC dataset)\\nIntensity estimation, that the proposed framework outperforms alternative\\nlearning approaches. Furthermore, we show that MIDORF can be employed to reduce\\nthe data annotation efforts in this context by large-scale.', shape=(), dtype=string)\n",
      "['cs.CV' 'cs.AI']\n",
      "tf.Tensor(b'Due to the realization that deep reinforcement learning algorithms trained on\\nhigh-dimensional tasks can strongly overfit to their training environments,\\nthere have been several studies that investigated the generalization\\nperformance of these algorithms. However, there has been no similar study that\\nevaluated the generalization performance of algorithms that were specifically\\ndesigned for generalization, i.e. meta-reinforcement learning algorithms. In\\nthis paper, we assess the generalization performance of these algorithms by\\nleveraging high-dimensional, procedurally generated environments. We find that\\nthese algorithms can display strong overfitting when they are evaluated on\\nchallenging tasks. We also observe that scalability to high-dimensional tasks\\nwith sparse rewards remains a significant problem among many of the current\\nmeta-reinforcement learning algorithms. With these results, we highlight the\\nneed for developing meta-reinforcement learning algorithms that can both\\ngeneralize and scale.', shape=(), dtype=string)\n",
      "['cs.LG' 'stat.ML' 'cs.AI']\n"
     ]
    }
   ],
   "source": [
    "def invert_multi_hot(encoded_label):\n",
    "    hot_indices=  np.argwhere(encoded_label==1.0)[...,0]\n",
    "    return np.take(vocab,hot_indices)\n",
    "\n",
    "text_batch, label_batch=next(iter(train_dataset))\n",
    "for i, text in enumerate(text_batch[:5]):\n",
    "    label=label_batch[i].numpy()[None,...]\n",
    "    print(text)\n",
    "    print(invert_multi_hot(label[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "3vvH5Xi7IGOT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159138\n"
     ]
    }
   ],
   "source": [
    "vocabulary=set()\n",
    "train_df['abstracts'].str.lower().str.split().apply(vocabulary.update)\n",
    "vocabulary_size=len(vocabulary) #no of words in the total abstractions\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAB811M6GYaN"
   },
   "source": [
    "##  Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'In this paper we present our work on developing an automated system for land\\ncover classification. This system takes a multiband satellite image of an area\\nas input and outputs the land cover map of the area at the same resolution as\\nthe input. For this purpose convolutional machine learning models were trained\\nin the task of predicting the land cover semantic segmentation of satellite\\nimages. This is a case of supervised learning. The land cover label data were\\ntaken from the CORINE Land Cover inventory and the satellite images were taken\\nfrom the Copernicus hub. As for the model, U-Net architecture variations were\\napplied. Our area of interest are the Ionian islands (Greece). We created a\\ndataset from scratch covering this particular area. In addition, transfer\\nlearning from the BigEarthNet dataset [1] was performed. In [1] simple\\nclassification of satellite images into the classes of CLC is performed but not\\nsegmentation as we do. However, their models have been trained into a dataset\\nmuch bigger than ours, so we applied transfer learning using their pretrained\\nmodels as the first part of out network, utilizing the ability these networks\\nhave developed to extract useful features from the satellite images (we\\ntransferred a pretrained ResNet50 into a U-Res-Net). Apart from transfer\\nlearning other techniques were applied in order to overcome the limitations set\\nby the small size of our area of interest. We used data augmentation (cutting\\nimages into overlapping patches, applying random transformations such as\\nrotations and flips) and cross validation. The results are tested on the 3 CLC\\nclass hierarchy levels and a comparative study is made on the results of\\ndifferent approaches.'\n",
      " b'Boltzmann exploration is widely used in reinforcement learning to provide a\\ntrade-off between exploration and exploitation. Recently, in (Cesa-Bianchi et\\nal., 2017) it has been shown that pure Boltzmann exploration does not perform\\nwell from a regret perspective, even in the simplest setting of stochastic\\nmulti-armed bandit (MAB) problems. In this paper, we show that a simple\\nmodification to Boltzmann exploration, motivated by a variation of the standard\\ndoubling trick, achieves $O(K\\\\log^{1+\\\\alpha} T)$ regret for a stochastic MAB\\nproblem with $K$ arms, where $\\\\alpha>0$ is a parameter of the algorithm. This\\nimproves on the result in (Cesa-Bianchi et al., 2017), where an algorithm\\ninspired by the Gumbel-softmax trick achieves $O(K\\\\log^2 T)$ regret. We also\\nshow that our algorithm achieves $O(\\\\beta(G) \\\\log^{1+\\\\alpha} T)$ regret in\\nstochastic MAB problems with graph-structured feedback, without knowledge of\\nthe graph structure, where $\\\\beta(G)$ is the independence number of the\\nfeedback graph. Additionally, we present extensive experimental results on real\\ndatasets and applications for multi-armed bandits with both traditional bandit\\nfeedback and graph-structured feedback. In all cases, our algorithm performs as\\nwell or better than the state-of-the-art.'\n",
      " b'We are interested in reconstructing the mesh representation of object\\nsurfaces from point clouds. Surface reconstruction is a prerequisite for\\ndownstream applications such as rendering, collision avoidance for planning,\\nanimation, etc. However, the task is challenging if the input point cloud has a\\nlow resolution, which is common in real-world scenarios (e.g., from LiDAR or\\nKinect sensors). Existing learning-based mesh generative methods mostly predict\\nthe surface by first building a shape embedding that is at the whole object\\nlevel, a design that causes issues in generating fine-grained details and\\ngeneralizing to unseen categories. Instead, we propose to leverage the input\\npoint cloud as much as possible, by only adding connectivity information to\\nexisting points. Particularly, we predict which triplets of points should form\\nfaces. Our key innovation is a surrogate of local connectivity, calculated by\\ncomparing the intrinsic/extrinsic metrics. We learn to predict this surrogate\\nusing a deep point cloud network and then feed it to an efficient\\npost-processing module for high-quality mesh generation. We demonstrate that\\nour method can not only preserve details, handle ambiguous structures, but also\\npossess strong generalizability to unseen categories by experiments on\\nsynthetic and real data. The code is available at\\nhttps://github.com/Colin97/Point2Mesh.'\n",
      " b'Image Registration is the process of aligning two or more images of the same\\nscene with reference to a particular image. The images are captured from\\nvarious sensors at different times and at multiple view-points. Thus to get a\\nbetter picture of any change of a scene or object over a considerable period of\\ntime image registration is important. Image registration finds application in\\nmedical sciences, remote sensing and in computer vision. This paper presents a\\ndetailed review of several approaches which are classified accordingly along\\nwith their contributions and drawbacks. The main steps of an image registration\\nprocedure are also discussed. Different performance measures are presented that\\ndetermine the registration quality and accuracy. The scope for the future\\nresearch are presented as well.'\n",
      " b'The ability to automatically detect other vehicles on the road is vital to\\nthe safety of partially-autonomous and fully-autonomous vehicles. Most of the\\nhigh-accuracy techniques for this task are based on R-CNN or one of its faster\\nvariants. In the research community, much emphasis has been applied to using 3D\\nvision or complex R-CNN variants to achieve higher accuracy. However, are there\\nmore straightforward modifications that could deliver higher accuracy? Yes. We\\nshow that increasing input image resolution (i.e. upsampling) offers up to 12\\npercentage-points higher accuracy compared to an off-the-shelf baseline. We\\nalso find situations where earlier/shallower layers of CNN provide higher\\naccuracy than later/deeper layers. We further show that shallow models and\\nupsampled images yield competitive accuracy. Our findings contrast with the\\ncurrent trend towards deeper and larger models to achieve high accuracy in\\ndomain specific detection tasks.'\n",
      " b'Choosing the best-performing optimizer(s) out of a portfolio of optimization\\nalgorithms is usually a difficult and complex task. It gets even worse, if the\\nunderlying functions are unknown, i.e., so-called Black-Box problems, and\\nfunction evaluations are considered to be expensive. In the case of continuous\\nsingle-objective optimization problems, Exploratory Landscape Analysis (ELA) -\\na sophisticated and effective approach for characterizing the landscapes of\\nsuch problems by means of numerical values before actually performing the\\noptimization task itself - is advantageous. Unfortunately, until now it has\\nbeen quite complicated to compute multiple ELA features simultaneously, as the\\ncorresponding code has been - if at all - spread across multiple platforms or\\nat least across several packages within these platforms.\\n  This article presents a broad summary of existing ELA approaches and\\nintroduces flacco, an R-package for feature-based landscape analysis of\\ncontinuous and constrained optimization problems. Although its functions\\nneither solve the optimization problem itself nor the related \"Algorithm\\nSelection Problem (ASP)\", it offers easy access to an essential ingredient of\\nthe ASP by providing a wide collection of ELA features on a single platform -\\neven within a single package. In addition, flacco provides multiple\\nvisualization techniques, which enhance the understanding of some of these\\nnumerical features, and thereby make certain landscape properties more\\ncomprehensible. On top of that, we will introduce the package\\'s build-in, as\\nwell as web-hosted and hence platform-independent, graphical user interface\\n(GUI), which facilitates the usage of the package - especially for people who\\nare not familiar with R - making it a very convenient toolbox when working\\ntowards algorithm selection of continuous single-objective optimization\\nproblems.'\n",
      " b'Multimodal image registration (MIR) is a fundamental procedure in many\\nimage-guided therapies. Recently, unsupervised learning-based methods have\\ndemonstrated promising performance over accuracy and efficiency in deformable\\nimage registration. However, the estimated deformation fields of the existing\\nmethods fully rely on the to-be-registered image pair. It is difficult for the\\nnetworks to be aware of the mismatched boundaries, resulting in unsatisfactory\\norgan boundary alignment. In this paper, we propose a novel multimodal\\nregistration framework, which leverages the deformation fields estimated from\\nboth: (i) the original to-be-registered image pair, (ii) their corresponding\\ngradient intensity maps, and adaptively fuses them with the proposed gated\\nfusion module. With the help of auxiliary gradient-space guidance, the network\\ncan concentrate more on the spatial relationship of the organ boundary.\\nExperimental results on two clinically acquired CT-MRI datasets demonstrate the\\neffectiveness of our proposed approach.'\n",
      " b'Forecasting accuracy is reliant on the quality of available past data. Data\\ndisruptions can adversely affect the quality of the generated model (e.g.\\nunexpected events such as out-of-stock products when forecasting demand). We\\naddress this problem by pastcasting: predicting how data should have been in\\nthe past to explain the future better. We propose Pastprop-LSTM, a data-centric\\nbackpropagation algorithm that assigns part of the responsibility for errors to\\nthe training data and changes it accordingly. We test three variants of\\nPastprop-LSTM on forecasting competition datasets, M4 and M5, plus the Numenta\\nAnomaly Benchmark. Empirical evaluation indicates that the proposed method can\\nimprove forecasting accuracy, especially when the prediction errors of standard\\nLSTM are high. It also demonstrates the potential of the algorithm on datasets\\ncontaining anomalies.'\n",
      " b'Modern Generative Adversarial Networks are capable of creating artificial,\\nphotorealistic images from latent vectors living in a low-dimensional learned\\nlatent space. It has been shown that a wide range of images can be projected\\ninto this space, including images outside of the domain that the generator was\\ntrained on. However, while in this case the generator reproduces the pixels and\\ntextures of the images, the reconstructed latent vectors are unstable and small\\nperturbations result in significant image distortions. In this work, we propose\\nto explicitly model the data distribution in latent space. We show that, under\\na simple nonlinear operation, the data distribution can be modeled as Gaussian\\nand therefore expressed using sufficient statistics. This yields a simple\\nGaussian prior, which we use to regularize the projection of images into the\\nlatent space. The resulting projections lie in smoother and better behaved\\nregions of the latent space, as shown using interpolation performance for both\\nreal and generated images. Furthermore, the Gaussian model of the distribution\\nin latent space allows us to investigate the origins of artifacts in the\\ngenerator output, and provides a method for reducing these artifacts while\\nmaintaining diversity of the generated images.'\n",
      " b'Unpaired image-to-image translation refers to learning inter-image-domain\\nmapping in an unsupervised manner. Existing methods often learn deterministic\\nmappings without explicitly modelling the robustness to outliers or predictive\\nuncertainty, leading to performance degradation when encountering unseen\\nout-of-distribution (OOD) patterns at test time. To address this limitation, we\\npropose a novel probabilistic method called Uncertainty-aware Generalized\\nAdaptive Cycle Consistency (UGAC), which models the per-pixel residual by\\ngeneralized Gaussian distribution, capable of modelling heavy-tailed\\ndistributions. We compare our model with a wide variety of state-of-the-art\\nmethods on two challenging tasks: unpaired image denoising in the natural image\\nand unpaired modality prorogation in medical image domains. Experimental\\nresults demonstrate that our model offers superior image generation quality\\ncompared to recent methods in terms of quantitative metrics such as\\nsignal-to-noise ratio and structural similarity. Our model also exhibits\\nstronger robustness towards OOD test data.'\n",
      " b'Image segmentation is a vital part of image processing. Segmentation has its\\napplication widespread in the field of medical images in order to diagnose\\ncurious diseases. The same medical images can be segmented manually. But the\\naccuracy of image segmentation using the segmentation algorithms is more when\\ncompared with the manual segmentation. In the field of medical diagnosis an\\nextensive diversity of imaging techniques is presently available, such as\\nradiography, computed tomography (CT) and magnetic resonance imaging (MRI).\\nMedical image segmentation is an essential step for most consequent image\\nanalysis tasks. Although the original FCM algorithm yields good results for\\nsegmenting noise free images, it fails to segment images corrupted by noise,\\noutliers and other imaging artifact. This paper presents an image segmentation\\napproach using Modified Fuzzy C-Means (FCM) algorithm and Fuzzy Possibilistic\\nc-means algorithm (FPCM). This approach is a generalized version of standard\\nFuzzy CMeans Clustering (FCM) algorithm. The limitation of the conventional FCM\\ntechnique is eliminated in modifying the standard technique. The Modified FCM\\nalgorithm is formulated by modifying the distance measurement of the standard\\nFCM algorithm to permit the labeling of a pixel to be influenced by other\\npixels and to restrain the noise effect during segmentation. Instead of having\\none term in the objective function, a second term is included, forcing the\\nmembership to be as high as possible without a maximum limit constraint of one.\\nExperiments are conducted on real images to investigate the performance of the\\nproposed modified FCM technique in segmenting the medical images. Standard FCM,\\nModified FCM, Fuzzy Possibilistic CMeans algorithm (FPCM) are compared to\\nexplore the accuracy of our proposed approach.'\n",
      " b'Endoscopic artefact detection challenge consists of 1) Artefact detection, 2)\\nSemantic segmentation, and 3) Out-of-sample generalisation. For Semantic\\nsegmentation task, we propose a multi-plateau ensemble of FPN (Feature Pyramid\\nNetwork) with EfficientNet as feature extractor/encoder. For Object detection\\ntask, we used a three model ensemble of RetinaNet with Resnet50 Backbone and\\nFasterRCNN (FPN + DC5) with Resnext101 Backbone}. A PyTorch implementation to\\nour approach to the problem is available at\\nhttps://github.com/ubamba98/EAD2020.'\n",
      " b\"Graph neural networks (GNN) have been ubiquitous in graph learning tasks such\\nas node classification. Most of GNN methods update the node embedding\\niteratively by aggregating its neighbors' information. However, they often\\nsuffer from negative disturbance, due to edges connecting nodes with different\\nlabels. One approach to alleviate this negative disturbance is to use\\nattention, but current attention always considers feature similarity and\\nsuffers from the lack of supervision. In this paper, we consider the label\\ndependency of graph nodes and propose a decoupling attention mechanism to learn\\nboth hard and soft attention. The hard attention is learned on labels for a\\nrefined graph structure with fewer inter-class edges. Its purpose is to reduce\\nthe aggregation's negative disturbance. The soft attention is learned on\\nfeatures maximizing the information gain by message passing over better graph\\nstructures. Moreover, the learned attention guides the label propagation and\\nthe feature propagation. Extensive experiments are performed on five well-known\\nbenchmark graph datasets to verify the effectiveness of the proposed method.\"\n",
      " b'Graph Neural Networks (GNNs) are effective in many applications. Still, there\\nis a limited understanding of the effect of common graph structures on the\\nlearning process of GNNs. In this work, we systematically study the impact of\\ncommunity structure on the performance of GNNs in semi-supervised node\\nclassification on graphs. Following an ablation study on six datasets, we\\nmeasure the performance of GNNs on the original graphs, and the change in\\nperformance in the presence and the absence of community structure. Our results\\nsuggest that communities typically have a major impact on the learning process\\nand classification performance. For example, in cases where the majority of\\nnodes from one community share a single classification label, breaking up\\ncommunity structure results in a significant performance drop. On the other\\nhand, for cases where labels show low correlation with communities, we find\\nthat the graph structure is rather irrelevant to the learning process, and a\\nfeature-only baseline becomes hard to beat. With our work, we provide deeper\\ninsights in the abilities and limitations of GNNs, including a set of general\\nguidelines for model selection based on the graph structure.'\n",
      " b'In recent years, graph neural networks (GNNs) have emerged as a powerful\\nneural architecture to learn vector representations of nodes and graphs in a\\nsupervised, end-to-end fashion. Up to now, GNNs have only been evaluated\\nempirically---showing promising results. The following work investigates GNNs\\nfrom a theoretical point of view and relates them to the $1$-dimensional\\nWeisfeiler-Leman graph isomorphism heuristic ($1$-WL). We show that GNNs have\\nthe same expressiveness as the $1$-WL in terms of distinguishing non-isomorphic\\n(sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on\\nthis, we propose a generalization of GNNs, so-called $k$-dimensional GNNs\\n($k$-GNNs), which can take higher-order graph structures at multiple scales\\ninto account. These higher-order structures play an essential role in the\\ncharacterization of social networks and molecule graphs. Our experimental\\nevaluation confirms our theoretical findings as well as confirms that\\nhigher-order information is useful in the task of graph classification and\\nregression.'\n",
      " b'We present an interactive approach to synthesizing realistic variations in\\nfacial hair in images, ranging from subtle edits to existing hair to the\\naddition of complex and challenging hair in images of clean-shaven subjects. To\\ncircumvent the tedious and computationally expensive tasks of modeling,\\nrendering and compositing the 3D geometry of the target hairstyle using the\\ntraditional graphics pipeline, we employ a neural network pipeline that\\nsynthesizes realistic and detailed images of facial hair directly in the target\\nimage in under one second. The synthesis is controlled by simple and sparse\\nguide strokes from the user defining the general structural and color\\nproperties of the target hairstyle. We qualitatively and quantitatively\\nevaluate our chosen method compared to several alternative approaches. We show\\ncompelling interactive editing results with a prototype user interface that\\nallows novice users to progressively refine the generated image to match their\\ndesired hairstyle, and demonstrate that our approach also allows for flexible\\nand high-fidelity scalp hair synthesis.'\n",
      " b'Reinforcement learning (RL) has proven its worth in a series of artificial\\ndomains, and is beginning to show some successes in real-world scenarios.\\nHowever, much of the research advances in RL are often hard to leverage in\\nreal-world systems due to a series of assumptions that are rarely satisfied in\\npractice. We present a set of nine unique challenges that must be addressed to\\nproductionize RL to real world problems. For each of these challenges, we\\nspecify the exact meaning of the challenge, present some approaches from the\\nliterature, and specify some metrics for evaluating that challenge. An approach\\nthat addresses all nine challenges would be applicable to a large number of\\nreal world problems. We also present an example domain that has been modified\\nto present these challenges as a testbed for practical RL research.'\n",
      " b'Entropy regularization is used to get improved optimization performance in\\nreinforcement learning tasks. A common form of regularization is to maximize\\npolicy entropy to avoid premature convergence and lead to more stochastic\\npolicies for exploration through action space. However, this does not ensure\\nexploration in the state space. In this work, we instead consider the\\ndistribution of discounted weighting of states, and propose to maximize the\\nentropy of a lower bound approximation to the weighting of a state, based on\\nlatent space state representation. We propose entropy regularization based on\\nthe marginal state distribution, to encourage the policy to have a more uniform\\ndistribution over the state space for exploration. Our approach based on\\nmarginal state distribution achieves superior state space coverage on complex\\ngridworld domains, that translate into empirical gains in sparse reward 3D maze\\nnavigation and continuous control domains compared to entropy regularization\\nwith stochastic policies.'\n",
      " b'Face editing represents a popular research topic within the computer vision\\nand image processing communities. While significant progress has been made\\nrecently in this area, existing solutions: (i) are still largely focused on\\nlow-resolution images, (ii) often generate editing results with visual\\nartefacts, or (iii) lack fine-grained control and alter multiple (entangled)\\nattributes at once, when trying to generate the desired facial semantics. In\\nthis paper, we aim to address these issues though a novel attribute editing\\napproach called MaskFaceGAN. The proposed approach is based on an optimization\\nprocedure that directly optimizes the latent code of a pre-trained\\n(state-of-the-art) Generative Adversarial Network (i.e., StyleGAN2) with\\nrespect to several constraints that ensure: (i) preservation of relevant image\\ncontent, (ii) generation of the targeted facial attributes, and (iii)\\nspatially--selective treatment of local image areas. The constraints are\\nenforced with the help of an (differentiable) attribute classifier and face\\nparser that provide the necessary reference information for the optimization\\nprocedure. MaskFaceGAN is evaluated in extensive experiments on the CelebA-HQ,\\nHelen and SiblingsDB-HQf datasets and in comparison with several\\nstate-of-the-art techniques from the literature, i.e., StarGAN, AttGAN, STGAN,\\nand two versions of InterFaceGAN. Our experimental results show that the\\nproposed approach is able to edit face images with respect to several facial\\nattributes with unprecedented image quality and at high-resolutions\\n(1024x1024), while exhibiting considerably less problems with attribute\\nentanglement than competing solutions. The source code is made freely available\\nfrom: https://github.com/MartinPernus/MaskFaceGAN.'\n",
      " b'In recent years, methods concerning the place recognition task have been\\nextensively examined from the robotics community within the scope of\\nsimultaneous localization and mapping applications. In this article, an\\nappearance-based loop closure detection pipeline is proposed, entitled \"FILD++\"\\n(Fast and Incremental Loop closure Detection). When the incoming camera\\nobservation arrives, global and local visual features are extracted through two\\npasses of a single convolutional neural network. Subsequently, a modified\\nhierarchical-navigable small-world graph incrementally generates a visual\\ndatabase that represents the robot\\'s traversed path based on global features.\\nGiven the query sensor measurement, similar locations from the trajectory are\\nretrieved using these representations, while an image-to-image pairing is\\nfurther evaluated thanks to the spatial information provided by the local\\nfeatures. Exhaustive experiments on several publicly-available datasets exhibit\\nthe system\\'s high performance and low execution time compared to other\\ncontemporary state-of-the-art pipelines.'\n",
      " b'Animals are able to discover the topological map (graph) of surrounding\\nenvironment, which will be used for navigation. Inspired by this biological\\nphenomenon, researchers have recently proposed to generate graph representation\\nfor Markov decision process (MDP) and use such graphs for planning in\\nreinforcement learning (RL). However, existing graph generation methods suffer\\nfrom many drawbacks. One drawback is that existing methods do not learn an\\nabstraction for graphs, which results in high memory and computation cost. This\\ndrawback also makes generated graph non-robust, which degrades the planning\\nperformance. Another drawback is that existing methods cannot be used for\\nfacilitating exploration which is important in RL. In this paper, we propose a\\nnew method, called topological map abstraction (TOMA), for graph generation.\\nTOMA can generate an abstract graph representation for MDP, which costs much\\nless memory and computation cost than existing methods. Furthermore, TOMA can\\nbe used for facilitating exploration. In particular, we propose planning to\\nexplore, in which TOMA is used to accelerate exploration by guiding the agent\\ntowards unexplored states. A novel experience replay module called vertex\\nmemory is also proposed to improve exploration performance. Experimental\\nresults show that TOMA can outperform existing methods to achieve the\\nstate-of-the-art performance.'\n",
      " b'The application of deep learning to medical image segmentation has been\\nhampered due to the lack of abundant pixel-level annotated data. Few-shot\\nSemantic Segmentation (FSS) is a promising strategy for breaking the deadlock.\\nHowever, a high-performing FSS model still requires sufficient pixel-level\\nannotated classes for training to avoid overfitting, which leads to its\\nperformance bottleneck in medical image segmentation due to the unmet need for\\nannotations. Thus, semi-supervised FSS for medical images is accordingly\\nproposed to utilize unlabeled data for further performance improvement.\\nNevertheless, existing semi-supervised FSS methods has two obvious defects: (1)\\nneglecting the relationship between the labeled and unlabeled data; (2) using\\nunlabeled data directly for end-to-end training leads to degenerated\\nrepresentation learning. To address these problems, we propose a novel\\nsemi-supervised FSS framework for medical image segmentation. The proposed\\nframework employs Poisson learning for modeling data relationship and\\npropagating supervision signals, and Spatial Consistency Calibration for\\nencouraging the model to learn more coherent representations. In this process,\\nunlabeled samples do not involve in end-to-end training, but provide\\nsupervisory information for query image segmentation through graph-based\\nlearning. We conduct extensive experiments on three medical image segmentation\\ndatasets (i.e. ISIC skin lesion segmentation, abdominal organs segmentation for\\nMRI and abdominal organs segmentation for CT) to demonstrate the\\nstate-of-the-art performance and broad applicability of the proposed framework.'\n",
      " b'Adversarially trained deep neural networks have significantly improved\\nperformance of single image super resolution, by hallucinating photorealistic\\nlocal textures, thereby greatly reducing the perception difference between a\\nreal high resolution image and its super resolved (SR) counterpart. However,\\napplication to medical imaging requires preservation of diagnostically relevant\\nfeatures while refraining from introducing any diagnostically confusing\\nartifacts. We propose using a deep convolutional super resolution network\\n(SRNet) trained for (i) minimising reconstruction loss between the real and SR\\nimages, and (ii) maximally confusing learned relativistic visual Turing test\\n(rVTT) networks to discriminate between (a) pair of real and SR images (T1) and\\n(b) pair of patches in real and SR selected from region of interest (T2). The\\nadversarial loss of T1 and T2 while backpropagated through SRNet helps it learn\\nto reconstruct pathorealism in the regions of interest such as white blood\\ncells (WBC) in peripheral blood smears or epithelial cells in histopathology of\\ncancerous biopsy tissues, which are experimentally demonstrated here.\\nExperiments performed for measuring signal distortion loss using peak signal to\\nnoise ratio (pSNR) and structural similarity (SSIM) with variation of SR scale\\nfactors, impact of rVTT adversarial losses, and impact on reporting using SR on\\na commercially available artificial intelligence (AI) digital pathology system\\nsubstantiate our claims.'\n",
      " b'We consider an important task of effective and efficient semantic image\\nsegmentation. In particular, we adapt a powerful semantic segmentation\\narchitecture, called RefineNet, into the more compact one, suitable even for\\ntasks requiring real-time performance on high-resolution inputs. To this end,\\nwe identify computationally expensive blocks in the original setup, and propose\\ntwo modifications aimed to decrease the number of parameters and floating point\\noperations. By doing that, we achieve more than twofold model reduction, while\\nkeeping the performance levels almost intact. Our fastest model undergoes a\\nsignificant speed-up boost from 20 FPS to 55 FPS on a generic GPU card on\\n512x512 inputs with solid 81.1% mean iou performance on the test set of PASCAL\\nVOC, while our slowest model with 32 FPS (from original 17 FPS) shows 82.7%\\nmean iou on the same dataset. Alternatively, we showcase that our approach is\\neasily mixable with light-weight classification networks: we attain 79.2% mean\\niou on PASCAL VOC using a model that contains only 3.3M parameters and performs\\nonly 9.3B floating point operations.'\n",
      " b'Issued from Optimal Transport, the Wasserstein distance has gained importance\\nin Machine Learning due to its appealing geometrical properties and the\\nincreasing availability of efficient approximations. In this work, we consider\\nthe problem of estimating the Wasserstein distance between two probability\\ndistributions when observations are polluted by outliers. To that end, we\\ninvestigate how to leverage Medians of Means (MoM) estimators to robustify the\\nestimation of Wasserstein distance. Exploiting the dual Kantorovitch\\nformulation of Wasserstein distance, we introduce and discuss novel MoM-based\\nrobust estimators whose consistency is studied under a data contamination model\\nand for which convergence rates are provided. These MoM estimators enable to\\nmake Wasserstein Generative Adversarial Network (WGAN) robust to outliers, as\\nwitnessed by an empirical study on two benchmarks CIFAR10 and Fashion MNIST.\\nEventually, we discuss how to combine MoM with the entropy-regularized\\napproximation of the Wasserstein distance and propose a simple MoM-based\\nre-weighting scheme that could be used in conjunction with the Sinkhorn\\nalgorithm.'\n",
      " b\"Proposed are alternative generator architectures for Boundary Equilibrium\\nGenerative Adversarial Networks, motivated by Learning from Simulated and\\nUnsupervised Images through Adversarial Training. It disentangles the need for\\na noise-based latent space. The generator will operate mainly as a refiner\\nnetwork to gain a photo-realistic presentation of the given synthetic images.\\nIt also attempts to resolve the latent space's poorly understood properties by\\neliminating the need for noise injection and replacing it with an image-based\\nconcept. The new flexible and simple generator architecture will also give the\\npower to control the trade-off between restrictive refinement and\\nexpressiveness ability. Contrary to other available methods, this architecture\\nwill not require a paired or unpaired dataset of real and synthetic images for\\nthe training phase. Only a relatively small set of real images would suffice.\"\n",
      " b'Deep Q-Network (DQN) based multi-agent systems (MAS) for reinforcement\\nlearning (RL) use various schemes where in the agents have to learn and\\ncommunicate. The learning is however specific to each agent and communication\\nmay be satisfactorily designed for the agents. As more complex Deep QNetworks\\ncome to the fore, the overall complexity of the multi-agent system increases\\nleading to issues like difficulty in training, need for higher resources and\\nmore training time, difficulty in fine-tuning, etc. To address these issues we\\npropose a simple but efficient DQN based MAS for RL which uses shared state and\\nrewards, but agent-specific actions, for updation of the experience replay pool\\nof the DQNs, where each agent is a DQN. The benefits of the approach are\\noverall simplicity, faster convergence and better performance as compared to\\nconventional DQN based approaches. It should be noted that the method can be\\nextended to any DQN. As such we use simple DQN and DDQN (Double Q-learning)\\nrespectively on three separate tasks i.e. Cartpole-v1 (OpenAI Gym environment)\\n, LunarLander-v2 (OpenAI Gym environment) and Maze Traversal (customized\\nenvironment). The proposed approach outperforms the baseline on these tasks by\\ndecent margins respectively.'\n",
      " b'This paper addresses two crucial problems of learning disentangled image\\nrepresentations, namely controlling the degree of disentanglement during image\\nediting, and balancing the disentanglement strength and the reconstruction\\nquality. To encourage disentanglement, we devise a distance covariance based\\ndecorrelation regularization. Further, for the reconstruction step, our model\\nleverages a soft target representation combined with the latent image code. By\\nexploring the real-valued space of the soft target representation, we are able\\nto synthesize novel images with the designated properties. To improve the\\nperceptual quality of images generated by autoencoder (AE)-based models, we\\nextend the encoder-decoder architecture with the generative adversarial network\\n(GAN) by collapsing the AE decoder and the GAN generator into one. We also\\ndesign a classification based protocol to quantitatively evaluate the\\ndisentanglement strength of our model. Experimental results showcase the\\nbenefits of the proposed model.'\n",
      " b'Annotated datasets are commonly used in the training and evaluation of tasks\\ninvolving natural language and vision (image description generation, action\\nrecognition and visual question answering). However, many of the existing\\ndatasets reflect problems that emerge in the process of data selection and\\nannotation. Here we point out some of the difficulties and problems one\\nconfronts when creating and validating annotated vision and language datasets.'\n",
      " b'Brain networks have received considerable attention given the critical\\nsignificance for understanding human brain organization, for investigating\\nneurological disorders and for clinical diagnostic applications. Structural\\nbrain network (e.g. DTI) and functional brain network (e.g. fMRI) are the\\nprimary networks of interest. Most existing works in brain network analysis\\nfocus on either structural or functional connectivity, which cannot leverage\\nthe complementary information from each other. Although multi-view learning\\nmethods have been proposed to learn from both networks (or views), these\\nmethods aim to reach a consensus among multiple views, and thus distinct\\nintrinsic properties of each view may be ignored. How to jointly learn\\nrepresentations from structural and functional brain networks while preserving\\ntheir inherent properties is a critical problem. In this paper, we propose a\\nframework of Siamese community-preserving graph convolutional network (SCP-GCN)\\nto learn the structural and functional joint embedding of brain networks.\\nSpecifically, we use graph convolutions to learn the structural and functional\\njoint embedding, where the graph structure is defined with structural\\nconnectivity and node features are from the functional connectivity. Moreover,\\nwe propose to preserve the community structure of brain networks in the graph\\nconvolutions by considering the intra-community and inter-community properties\\nin the learning process. Furthermore, we use Siamese architecture which models\\nthe pair-wise similarity learning to guide the learning process. To evaluate\\nthe proposed approach, we conduct extensive experiments on two real brain\\nnetwork datasets. The experimental results demonstrate the superior performance\\nof the proposed approach in structural and functional joint embedding for\\nneurological disorder analysis, indicating its promising value for clinical\\napplications.'\n",
      " b'Graph neural networks (GNNs) achieve remarkable success in graph-based\\nsemi-supervised node classification, leveraging the information from\\nneighboring nodes to improve the representation learning of target node. The\\nsuccess of GNNs at node classification depends on the assumption that connected\\nnodes tend to have the same label. However, such an assumption does not always\\nwork, limiting the performance of GNNs at node classification. In this paper,\\nwe propose label-consistency based graph neural network(LC-GNN), leveraging\\nnode pairs unconnected but with the same labels to enlarge the receptive field\\nof nodes in GNNs. Experiments on benchmark datasets demonstrate the proposed\\nLC-GNN outperforms traditional GNNs in graph-based semi-supervised node\\nclassification.We further show the superiority of LC-GNN in sparse scenarios\\nwith only a handful of labeled nodes.'\n",
      " b'This paper studies the task of Visual Question Answering (VQA), which is\\ntopical in Multimedia community recently. Particularly, we explore two critical\\nresearch problems existed in VQA: (1) efficiently fusing the visual and textual\\nmodalities; (2) enabling the visual reasoning ability of VQA models in\\nanswering complex questions. To address these challenging problems, a novel\\nQuestion Guided Modular Routing Networks (QGMRN) has been proposed in this\\npaper. Particularly, The QGMRN is composed of visual, textual and routing\\nnetwork. The visual and textual network serve as the backbones for the generic\\nfeature extractors of visual and textual modalities. QGMRN can fuse the visual\\nand textual modalities at multiple semantic levels. Typically, the visual\\nreasoning is facilitated by the routing network in a discrete and stochastic\\nway by using Gumbel-Softmax trick for module selection. When the input reaches\\na certain modular layer, routing network newly proposed in this paper,\\ndynamically selects a portion of modules from that layer to process the input\\ndepending on the question features generated by the textual network. It can\\nalso learn to reason by routing between the generic modules without additional\\nsupervision information or expert knowledge. Benefiting from the dynamic\\nrouting mechanism, QGMRN can outperform the previous classical VQA methods by a\\nlarge margin and achieve the competitive results against the state-of-the-art\\nmethods. Furthermore, attention mechanism is integrated into our QGMRN model\\nand thus can further boost the model performance. Empirically, extensive\\nexperiments on the CLEVR and CLEVR-Humans datasets validate the effectiveness\\nof our proposed model, and the state-of-the-art performance has been achieved.'\n",
      " b'An increasing need of running Convolutional Neural Network (CNN) models on\\nmobile devices with limited computing power and memory resource encourages\\nstudies on efficient model design. A number of efficient architectures have\\nbeen proposed in recent years, for example, MobileNet, ShuffleNet, and\\nMobileNetV2. However, all these models are heavily dependent on depthwise\\nseparable convolution which lacks efficient implementation in most deep\\nlearning frameworks. In this study, we propose an efficient architecture named\\nPeleeNet, which is built with conventional convolution instead. On ImageNet\\nILSVRC 2012 dataset, our proposed PeleeNet achieves a higher accuracy and over\\n1.8 times faster speed than MobileNet and MobileNetV2 on NVIDIA TX2. Meanwhile,\\nPeleeNet is only 66% of the model size of MobileNet. We then propose a\\nreal-time object detection system by combining PeleeNet with Single Shot\\nMultiBox Detector (SSD) method and optimizing the architecture for fast speed.\\nOur proposed detection system2, named Pelee, achieves 76.4% mAP (mean average\\nprecision) on PASCAL VOC2007 and 22.4 mAP on MS COCO dataset at the speed of\\n23.6 FPS on iPhone 8 and 125 FPS on NVIDIA TX2. The result on COCO outperforms\\nYOLOv2 in consideration of a higher precision, 13.6 times lower computational\\ncost and 11.3 times smaller model size.'\n",
      " b\"In this work, a region-based Deep Convolutional Neural Network framework is\\nproposed for document structure learning. The contribution of this work\\ninvolves efficient training of region based classifiers and effective\\nensembling for document image classification. A primary level of `inter-domain'\\ntransfer learning is used by exporting weights from a pre-trained VGG16\\narchitecture on the ImageNet dataset to train a document classifier on whole\\ndocument images. Exploiting the nature of region based influence modelling, a\\nsecondary level of `intra-domain' transfer learning is used for rapid training\\nof deep learning models for image segments. Finally, stacked generalization\\nbased ensembling is utilized for combining the predictions of the base deep\\nneural network models. The proposed method achieves state-of-the-art accuracy\\nof 92.2% on the popular RVL-CDIP document image dataset, exceeding benchmarks\\nset by existing algorithms.\"\n",
      " b'Object detection and instance segmentation are two fundamental computer\\nvision tasks. They are closely correlated but their relations have not yet been\\nfully explored in most previous work. This paper presents RDSNet, a novel deep\\narchitecture for reciprocal object detection and instance segmentation. To\\nreciprocate these two tasks, we design a two-stream structure to learn features\\non both the object level (i.e., bounding boxes) and the pixel level (i.e.,\\ninstance masks) jointly. Within this structure, information from the two\\nstreams is fused alternately, namely information on the object level introduces\\nthe awareness of instance and translation variance to the pixel level, and\\ninformation on the pixel level refines the localization accuracy of objects on\\nthe object level in return. Specifically, a correlation module and a cropping\\nmodule are proposed to yield instance masks, as well as a mask based boundary\\nrefinement module for more accurate bounding boxes. Extensive experimental\\nanalyses and comparisons on the COCO dataset demonstrate the effectiveness and\\nefficiency of RDSNet. The source code is available at\\nhttps://github.com/wangsr126/RDSNet.'\n",
      " b'Model-based reinforcement learning (RL), which finds an optimal policy using\\nan empirical model, has long been recognized as one of the corner stones of RL.\\nIt is especially suitable for multi-agent RL (MARL), as it naturally decouples\\nthe learning and the planning phases, and avoids the non-stationarity problem\\nwhen all agents are improving their policies simultaneously using samples.\\nThough intuitive and widely-used, the sample complexity of model-based MARL\\nalgorithms has not been fully investigated. In this paper, our goal is to\\naddress the fundamental question about its sample complexity. We study arguably\\nthe most basic MARL setting: two-player discounted zero-sum Markov games, given\\nonly access to a generative model. We show that model-based MARL achieves a\\nsample complexity of $\\\\tilde O(|S||A||B|(1-\\\\gamma)^{-3}\\\\epsilon^{-2})$ for\\nfinding the Nash equilibrium (NE) value up to some $\\\\epsilon$ error, and the\\n$\\\\epsilon$-NE policies with a smooth planning oracle, where $\\\\gamma$ is the\\ndiscount factor, and $S,A,B$ denote the state space, and the action spaces for\\nthe two agents. We further show that such a sample bound is minimax-optimal (up\\nto logarithmic factors) if the algorithm is reward-agnostic, where the\\nalgorithm queries state transition samples without reward knowledge, by\\nestablishing a matching lower bound. This is in contrast to the usual\\nreward-aware setting, with a\\n$\\\\tilde\\\\Omega(|S|(|A|+|B|)(1-\\\\gamma)^{-3}\\\\epsilon^{-2})$ lower bound, where\\nthis model-based approach is near-optimal with only a gap on the $|A|,|B|$\\ndependence. Our results not only demonstrate the sample-efficiency of this\\nbasic model-based approach in MARL, but also elaborate on the fundamental\\ntradeoff between its power (easily handling the more challenging\\nreward-agnostic case) and limitation (less adaptive and suboptimal in\\n$|A|,|B|$), particularly arises in the multi-agent context.'\n",
      " b'Continual learning (CL) is a setting in which an agent has to learn from an\\nincoming stream of data during its entire lifetime. Although major advances\\nhave been made in the field, one recurring problem which remains unsolved is\\nthat of Catastrophic Forgetting (CF). While the issue has been extensively\\nstudied empirically, little attention has been paid from a theoretical angle.\\nIn this paper, we show that the impact of CF increases as two tasks\\nincreasingly align. We introduce a measure of task similarity called the NTK\\noverlap matrix which is at the core of CF. We analyze common projected gradient\\nalgorithms and demonstrate how they mitigate forgetting. Then, we propose a\\nvariant of Orthogonal Gradient Descent (OGD) which leverages structure of the\\ndata through Principal Component Analysis (PCA). Experiments support our\\ntheoretical findings and show how our method can help reduce CF on classical CL\\ndatasets.'\n",
      " b'This paper presents a novel physics-inspired deep learning approach for point\\ncloud processing motivated by the natural flow phenomena in fluid mechanics.\\nOur learning architecture jointly defines data in an Eulerian world space,\\nusing a static background grid, and a Lagrangian material space, using moving\\nparticles. By introducing this Eulerian-Lagrangian representation, we are able\\nto naturally evolve and accumulate particle features using flow velocities\\ngenerated from a generalized, high-dimensional force field. We demonstrate the\\nefficacy of this system by solving various point cloud classification and\\nsegmentation problems with state-of-the-art performance. The entire geometric\\nreservoir and data flow mimics the pipeline of the classic PIC/FLIP scheme in\\nmodeling natural flow, bridging the disciplines of geometric machine learning\\nand physical simulation.'\n",
      " b'A visually-grounded navigation instruction can be interpreted as a sequence\\nof expected observations and actions an agent following the correct trajectory\\nwould encounter and perform. Based on this intuition, we formulate the problem\\nof finding the goal location in Vision-and-Language Navigation (VLN) within the\\nframework of Bayesian state tracking - learning observation and motion models\\nconditioned on these expectable events. Together with a mapper that constructs\\na semantic spatial map on-the-fly during navigation, we formulate an end-to-end\\ndifferentiable Bayes filter and train it to identify the goal by predicting the\\nmost likely trajectory through the map according to the instructions. The\\nresulting navigation policy constitutes a new approach to instruction following\\nthat explicitly models a probability distribution over states, encoding strong\\ngeometric and algorithmic priors while enabling greater explainability. Our\\nexperiments show that our approach outperforms a strong LingUNet baseline when\\npredicting the goal location on the map. On the full VLN task, i.e. navigating\\nto the goal location, our approach achieves promising results with less\\nreliance on navigation constraints.'\n",
      " b'We consider the problem of finitely parameterized multi-armed bandits where\\nthe model of the underlying stochastic environment can be characterized based\\non a common unknown parameter. The true parameter is unknown to the learning\\nagent. However, the set of possible parameters, which is finite, is known a\\npriori. We propose an algorithm that is simple and easy to implement, which we\\ncall Finitely Parameterized Upper Confidence Bound (FP-UCB) algorithm, which\\nuses the information about the underlying parameter set for faster learning. In\\nparticular, we show that the FP-UCB algorithm achieves a bounded regret under\\nsome structural condition on the underlying parameter set. We also show that,\\nif the underlying parameter set does not satisfy the necessary structural\\ncondition, the FP-UCB algorithm achieves a logarithmic regret, but with a\\nsmaller preceding constant compared to the standard UCB algorithm. We also\\nvalidate the superior performance of the FP-UCB algorithm through extensive\\nnumerical simulations.'\n",
      " b'This paper aims at finding a method to register two different point clouds\\nconstructed by ORB-SLAM2 and OpenSfM. To do this, we post some tags with unique\\ntextures in the scene and take videos and photos of that area. Then we take\\nshort videos of only the tags to extract their features. By matching the ORB\\nfeature of the tags with their corresponding features in the scene, it is then\\npossible to localize the position of these tags both in point clouds\\nconstructed by ORB-SLAM2 and OpenSfM. Thus, the best transformation matrix\\nbetween two point clouds can be calculated, and the two point clouds can be\\naligned.'\n",
      " b'We propose to restore old photos that suffer from severe degradation through\\na deep learning approach. Unlike conventional restoration tasks that can be\\nsolved through supervised learning, the degradation in real photos is complex\\nand the domain gap between synthetic images and real old photos makes the\\nnetwork fail to generalize. Therefore, we propose a novel triplet domain\\ntranslation network by leveraging real photos along with massive synthetic\\nimage pairs. Specifically, we train two variational autoencoders (VAEs) to\\nrespectively transform old photos and clean photos into two latent spaces. And\\nthe translation between these two latent spaces is learned with synthetic\\npaired data. This translation generalizes well to real photos because the\\ndomain gap is closed in the compact latent space. Besides, to address multiple\\ndegradations mixed in one old photo, we design a global branch with apartial\\nnonlocal block targeting to the structured defects, such as scratches and dust\\nspots, and a local branch targeting to the unstructured defects, such as noises\\nand blurriness. Two branches are fused in the latent space, leading to improved\\ncapability to restore old photos from multiple defects. Furthermore, we apply\\nanother face refinement network to recover fine details of faces in the old\\nphotos, thus ultimately generating photos with enhanced perceptual quality.\\nWith comprehensive experiments, the proposed pipeline demonstrates superior\\nperformance over state-of-the-art methods as well as existing commercial tools\\nin terms of visual quality for old photos restoration.'\n",
      " b'We develop a general problem setting for training and testing the ability of\\nagents to gather information efficiently. Specifically, we present a collection\\nof tasks in which success requires searching through a partially-observed\\nenvironment, for fragments of information which can be pieced together to\\naccomplish various goals. We combine deep architectures with techniques from\\nreinforcement learning to develop agents that solve our tasks. We shape the\\nbehavior of these agents by combining extrinsic and intrinsic rewards. We\\nempirically demonstrate that these agents learn to search actively and\\nintelligently for new information to reduce their uncertainty, and to exploit\\ninformation they have already acquired.'\n",
      " b'Identification of an entity that is of interest is prominent in any\\nintelligent system. The visual intelligence of the model is enhanced when the\\ncapability of recognition is added. Several methods such as transfer learning\\nand zero shot learning help to reuse the existing models or augment the\\nexisting model to achieve improved performance at the task of object\\nrecognition. Transferred fusion learning is one such mechanism that intends to\\nuse the best of both worlds and build a model that is capable of outperforming\\nthe models involved in the system. We propose a novel mechanism to amplify the\\nprocess of transfer learning by introducing a student architecture where the\\nnetworks learn from each other.'\n",
      " b'We propose a self-supervised framework to learn scene representations from\\nvideo that are automatically delineated into objects and background. Our method\\nrelies on moving objects being equivariant with respect to their transformation\\nacross frames and the background being constant. After training, we can\\nmanipulate and render the scenes in real time to create unseen combinations of\\nobjects, transformations, and backgrounds. We show results on moving MNIST with\\nbackgrounds.'\n",
      " b\"We investigate whether post-hoc model explanations are effective for\\ndiagnosing model errors--model debugging. In response to the challenge of\\nexplaining a model's prediction, a vast array of explanation methods have been\\nproposed. Despite increasing use, it is unclear if they are effective. To\\nstart, we categorize \\\\textit{bugs}, based on their source, into:~\\\\textit{data,\\nmodel, and test-time} contamination bugs. For several explanation methods, we\\nassess their ability to: detect spurious correlation artifacts (data\\ncontamination), diagnose mislabeled training examples (data contamination),\\ndifferentiate between a (partially) re-initialized model and a trained one\\n(model contamination), and detect out-of-distribution inputs (test-time\\ncontamination). We find that the methods tested are able to diagnose a spurious\\nbackground bug, but not conclusively identify mislabeled training examples. In\\naddition, a class of methods, that modify the back-propagation algorithm are\\ninvariant to the higher layer parameters of a deep network; hence, ineffective\\nfor diagnosing model contamination. We complement our analysis with a human\\nsubject study, and find that subjects fail to identify defective models using\\nattributions, but instead rely, primarily, on model predictions. Taken\\ntogether, our results provide guidance for practitioners and researchers\\nturning to explanations as tools for model debugging.\"\n",
      " b\"Webly-supervised learning has recently emerged as an alternative paradigm to\\ntraditional supervised learning based on large-scale datasets with manual\\nannotations. The key idea is that models such as CNNs can be learned from the\\nnoisy visual data available on the web. In this work we aim to exploit web data\\nfor video understanding tasks such as action recognition and detection. One of\\nthe main problems in webly-supervised learning is cleaning the noisy labeled\\ndata from the web. The state-of-the-art paradigm relies on training a first\\nclassifier on noisy data that is then used to clean the remaining dataset. Our\\nkey insight is that this procedure biases the second classifier towards samples\\nthat the first one understands. Here we train two independent CNNs, a RGB\\nnetwork on web images and video frames and a second network using temporal\\ninformation from optical flow. We show that training the networks independently\\nis vastly superior to selecting the frames for the flow classifier by using our\\nRGB network. Moreover, we show benefits in enriching the training set with\\ndifferent data sources from heterogeneous public web databases. We demonstrate\\nthat our framework outperforms all other webly-supervised methods on two public\\nbenchmarks, UCF-101 and Thumos'14.\"\n",
      " b'The large amount of audiovisual content being shared online today has drawn\\nsubstantial attention to the prospect of audiovisual self-supervised learning.\\nRecent works have focused on each of these modalities separately, while others\\nhave attempted to model both simultaneously in a cross-modal fashion. However,\\ncomparatively little attention has been given to leveraging one modality as a\\ntraining objective to learn from the other. In this work, we propose Learning\\nvisual speech Representations from Audio via self-supervision (LiRA).\\nSpecifically, we train a ResNet+Conformer model to predict acoustic features\\nfrom unlabelled visual speech. We find that this pre-trained model can be\\nleveraged towards word-level and sentence-level lip-reading through feature\\nextraction and fine-tuning experiments. We show that our approach significantly\\noutperforms other self-supervised methods on the Lip Reading in the Wild (LRW)\\ndataset and achieves state-of-the-art performance on Lip Reading Sentences 2\\n(LRS2) using only a fraction of the total labelled data.'\n",
      " b'Learning data representations that capture task-related features, but are\\ninvariant to nuisance variations remains a key challenge in machine learning.\\nWe introduce an automated Bayesian inference framework, called AutoBayes, that\\nexplores different graphical models linking classifier, encoder, decoder,\\nestimator and adversarial network blocks to optimize nuisance-invariant machine\\nlearning pipelines. AutoBayes also enables learning disentangled\\nrepresentations, where the latent variable is split into multiple pieces to\\nimpose various relationships with the nuisance variation and task labels. We\\nbenchmark the framework on several public datasets, and provide analysis of its\\ncapability for subject-transfer learning with/without variational modeling and\\nadversarial training. We demonstrate a significant performance improvement with\\nensemble learning across explored graphical models.'\n",
      " b'While early AutoML frameworks focused on optimizing traditional ML pipelines\\nand their hyperparameters, a recent trend in AutoML is to focus on neural\\narchitecture search. In this paper, we introduce Auto-PyTorch, which brings the\\nbest of these two worlds together by jointly and robustly optimizing the\\narchitecture of networks and the training hyperparameters to enable fully\\nautomated deep learning (AutoDL). Auto-PyTorch achieves state-of-the-art\\nperformance on several tabular benchmarks by combining multi-fidelity\\noptimization with portfolio construction for warmstarting and ensembling of\\ndeep neural networks (DNNs) and common baselines for tabular data. To\\nthoroughly study our assumptions on how to design such an AutoDL system, we\\nadditionally introduce a new benchmark on learning curves for DNNs, dubbed\\nLCBench, and run extensive ablation studies of the full Auto-PyTorch on typical\\nAutoML benchmarks, eventually showing that Auto-PyTorch performs better than\\nseveral state-of-the-art competitors on average.'\n",
      " b'Softmax is widely used in neural networks for multiclass classification, gate\\nstructure and attention mechanisms. The statistical assumption that the input\\nis normal distributed supports the gradient stability of Softmax. However, when\\nused in attention mechanisms such as transformers, since the correlation scores\\nbetween embeddings are often not normally distributed, the gradient vanishing\\nproblem appears, and we prove this point through experimental confirmation. In\\nthis work, we suggest that replacing the exponential function by periodic\\nfunctions, and we delve into some potential periodic alternatives of Softmax\\nfrom the view of value and gradient. Through experiments on a simply designed\\ndemo referenced to LeViT, our method is proved to be able to alleviate the\\ngradient problem and yield substantial improvements compared to Softmax and its\\nvariants. Further, we analyze the impact of pre-normalization for Softmax and\\nour methods through mathematics and experiments. Lastly, we increase the depth\\nof the demo and prove the applicability of our method in deep structures.'\n",
      " b'In this paper, we address the Online Unsupervised Domain Adaptation (OUDA)\\nproblem, where the target data are unlabelled and arriving sequentially. The\\ntraditional methods on the OUDA problem mainly focus on transforming each\\narriving target data to the source domain, and they do not sufficiently\\nconsider the temporal coherency and accumulative statistics among the arriving\\ntarget data. We propose a multi-step framework for the OUDA problem, which\\ninstitutes a novel method to compute the mean-target subspace inspired by the\\ngeometrical interpretation on the Euclidean space. This mean-target subspace\\ncontains accumulative temporal information among the arrived target data.\\nMoreover, the transformation matrix computed from the mean-target subspace is\\napplied to the next target data as a preprocessing step, aligning the target\\ndata closer to the source domain. Experiments on four datasets demonstrated the\\ncontribution of each step in our proposed multi-step OUDA framework and its\\nperformance over previous approaches.'\n",
      " b'Image Segmentation plays an essential role in computer vision and image\\nprocessing with various applications from medical diagnosis to autonomous car\\ndriving. A lot of segmentation algorithms have been proposed for addressing\\nspecific problems. In recent years, the success of deep learning techniques has\\ntremendously influenced a wide range of computer vision areas, and the modern\\napproaches of image segmentation based on deep learning are becoming prevalent.\\nIn this article, we introduce a high-efficient development toolkit for image\\nsegmentation, named PaddleSeg. The toolkit aims to help both developers and\\nresearchers in the whole process of designing segmentation models, training\\nmodels, optimizing performance and inference speed, and deploying models.\\nCurrently, PaddleSeg supports around 20 popular segmentation models and more\\nthan 50 pre-trained models from real-time and high-accuracy levels. With\\nmodular components and backbone networks, users can easily build over one\\nhundred models for different requirements. Furthermore, we provide\\ncomprehensive benchmarks and evaluations to show that these segmentation\\nalgorithms trained on our toolkit have more competitive accuracy. Also, we\\nprovide various real industrial applications and practical cases based on\\nPaddleSeg. All codes and examples of PaddleSeg are available at\\nhttps://github.com/PaddlePaddle/PaddleSeg.'\n",
      " b'We show how we can globally edit images using textual instructions: given a\\nsource image and a textual instruction for the edit, generate a new image\\ntransformed under this instruction. To tackle this novel problem, we develop\\nthree different trainable models based on RNN and Generative Adversarial\\nNetwork (GAN). The models (bucket, filter bank, and end-to-end) differ in how\\nmuch expert knowledge is encoded, with the most general version being purely\\nend-to-end. To train these systems, we use Amazon Mechanical Turk to collect\\ntextual descriptions for around 2000 image pairs sampled from several datasets.\\nExperimental results evaluated on our dataset validate our approaches. In\\naddition, given that the filter bank model is a good compromise between\\ngenerality and performance, we investigate it further by replacing RNN with\\nGraph RNN, and show that Graph RNN improves performance. To the best of our\\nknowledge, this is the first computational photography work on global image\\nediting that is purely based on free-form textual instructions.'\n",
      " b'This paper studies the problem of developing an approximate dynamic\\nprogramming (ADP) framework for learning online the value function of an\\ninfinite-horizon optimal problem while obeying safety constraints expressed as\\ncontrol barrier functions (CBFs). Our approach is facilitated by the\\ndevelopment of a novel class of CBFs, termed Lyapunov-like CBFs (LCBFs), that\\nretain the beneficial properties of CBFs for developing minimally-invasive safe\\ncontrol policies while also possessing desirable Lyapunov-like qualities such\\nas positive semi-definiteness. We show how these LCBFs can be used to augment a\\nlearning-based control policy so as to guarantee safety and then leverage this\\napproach to develop a safe exploration framework in a model-based reinforcement\\nlearning setting. We demonstrate that our developed approach can handle more\\ngeneral safety constraints than state-of-the-art safe ADP methods through a\\nvariety of numerical examples.'\n",
      " b'This work investigates the task of unsupervised model personalization,\\nadapted to continually evolving, unlabeled local user images. We consider the\\npractical scenario where a high capacity server interacts with a myriad of\\nresource-limited edge devices, imposing strong requirements on scalability and\\nlocal data privacy. We aim to address this challenge within the continual\\nlearning paradigm and provide a novel Dual User-Adaptation framework (DUA) to\\nexplore the problem. This framework flexibly disentangles user-adaptation into\\nmodel personalization on the server and local data regularization on the user\\ndevice, with desirable properties regarding scalability and privacy\\nconstraints. First, on the server, we introduce incremental learning of\\ntask-specific expert models, subsequently aggregated using a concealed\\nunsupervised user prior. Aggregation avoids retraining, whereas the user prior\\nconceals sensitive raw user data, and grants unsupervised adaptation. Second,\\nlocal user-adaptation incorporates a domain adaptation point of view, adapting\\nregularizing batch normalization parameters to the user data. We explore\\nvarious empirical user configurations with different priors in categories and a\\ntenfold of transforms for MIT Indoor Scene recognition, and classify numbers in\\na combined MNIST and SVHN setup. Extensive experiments yield promising results\\nfor data-driven local adaptation and elicit user priors for server adaptation\\nto depend on the model rather than user data. Hence, although user-adaptation\\nremains a challenging open problem, the DUA framework formalizes a principled\\nfoundation for personalizing both on server and user device, while maintaining\\nprivacy and scalability.'\n",
      " b'Segmentation of organs of interest in 3D medical images is necessary for\\naccurate diagnosis and longitudinal studies. Though recent advances using deep\\nlearning have shown success for many segmentation tasks, large datasets are\\nrequired for high performance and the annotation process is both time consuming\\nand labor intensive. In this paper, we propose a 3D few shot segmentation\\nframework for accurate organ segmentation using limited training samples of the\\ntarget organ annotation. To achieve this, a U-Net like network is designed to\\npredict segmentation by learning the relationship between 2D slices of support\\ndata and a query image, including a bidirectional gated recurrent unit (GRU)\\nthat learns consistency of encoded features between adjacent slices. Also, we\\nintroduce a transfer learning method to adapt the characteristics of the target\\nimage and organ by updating the model before testing with arbitrary support and\\nquery data sampled from the support data. We evaluate our proposed model using\\nthree 3D CT datasets with annotations of different organs. Our model yielded\\nsignificantly improved performance over state-of-the-art few shot segmentation\\nmodels and was comparable to a fully supervised model trained with more target\\ntraining data.'\n",
      " b'Graph neural networks (GNN) have recently emerged as a vehicle for applying\\ndeep network architectures to graph and relational data. However, given the\\nincreasing size of industrial datasets, in many practical situations the\\nmessage passing computations required for sharing information across GNN layers\\nare no longer scalable. Although various sampling methods have been introduced\\nto approximate full-graph training within a tractable budget, there remain\\nunresolved complications such as high variances and limited theoretical\\nguarantees. To address these issues, we build upon existing work and treat GNN\\nneighbor sampling as a multi-armed bandit problem but with a newly-designed\\nreward function that introduces some degree of bias designed to reduce variance\\nand avoid unstable, possibly-unbounded pay outs. And unlike prior bandit-GNN\\nuse cases, the resulting policy leads to near-optimal regret while accounting\\nfor the GNN training dynamics introduced by SGD. From a practical standpoint,\\nthis translates into lower variance estimates and competitive or superior test\\naccuracy across several benchmarks.'\n",
      " b'Given a pair of images-target person and garment on another person-we\\nautomatically generate the target person in the given garment. Previous methods\\nmostly focused on texture transfer via paired data training, while overlooking\\nbody shape deformations, skin color, and seamless blending of garment with the\\nperson. This work focuses on those three components, while also not requiring\\npaired data training. We designed a pose conditioned StyleGAN2 architecture\\nwith a clothing segmentation branch that is trained on images of people wearing\\ngarments. Once trained, we propose a new layered latent space interpolation\\nmethod that allows us to preserve and synthesize skin color and target body\\nshape while transferring the garment from a different person. We demonstrate\\nresults on high resolution 512x512 images, and extensively compare to state of\\nthe art in try-on on both latent space generated and real images.'\n",
      " b'A text to image generation (T2I) model aims to generate photo-realistic\\nimages which are semantically consistent with the text descriptions. Built upon\\nthe recent advances in generative adversarial networks (GANs), existing T2I\\nmodels have made great progress. However, a close inspection of their generated\\nimages reveals two major limitations: (1) The condition batch normalization\\nmethods are applied on the whole image feature maps equally, ignoring the local\\nsemantics; (2) The text encoder is fixed during training, which should be\\ntrained with the image generator jointly to learn better text representations\\nfor image generation. To address these limitations, we propose a novel\\nframework Semantic-Spatial Aware GAN, which is trained in an end-to-end fashion\\nso that the text encoder can exploit better text information. Concretely, we\\nintroduce a novel Semantic-Spatial Aware Convolution Network, which (1) learns\\nsemantic-adaptive transformation conditioned on text to effectively fuse text\\nfeatures and image features, and (2) learns a mask map in a weakly-supervised\\nway that depends on the current text-image fusion process in order to guide the\\ntransformation spatially. Experiments on the challenging COCO and CUB bird\\ndatasets demonstrate the advantage of our method over the recent\\nstate-of-the-art approaches, regarding both visual fidelity and alignment with\\ninput text description. Code is available at\\nhttps://github.com/wtliao/text2image.'\n",
      " b'When minimizing the empirical risk in binary classification, it is a common\\npractice to replace the zero-one loss with a surrogate loss to make the\\nlearning objective feasible to optimize. Examples of well-known surrogate\\nlosses for binary classification include the logistic loss, hinge loss, and\\nsigmoid loss. It is known that the choice of a surrogate loss can highly\\ninfluence the performance of the trained classifier and therefore it should be\\ncarefully chosen. Recently, surrogate losses that satisfy a certain symmetric\\ncondition (aka., symmetric losses) have demonstrated their usefulness in\\nlearning from corrupted labels. In this article, we provide an overview of\\nsymmetric losses and their applications. First, we review how a symmetric loss\\ncan yield robust classification from corrupted labels in balanced error rate\\n(BER) minimization and area under the receiver operating characteristic curve\\n(AUC) maximization. Then, we demonstrate how the robust AUC maximization method\\ncan benefit natural language processing in the problem where we want to learn\\nonly from relevant keywords and unlabeled documents. Finally, we conclude this\\narticle by discussing future directions, including potential applications of\\nsymmetric losses for reliable machine learning and the design of non-symmetric\\nlosses that can benefit from the symmetric condition.'\n",
      " b'In this paper, a neural architecture search (NAS) framework is proposed for\\n3D medical image segmentation, to automatically optimize a neural architecture\\nfrom a large design space. Our NAS framework searches the structure of each\\nlayer including neural connectivities and operation types in both of the\\nencoder and decoder. Since optimizing over a large discrete architecture space\\nis difficult due to high-resolution 3D medical images, a novel stochastic\\nsampling algorithm based on a continuous relaxation is also proposed for\\nscalable gradient based optimization. On the 3D medical image segmentation\\ntasks with a benchmark dataset, an automatically designed architecture by the\\nproposed NAS framework outperforms the human-designed 3D U-Net, and moreover\\nthis optimized architecture is well suited to be transferred for different\\ntasks.'\n",
      " b'Optimal transport (OT)-based methods have a wide range of applications and\\nhave attracted a tremendous amount of attention in recent years. However, most\\nof the computational approaches of OT do not learn the underlying transport\\nmap. Although some algorithms have been proposed to learn this map, they rely\\non kernel-based methods, which makes them prohibitively slow when the number of\\nsamples increases. Here, we propose a way to learn an approximate transport map\\nand a parametric approximation of the Wasserstein barycenter. We build an\\napproximated transport mapping by leveraging the closed-form of Gaussian\\n(Bures-Wasserstein) transport; we compute local transport plans between matched\\npairs of the Gaussian components of each density. The learned map generalizes\\nto out-of-sample examples. We provide experimental results on simulated and\\nreal data, comparing our proposed method with other mapping estimation\\nalgorithms. Preliminary experiments suggest that our proposed method is not\\nonly faster, with a factor 80 overall running time, but it also requires fewer\\ncomponents than state-of-the-art methods to recover the support of the\\nbarycenter. From a practical standpoint, it is straightforward to implement and\\ncan be used with a conventional machine learning pipeline.'\n",
      " b'With the rapid scaling up of deep neural networks (DNNs), extensive research\\nstudies on network model compression such as weight pruning have been performed\\nfor improving deployment efficiency. This work aims to advance the compression\\nbeyond the weights to neuron activations. We propose the joint regularization\\ntechnique which simultaneously regulates the distribution of weights and\\nactivations. By distinguishing and leveraging the significance difference among\\nneuron responses and connections during learning, the jointly pruned network,\\nnamely \\\\textit{JPnet}, optimizes the sparsity of activations and weights for\\nimproving execution efficiency. The derived deep sparsification of JPnet\\nreveals more optimization space for the existing DNN accelerators dedicated for\\nsparse matrix operations. We thoroughly evaluate the effectiveness of joint\\nregularization through various network models with different activation\\nfunctions and on different datasets. With $0.4\\\\%$ degradation constraint on\\ninference accuracy, a JPnet can save $72.3\\\\% \\\\sim 98.8\\\\%$ of computation cost\\ncompared to the original dense models, with up to $5.2\\\\times$ and $12.3\\\\times$\\nreductions in activation and weight numbers, respectively.'\n",
      " b'Time series are widely used as signals in many classification/regression\\ntasks. It is ubiquitous that time series contains many missing values. Given\\nmultiple correlated time series data, how to fill in missing values and to\\npredict their class labels? Existing imputation methods often impose strong\\nassumptions of the underlying data generating process, such as linear dynamics\\nin the state space. In this paper, we propose BRITS, a novel method based on\\nrecurrent neural networks for missing value imputation in time series data. Our\\nproposed method directly learns the missing values in a bidirectional recurrent\\ndynamical system, without any specific assumption. The imputed values are\\ntreated as variables of RNN graph and can be effectively updated during the\\nbackpropagation.BRITS has three advantages: (a) it can handle multiple\\ncorrelated missing values in time series; (b) it generalizes to time series\\nwith nonlinear dynamics underlying; (c) it provides a data-driven imputation\\nprocedure and applies to general settings with missing data.We evaluate our\\nmodel on three real-world datasets, including an air quality dataset, a\\nhealth-care data, and a localization data for human activity. Experiments show\\nthat our model outperforms the state-of-the-art methods in both imputation and\\nclassification/regression accuracies.'\n",
      " b'We provide theoretical convergence guarantees on training Generative\\nAdversarial Networks (GANs) via SGD. We consider learning a target distribution\\nmodeled by a 1-layer Generator network with a non-linear activation function\\n$\\\\phi(\\\\cdot)$ parametrized by a $d \\\\times d$ weight matrix $\\\\mathbf W_*$, i.e.,\\n$f_*(\\\\mathbf x) = \\\\phi(\\\\mathbf W_* \\\\mathbf x)$.\\n  Our main result is that by training the Generator together with a\\nDiscriminator according to the Stochastic Gradient Descent-Ascent iteration\\nproposed by Goodfellow et al. yields a Generator distribution that approaches\\nthe target distribution of $f_*$. Specifically, we can learn the target\\ndistribution within total-variation distance $\\\\epsilon$ using $\\\\tilde\\nO(d^2/\\\\epsilon^2)$ samples which is (near-)information theoretically optimal.\\n  Our results apply to a broad class of non-linear activation functions $\\\\phi$,\\nincluding ReLUs and is enabled by a connection with truncated statistics and an\\nappropriate design of the Discriminator network. Our approach relies on a\\nbilevel optimization framework to show that vanilla SGDA works.'\n",
      " b'Neural Architecture Search (NAS) has shown great potentials in automatically\\ndesigning scalable network architectures for dense image predictions. However,\\nexisting NAS algorithms usually compromise on restricted search space and\\nsearch on proxy task to meet the achievable computational demands. To allow as\\nwide as possible network architectures and avoid the gap between target and\\nproxy dataset, we propose a Densely Connected NAS (DCNAS) framework, which\\ndirectly searches the optimal network structures for the multi-scale\\nrepresentations of visual information, over a large-scale target dataset.\\nSpecifically, by connecting cells with each other using learnable weights, we\\nintroduce a densely connected search space to cover an abundance of mainstream\\nnetwork designs. Moreover, by combining both path-level and channel-level\\nsampling strategies, we design a fusion module to reduce the memory consumption\\nof ample search space. We demonstrate that the architecture obtained from our\\nDCNAS algorithm achieves state-of-the-art performances on public semantic image\\nsegmentation benchmarks, including 84.3% on Cityscapes, and 86.9% on PASCAL VOC\\n2012. We also retain leading performances when evaluating the architecture on\\nthe more challenging ADE20K and Pascal Context dataset.'\n",
      " b'Bottom-up human pose estimation methods have difficulties in predicting the\\ncorrect pose for small persons due to challenges in scale variation. In this\\npaper, we present HigherHRNet: a novel bottom-up human pose estimation method\\nfor learning scale-aware representations using high-resolution feature\\npyramids. Equipped with multi-resolution supervision for training and\\nmulti-resolution aggregation for inference, the proposed approach is able to\\nsolve the scale variation challenge in bottom-up multi-person pose estimation\\nand localize keypoints more precisely, especially for small person. The feature\\npyramid in HigherHRNet consists of feature map outputs from HRNet and upsampled\\nhigher-resolution outputs through a transposed convolution. HigherHRNet\\noutperforms the previous best bottom-up method by 2.5% AP for medium person on\\nCOCO test-dev, showing its effectiveness in handling scale variation.\\nFurthermore, HigherHRNet achieves new state-of-the-art result on COCO test-dev\\n(70.5% AP) without using refinement or other post-processing techniques,\\nsurpassing all existing bottom-up methods. HigherHRNet even surpasses all\\ntop-down methods on CrowdPose test (67.6% AP), suggesting its robustness in\\ncrowded scene. The code and models are available at\\nhttps://github.com/HRNet/Higher-HRNet-Human-Pose-Estimation.'\n",
      " b'Recently, many methods have been proposed for object detection. They cannot\\ndetect objects by semantic features, adaptively. In this work, according to\\nchannel and spatial attention mechanisms, we mainly analyze that different\\nmethods detect objects adaptively. Some state-of-the-art detectors combine\\ndifferent feature pyramids with many mechanisms to enhance multi-level semantic\\ninformation. However, they require more cost. This work addresses that by an\\nanchor-free detector with shared encoder-decoder with attention mechanism,\\nextracting shared features. We consider features of different levels from\\nbackbone (e.g., ResNet-50) as the basis features. Then, we feed the features\\ninto a simple module, followed by a detector header to detect objects.\\nMeantime, we use the semantic features to revise geometric locations, and the\\ndetector is a pixel-semantic revising of position. More importantly, this work\\nanalyzes the impact of different pooling strategies (e.g., mean, maximum or\\nminimum) on multi-scale objects, and finds the minimum pooling improve\\ndetection performance on small objects better. Compared with state-of-the-art\\nMNC based on ResNet-101 for the standard MSCOCO 2014 baseline, our method\\nimproves detection AP of 3.8%.'\n",
      " b'Online anomaly detection of time-series data is an important and challenging\\ntask in machine learning. Gaussian processes (GPs) are powerful and flexible\\nmodels for modeling time-series data. However, the high time complexity of GPs\\nlimits their applications in online anomaly detection. Attributed to some\\ninternal or external changes, concept drift usually occurs in time-series data,\\nwhere the characteristics of data and meanings of abnormal behaviors alter over\\ntime. Online anomaly detection methods should have the ability to adapt to\\nconcept drift. Motivated by the above facts, this paper proposes the method of\\nsparse Gaussian processes with Q-function (SGP-Q). The SGP-Q employs sparse\\nGaussian processes (SGPs) whose time complexity is lower than that of GPs, thus\\nsignificantly speeding up online anomaly detection. By using Q-function\\nproperly, the SGP-Q can adapt to concept drift well. Moreover, the SGP-Q makes\\nuse of few abnormal data in the training data by its strategy of updating\\ntraining data, resulting in more accurate sparse Gaussian process regression\\nmodels and better anomaly detection results. We evaluate the SGP-Q on various\\nartificial and real-world datasets. Experimental results validate the\\neffectiveness of the SGP-Q.'\n",
      " b\"Sketches are a medium to convey a visual scene from an individual's creative\\nperspective. The addition of color substantially enhances the overall\\nexpressivity of a sketch. This paper proposes two methods to mimic human-drawn\\ncolored sketches by utilizing the Contour Drawing Dataset. Our first approach\\nrenders colored outline sketches by applying image processing techniques aided\\nby k-means color clustering. The second method uses a generative adversarial\\nnetwork to develop a model that can generate colored sketches from previously\\nunobserved images. We assess the results obtained through quantitative and\\nqualitative evaluations.\"\n",
      " b'Transfer learning aims to exploit pre-trained models for more efficient\\nfollow-up training on wide range of downstream tasks and datasets, enabling\\nsuccessful training also on small data. Recently, strong improvement was shown\\nfor transfer learning and model generalization when increasing model, data and\\ncompute budget scale in the pre-training. To compare effect of scale both in\\nintra- and inter-domain full and few-shot transfer, in this study we combine\\nfor the first time large openly available medical X-Ray chest imaging datasets\\nto reach a dataset scale comparable to ImageNet-1k. We then conduct\\npre-training and transfer to different natural or medical targets while varying\\nnetwork size and source data scale and domain, being either large natural\\n(ImageNet-1k/21k) or large medical chest X-Ray datasets. We observe strong\\nimprovement due to larger pre-training scale for intra-domain natural-natural\\nand medical-medical transfer. For inter-domain natural-medical transfer, we\\nfind improvements due to larger pre-training scale on larger X-Ray targets in\\nfull shot regime, while for smaller targets and for few-shot regime the\\nimprovement is not visible. Remarkably, large networks pre-trained on very\\nlarge natural ImageNet-21k are as good or better than networks pre-trained on\\nlargest available medical X-Ray data when performing transfer to large X-Ray\\ntargets. We conclude that high quality models for inter-domain transfer can be\\nalso obtained by substantially increasing scale of model and generic natural\\nsource data, removing necessity for large domain-specific medical source data\\nin the pre-training. Code is available at:\\n\\\\url{https://github.com/SLAMPAI/large-scale-pretraining-transfer}}'\n",
      " b\"Much of the recent progress made in image classification research can be\\ncredited to training procedure refinements, such as changes in data\\naugmentations and optimization methods. In the literature, however, most\\nrefinements are either briefly mentioned as implementation details or only\\nvisible in source code. In this paper, we will examine a collection of such\\nrefinements and empirically evaluate their impact on the final model accuracy\\nthrough ablation study. We will show that, by combining these refinements\\ntogether, we are able to improve various CNN models significantly. For example,\\nwe raise ResNet-50's top-1 validation accuracy from 75.3% to 79.29% on\\nImageNet. We will also demonstrate that improvement on image classification\\naccuracy leads to better transfer learning performance in other application\\ndomains such as object detection and semantic segmentation.\"\n",
      " b'Many studies have been conducted so far to build systems for recommending\\nfashion items and outfits. Although they achieve good performances in their\\nrespective tasks, most of them cannot explain their judgments to the users,\\nwhich compromises their usefulness. Toward explainable fashion recommendation,\\nthis study proposes a system that is able not only to provide a goodness score\\nfor an outfit but also to explain the score by providing reason behind it. For\\nthis purpose, we propose a method for quantifying how influential each feature\\nof each item is to the score. Using this influence value, we can identify which\\nitem and what feature make the outfit good or bad. We represent the image of\\neach item with a combination of human-interpretable features, and thereby the\\nidentification of the most influential item-feature pair gives useful\\nexplanation of the output score. To evaluate the performance of this approach,\\nwe design an experiment that can be performed without human annotation; we\\nreplace a single item-feature pair in an outfit so that the score will\\ndecrease, and then we test if the proposed method can detect the replaced item\\ncorrectly using the above influence values. The experimental results show that\\nthe proposed method can accurately detect bad items in outfits lowering their\\nscores.'\n",
      " b'Graph convolutional neural networks (GCNNs) are nonlinear processing tools to\\nlearn representations from network data. A key property of GCNNs is their\\nstability to graph perturbations. Current analysis considers deterministic\\nperturbations but fails to provide relevant insights when topological changes\\nare random. This paper investigates the stability of GCNNs to stochastic graph\\nperturbations induced by link losses. In particular, it proves the expected\\noutput difference between the GCNN over random perturbed graphs and the GCNN\\nover the nominal graph is upper bounded by a factor that is linear in the link\\nloss probability. We perform the stability analysis in the graph spectral\\ndomain such that the result holds uniformly for any graph. This result also\\nshows the role of the nonlinearity and the architecture width and depth, and\\nallows identifying handle to improve the GCNN robustness. Numerical simulations\\non source localization and robot swarm control corroborate our theoretical\\nfindings.'\n",
      " b'Deep Learning (DL) is the most widely used tool in the contemporary field of\\ncomputer vision. Its ability to accurately solve complex problems is employed\\nin vision research to learn deep neural models for a variety of tasks,\\nincluding security critical applications. However, it is now known that DL is\\nvulnerable to adversarial attacks that can manipulate its predictions by\\nintroducing visually imperceptible perturbations in images and videos. Since\\nthe discovery of this phenomenon in 2013~[1], it has attracted significant\\nattention of researchers from multiple sub-fields of machine intelligence. In\\n[2], we reviewed the contributions made by the computer vision community in\\nadversarial attacks on deep learning (and their defenses) until the advent of\\nyear 2018. Many of those contributions have inspired new directions in this\\narea, which has matured significantly since witnessing the first generation\\nmethods. Hence, as a legacy sequel of [2], this literature review focuses on\\nthe advances in this area since 2018. To ensure authenticity, we mainly\\nconsider peer-reviewed contributions published in the prestigious sources of\\ncomputer vision and machine learning research. Besides a comprehensive\\nliterature review, the article also provides concise definitions of technical\\nterminologies for non-experts in this domain. Finally, this article discusses\\nchallenges and future outlook of this direction based on the literature\\nreviewed herein and [2].'\n",
      " b'The attribution method provides a direction for interpreting opaque neural\\nnetworks in a visual way by identifying and visualizing the input\\nregions/pixels that dominate the output of a network. Regarding the attribution\\nmethod for visually explaining video understanding networks, it is challenging\\nbecause of the unique spatiotemporal dependencies existing in video inputs and\\nthe special 3D convolutional or recurrent structures of video understanding\\nnetworks. However, most existing attribution methods focus on explaining\\nnetworks taking a single image as input and a few works specifically devised\\nfor video attribution come short of dealing with diversified structures of\\nvideo understanding networks. In this paper, we investigate a generic\\nperturbation-based attribution method that is compatible with diversified video\\nunderstanding networks. Besides, we propose a novel regularization term to\\nenhance the method by constraining the smoothness of its attribution results in\\nboth spatial and temporal dimensions. In order to assess the effectiveness of\\ndifferent video attribution methods without relying on manual judgement, we\\nintroduce reliable objective metrics which are checked by a newly proposed\\nreliability measurement. We verified the effectiveness of our method by both\\nsubjective and objective evaluation and comparison with multiple significant\\nattribution methods.'\n",
      " b'The demosaicking provokes the spatial and color correlation of noise, which\\nis afterwards enhanced by the imaging pipeline. The correct removal previous or\\nsimultaneously with the demosaicking process is not usually considered in the\\nliterature. We present a novel imaging chain including a denoising of the Bayer\\nCFA and a demosaicking method for image sequences. The proposed algorithm uses\\na spatio-temporal patch method for the noise removal and demosaicking of the\\nCFA. The experimentation, including real examples, illustrates the superior\\nperformance of the proposed chain, avoiding the creation of artifacts and\\ncolored spots in the final image.'\n",
      " b'Tunnel CCTVs are installed to low height and long-distance interval. However,\\nbecause of the limitation of installation height, severe perspective effect in\\ndistance occurs, and it is almost impossible to detect vehicles in far distance\\nfrom the CCTV in the existing tunnel CCTV-based accident detection system\\n(Pflugfelder 2005). To overcome the limitation, a vehicle object is detected\\nthrough an object detection algorithm based on an inverse perspective transform\\nby re-setting the region of interest (ROI). It can detect vehicles that are far\\naway from the CCTV. To verify this process, this paper creates each dataset\\nconsisting of images and bounding boxes based on the original and warped images\\nof the CCTV at the same time, and then compares performance of the deep\\nlearning object detection models trained with the two datasets. As a result,\\nthe model that trained the warped image was able to detect vehicle objects more\\naccurately at the position far from the CCTV compared to the model that trained\\nthe original image.'\n",
      " b'Most state-of-the-art object detection systems follow an anchor-based\\ndiagram. Anchor boxes are densely proposed over the images and the network is\\ntrained to predict the boxes position offset as well as the classification\\nconfidence. Existing systems pre-define anchor box shapes and sizes and ad-hoc\\nheuristic adjustments are used to define the anchor configurations. However,\\nthis might be sub-optimal or even wrong when a new dataset or a new model is\\nadopted. In this paper, we study the problem of automatically optimizing anchor\\nboxes for object detection. We first demonstrate that the number of anchors,\\nanchor scales and ratios are crucial factors for a reliable object detection\\nsystem. By carefully analyzing the existing bounding box patterns on the\\nfeature hierarchy, we design a flexible and tight hyper-parameter space for\\nanchor configurations. Then we propose a novel hyper-parameter optimization\\nmethod named AABO to determine more appropriate anchor boxes for a certain\\ndataset, in which Bayesian Optimization and subsampling method are combined to\\nachieve precise and efficient anchor configuration optimization. Experiments\\ndemonstrate the effectiveness of our proposed method on different detectors and\\ndatasets, e.g. achieving around 2.4% mAP improvement on COCO, 1.6% on ADE and\\n1.5% on VG, and the optimal anchors can bring 1.4% to 2.4% mAP improvement on\\nSOTA detectors by only optimizing anchor configurations, e.g. boosting Mask\\nRCNN from 40.3% to 42.3%, and HTC detector from 46.8% to 48.2%.'\n",
      " b'Learning good interventions in a causal graph can be modelled as a stochastic\\nmulti-armed bandit problem with side-information. First, we study this problem\\nwhen interventions are more expensive than observations and a budget is\\nspecified. If there are no backdoor paths from an intervenable node to the\\nreward node then we propose an algorithm to minimize simple regret that\\noptimally trades-off observations and interventions based on the cost of\\nintervention. We also propose an algorithm that accounts for the cost of\\ninterventions, utilizes causal side-information, and minimizes the expected\\ncumulative regret without exceeding the budget. Our cumulative-regret\\nminimization algorithm performs better than standard algorithms that do not\\ntake side-information into account. Finally, we study the problem of learning\\nbest interventions without budget constraint in general graphs and give an\\nalgorithm that achieves constant expected cumulative regret in terms of the\\ninstance parameters when the parent distribution of the reward variable for\\neach intervention is known. Our results are experimentally validated and\\ncompared to the best-known bounds in the current literature.'\n",
      " b'Autoregressive models and their sequential factorization of the data\\nlikelihood have recently demonstrated great potential for image representation\\nand synthesis. Nevertheless, they incorporate image context in a linear 1D\\norder by attending only to previously synthesized image patches above or to the\\nleft. Not only is this unidirectional, sequential bias of attention unnatural\\nfor images as it disregards large parts of a scene until synthesis is almost\\ncomplete. It also processes the entire image on a single scale, thus ignoring\\nmore global contextual information up to the gist of the entire scene. As a\\nremedy we incorporate a coarse-to-fine hierarchy of context by combining the\\nautoregressive formulation with a multinomial diffusion process: Whereas a\\nmultistage diffusion process successively removes information to coarsen an\\nimage, we train a (short) Markov chain to invert this process. In each stage,\\nthe resulting autoregressive ImageBART model progressively incorporates context\\nfrom previous stages in a coarse-to-fine manner. Experiments show greatly\\nimproved image modification capabilities over autoregressive models while also\\nproviding high-fidelity image generation, both of which are enabled through\\nefficient training in a compressed latent space. Specifically, our approach can\\ntake unrestricted, user-provided masks into account to perform local image\\nediting. Thus, in contrast to pure autoregressive models, it can solve\\nfree-form image inpainting and, in the case of conditional models, local,\\ntext-guided image modification without requiring mask-specific training.'\n",
      " b'Visual Question Answering (VQA) is challenging due to the complex cross-modal\\nrelations. It has received extensive attention from the research community.\\nFrom the human perspective, to answer a visual question, one needs to read the\\nquestion and then refer to the image to generate an answer. This answer will\\nthen be checked against the question and image again for the final\\nconfirmation. In this paper, we mimic this process and propose a fully\\nattention based VQA architecture. Moreover, an answer-checking module is\\nproposed to perform a unified attention on the jointly answer, question and\\nimage representation to update the answer. This mimics the human answer\\nchecking process to consider the answer in the context. With answer-checking\\nmodules and transferred BERT layers, our model achieves the state-of-the-art\\naccuracy 71.57\\\\% using fewer parameters on VQA-v2.0 test-standard split.'\n",
      " b'A relatively new set of transport-based transforms (CDT, R-CDT, LOT) have\\nshown their strength and great potential in various image and data processing\\ntasks such as parametric signal estimation, classification, cancer detection\\namong many others. It is hence worthwhile to elucidate some of the mathematical\\nproperties that explain the successes of these transforms when they are used as\\ntools in data analysis, signal processing or data classification. In\\nparticular, we give conditions under which classes of signals that are created\\nby algebraic generative models are transformed into convex sets by the\\ntransport transforms. Such convexification of the classes simplify the\\nclassification and other data analysis and processing problems when viewed in\\nthe transform domain. More specifically, we study the extent and limitation of\\nthe convexification ability of these transforms under an algebraic generative\\nmodeling framework. We hope that this paper will serve as an introduction to\\nthese transforms and will encourage mathematicians and other researchers to\\nfurther explore the theoretical underpinnings and algorithmic tools that will\\nhelp understand the successes of these transforms and lay the groundwork for\\nfurther successful applications.'\n",
      " b'Reinforcement learning (RL) algorithms typically start tabula rasa, without\\nany prior knowledge of the environment, and without any prior skills. This\\nhowever often leads to low sample efficiency, requiring a large amount of\\ninteraction with the environment. This is especially true in a lifelong\\nlearning setting, in which the agent needs to continually extend its\\ncapabilities. In this paper, we examine how a pre-trained task-independent\\nlanguage model can make a goal-conditional RL agent more sample efficient. We\\ndo this by facilitating transfer learning between different related tasks. We\\nexperimentally demonstrate our approach on a set of object navigation tasks.'\n",
      " b'Single-image super-resolution (SR) and multi-frame SR are two ways to super\\nresolve low-resolution images. Single-Image SR generally handles each image\\nindependently, but ignores the temporal information implied in continuing\\nframes. Multi-frame SR is able to model the temporal dependency via capturing\\nmotion information. However, it relies on neighbouring frames which are not\\nalways available in the real world. Meanwhile, slight camera shake easily\\ncauses heavy motion blur on long-distance-shot low-resolution images. To\\naddress these problems, a Blind Motion Deblurring Super-Reslution Networks,\\nBMDSRNet, is proposed to learn dynamic spatio-temporal information from single\\nstatic motion-blurred images. Motion-blurred images are the accumulation over\\ntime during the exposure of cameras, while the proposed BMDSRNet learns the\\nreverse process and uses three-streams to learn Bidirectional spatio-temporal\\ninformation based on well designed reconstruction loss functions to recover\\nclean high-resolution images. Extensive experiments demonstrate that the\\nproposed BMDSRNet outperforms recent state-of-the-art methods, and has the\\nability to simultaneously deal with image deblurring and SR.'\n",
      " b'Answering complex logical queries on large-scale incomplete knowledge graphs\\n(KGs) is a fundamental yet challenging task. Recently, a promising approach to\\nthis problem has been to embed KG entities as well as the query into a vector\\nspace such that entities that answer the query are embedded close to the query.\\nHowever, prior work models queries as single points in the vector space, which\\nis problematic because a complex query represents a potentially large set of\\nits answer entities, but it is unclear how such a set can be represented as a\\nsingle point. Furthermore, prior work can only handle queries that use\\nconjunctions ($\\\\wedge$) and existential quantifiers ($\\\\exists$). Handling\\nqueries with logical disjunctions ($\\\\vee$) remains an open problem. Here we\\npropose query2box, an embedding-based framework for reasoning over arbitrary\\nqueries with $\\\\wedge$, $\\\\vee$, and $\\\\exists$ operators in massive and\\nincomplete KGs. Our main insight is that queries can be embedded as boxes\\n(i.e., hyper-rectangles), where a set of points inside the box corresponds to a\\nset of answer entities of the query. We show that conjunctions can be naturally\\nrepresented as intersections of boxes and also prove a negative result that\\nhandling disjunctions would require embedding with dimension proportional to\\nthe number of KG entities. However, we show that by transforming queries into a\\nDisjunctive Normal Form, query2box is capable of handling arbitrary logical\\nqueries with $\\\\wedge$, $\\\\vee$, $\\\\exists$ in a scalable manner. We demonstrate\\nthe effectiveness of query2box on three large KGs and show that query2box\\nachieves up to 25% relative improvement over the state of the art.'\n",
      " b\"Deep Reinforcement Learning (DRL) has shown outstanding performance on\\ninducing effective action policies that maximize expected long-term return on\\nmany complex tasks. Much of DRL work has been focused on sequences of events\\nwith discrete time steps and ignores the irregular time intervals between\\nconsecutive events. Given that in many real-world domains, data often consists\\nof temporal sequences with irregular time intervals, and it is important to\\nconsider the time intervals between temporal events to capture latent\\nprogressive patterns of states. In this work, we present a general Time-Aware\\nRL framework: Time-aware Q-Networks (TQN), which takes into account physical\\ntime intervals within a deep RL framework. TQN deals with time irregularity\\nfrom two aspects: 1) elapsed time in the past and an expected next observation\\ntime for time-aware state approximation, and 2) action time window for the\\nfuture for time-aware discounting of rewards. Experimental results show that by\\ncapturing the underlying structures in the sequences with time irregularities\\nfrom both aspects, TQNs significantly outperform DQN in four types of contexts\\nwith irregular time intervals. More specifically, our results show that in\\nclassic RL tasks such as CartPole and MountainCar and Atari benchmark with\\nrandomly segmented time intervals, time-aware discounting alone is more\\nimportant while in the real-world tasks such as nuclear reactor operation and\\nseptic patient treatment with intrinsic time intervals, both time-aware state\\nand time-aware discounting are crucial. Moreover, to improve the agent's\\nlearning capacity, we explored three boosting methods: Double networks, Dueling\\nnetworks, and Prioritized Experience Replay, and our results show that for the\\ntwo real-world tasks, combining all three boosting methods with TQN is\\nespecially effective.\"\n",
      " b'Magnetic resonance imaging (MRI) reconstruction is an active inverse problem\\nwhich can be addressed by conventional compressed sensing (CS) MRI algorithms\\nthat exploit the sparse nature of MRI in an iterative optimization-based\\nmanner. However, two main drawbacks of iterative optimization-based CSMRI\\nmethods are time-consuming and are limited in model capacity. Meanwhile, one\\nmain challenge for recent deep learning-based CSMRI is the trade-off between\\nmodel performance and network size. To address the above issues, we develop a\\nnew multi-scale dilated network for MRI reconstruction with high speed and\\noutstanding performance. Comparing to convolutional kernels with same receptive\\nfields, dilated convolutions reduce network parameters with smaller kernels and\\nexpand receptive fields of kernels to obtain almost same information. To\\nmaintain the abundance of features, we present global and local residual\\nlearnings to extract more image edges and details. Then we utilize\\nconcatenation layers to fuse multi-scale features and residual learnings for\\nbetter reconstruction. Compared with several non-deep and deep learning CSMRI\\nalgorithms, the proposed method yields better reconstruction accuracy and\\nnoticeable visual improvements. In addition, we perform the noisy setting to\\nverify the model stability, and then extend the proposed model on a MRI\\nsuper-resolution task.'\n",
      " b'Analysis of microscopy images can provide insight into many biological\\nprocesses. One particularly challenging problem is cell nuclear segmentation in\\nhighly anisotropic and noisy 3D image data. Manually localizing and segmenting\\neach and every cell nuclei is very time consuming, which remains a bottleneck\\nin large scale biological experiments. In this work we present a tool for\\nautomated segmentation of cell nuclei from 3D fluorescent microscopic data. Our\\ntool is based on state-of-the-art image processing and machine learning\\ntechniques and supports a friendly graphical user interface (GUI). We show that\\nour tool is as accurate as manual annotation but greatly reduces the time for\\nthe registration.'\n",
      " b'Many countries are now experiencing the third wave of the COVID-19 pandemic\\nstraining the healthcare resources with an acute shortage of hospital beds and\\nventilators for the critically ill patients. This situation is especially worse\\nin India with the second largest load of COVID-19 cases and a relatively\\nresource-scarce medical infrastructure. Therefore, it becomes essential to\\ntriage the patients based on the severity of their disease and devote resources\\ntowards critically ill patients. Yan et al. 1 have published a very pertinent\\nresearch that uses Machine learning (ML) methods to predict the outcome of\\nCOVID-19 patients based on their clinical parameters at the day of admission.\\nThey used the XGBoost algorithm, a type of ensemble model, to build the\\nmortality prediction model. The final classifier is built through the\\nsequential addition of multiple weak classifiers. The clinically operable\\ndecision rule was obtained from a \\'single-tree XGBoost\\' and used lactic\\ndehydrogenase (LDH), lymphocyte and high-sensitivity C-reactive protein\\n(hs-CRP) values. This decision tree achieved a 100% survival prediction and 81%\\nmortality prediction. However, these models have several technical challenges\\nand do not provide an out of the box solution that can be deployed for other\\npopulations as has been reported in the \"Matters Arising\" section of Yan et al.\\nHere, we show the limitations of this model by deploying it on one of the\\nlargest datasets of COVID-19 patients containing detailed clinical parameters\\ncollected from India.'\n",
      " b'Image captioning using Encoder-Decoder based approach where CNN is used as\\nthe Encoder and sequence generator like RNN as Decoder has proven to be very\\neffective. However, this method has a drawback that is sequence needs to be\\nprocessed in order. To overcome this drawback some researcher has utilized the\\nTransformer model to generate captions from images using English datasets.\\nHowever, none of them generated captions in Bengali using the transformer\\nmodel. As a result, we utilized three different Bengali datasets to generate\\nBengali captions from images using the Transformer model. Additionally, we\\ncompared the performance of the transformer-based model with a visual\\nattention-based Encoder-Decoder approach. Finally, we compared the result of\\nthe transformer-based model with other models that employed different Bengali\\nimage captioning datasets.'\n",
      " b'We provide an algorithm that achieves the optimal regret rate in an unknown\\nweakly communicating Markov Decision Process (MDP). The algorithm proceeds in\\nepisodes where, in each episode, it picks a policy using regularization based\\non the span of the optimal bias vector. For an MDP with S states and A actions\\nwhose optimal bias vector has span bounded by H, we show a regret bound of\\n~O(HSpAT). We also relate the span to various diameter-like quantities\\nassociated with the MDP, demonstrating how our results improve on previous\\nregret bounds.'\n",
      " b'Conditional generative models enjoy remarkable progress over the past few\\nyears. One of the popular conditional models is Auxiliary Classifier GAN\\n(AC-GAN), which generates highly discriminative images by extending the loss\\nfunction of GAN with an auxiliary classifier. However, the diversity of the\\ngenerated samples by AC-GAN tends to decrease as the number of classes\\nincreases, hence limiting its power on large-scale data. In this paper, we\\nidentify the source of the low diversity issue theoretically and propose a\\npractical solution to solve the problem. We show that the auxiliary classifier\\nin AC-GAN imposes perfect separability, which is disadvantageous when the\\nsupports of the class distributions have significant overlap. To address the\\nissue, we propose Twin Auxiliary Classifiers Generative Adversarial Net\\n(TAC-GAN) that further benefits from a new player that interacts with other\\nplayers (the generator and the discriminator) in GAN. Theoretically, we\\ndemonstrate that TAC-GAN can effectively minimize the divergence between the\\ngenerated and real-data distributions. Extensive experimental results show that\\nour TAC-GAN can successfully replicate the true data distributions on simulated\\ndata, and significantly improves the diversity of class-conditional image\\ngeneration on real datasets.'\n",
      " b'We show that the popular reinforcement learning (RL) strategy of estimating\\nthe state-action value (Q-function) by minimizing the mean squared Bellman\\nerror leads to a regression problem with confounding, the inputs and output\\nnoise being correlated. Hence, direct minimization of the Bellman error can\\nresult in significantly biased Q-function estimates. We explain why fixing the\\ntarget Q-network in Deep Q-Networks and Fitted Q Evaluation provides a way of\\novercoming this confounding, thus shedding new light on this popular but not\\nwell understood trick in the deep RL literature. An alternative approach to\\naddress confounding is to leverage techniques developed in the causality\\nliterature, notably instrumental variables (IV). We bring together here the\\nliterature on IV and RL by investigating whether IV approaches can lead to\\nimproved Q-function estimates. This paper analyzes and compares a wide range of\\nrecent IV methods in the context of offline policy evaluation (OPE), where the\\ngoal is to estimate the value of a policy using logged data only. By applying\\ndifferent IV techniques to OPE, we are not only able to recover previously\\nproposed OPE methods such as model-based techniques but also to obtain\\ncompetitive new techniques. We find empirically that state-of-the-art OPE\\nmethods are closely matched in performance by some IV methods such as AGMM,\\nwhich were not developed for OPE. We open-source all our code and datasets at\\nhttps://github.com/liyuan9988/IVOPEwithACME.'\n",
      " b\"Deep Q-Learning (DQL), a family of temporal difference algorithms for\\ncontrol, employs three techniques collectively known as the `deadly triad' in\\nreinforcement learning: bootstrapping, off-policy learning, and function\\napproximation. Prior work has demonstrated that together these can lead to\\ndivergence in Q-learning algorithms, but the conditions under which divergence\\noccurs are not well-understood. In this note, we give a simple analysis based\\non a linear approximation to the Q-value updates, which we believe provides\\ninsight into divergence under the deadly triad. The central point in our\\nanalysis is to consider when the leading order approximation to the deep-Q\\nupdate is or is not a contraction in the sup norm. Based on this analysis, we\\ndevelop an algorithm which permits stable deep Q-learning for continuous\\ncontrol without any of the tricks conventionally used (such as target networks,\\nadaptive gradient optimizers, or using multiple Q functions). We demonstrate\\nthat our algorithm performs above or near state-of-the-art on standard MuJoCo\\nbenchmarks from the OpenAI Gym.\"\n",
      " b'Imitation learning, followed by reinforcement learning algorithms, is a\\npromising paradigm to solve complex control tasks sample-efficiently. However,\\nlearning from demonstrations often suffers from the covariate shift problem,\\nwhich results in cascading errors of the learned policy. We introduce a notion\\nof conservatively-extrapolated value functions, which provably lead to policies\\nwith self-correction. We design an algorithm Value Iteration with Negative\\nSampling (VINS) that practically learns such value functions with conservative\\nextrapolation. We show that VINS can correct mistakes of the behavioral cloning\\npolicy on simulated robotics benchmark tasks. We also propose the algorithm of\\nusing VINS to initialize a reinforcement learning algorithm, which is shown to\\noutperform significantly prior works in sample efficiency.'\n",
      " b'We introduce the concept of \"dynamic image\", a novel compact representation\\nof videos useful for video analysis, particularly in combination with\\nconvolutional neural networks (CNNs). A dynamic image encodes temporal data\\nsuch as RGB or optical flow videos by using the concept of `rank pooling\\'. The\\nidea is to learn a ranking machine that captures the temporal evolution of the\\ndata and to use the parameters of the latter as a representation. When a linear\\nranking machine is used, the resulting representation is in the form of an\\nimage, which we call dynamic because it summarizes the video dynamics in\\naddition of appearance. This is a powerful idea because it allows to convert\\nany video to an image so that existing CNN models pre-trained for the analysis\\nof still images can be immediately extended to videos. We also present an\\nefficient and effective approximate rank pooling operator, accelerating\\nstandard rank pooling algorithms by orders of magnitude, and formulate that as\\na CNN layer. This new layer allows generalizing dynamic images to dynamic\\nfeature maps. We demonstrate the power of the new representations on standard\\nbenchmarks in action recognition achieving state-of-the-art performance.'\n",
      " b'In this paper, we propose a distributed zeroth-order policy optimization\\nmethod for Multi-Agent Reinforcement Learning (MARL). Existing MARL algorithms\\noften assume that every agent can observe the states and actions of all the\\nother agents in the network. This can be impractical in large-scale problems,\\nwhere sharing the state and action information with multi-hop neighbors may\\nincur significant communication overhead. The advantage of the proposed\\nzeroth-order policy optimization method is that it allows the agents to compute\\nthe local policy gradients needed to update their local policy functions using\\nlocal estimates of the global accumulated rewards that depend on partial state\\nand action information only and can be obtained using consensus. Specifically,\\nto calculate the local policy gradients, we develop a new distributed\\nzeroth-order policy gradient estimator that relies on one-point\\nresidual-feedback which, compared to existing zeroth-order estimators that also\\nrely on one-point feedback, significantly reduces the variance of the policy\\ngradient estimates improving, in this way, the learning performance. We show\\nthat the proposed distributed zeroth-order policy optimization method with\\nconstant stepsize converges to a neighborhood of the global optimal policy that\\ndepends on the number of consensus steps used to calculate the local estimates\\nof the global accumulated rewards. Moreover, we provide numerical experiments\\nthat demonstrate that our new zeroth-order policy gradient estimator is more\\nsample-efficient compared to other existing one-point estimators.'\n",
      " b'Recently, semi-supervised learning methods based on generative adversarial\\nnetworks (GANs) have received much attention. Among them, two distinct\\napproaches have achieved competitive results on a variety of benchmark\\ndatasets. Bad GAN learns a classifier with unrealistic samples distributed on\\nthe complement of the support of the input data. Conversely, Triple GAN\\nconsists of a three-player game that tries to leverage good generated samples\\nto boost classification results. In this paper, we perform a comprehensive\\ncomparison of these two approaches on different benchmark datasets. We\\ndemonstrate their different properties on image generation, and sensitivity to\\nthe amount of labeled data provided. By comprehensively comparing these two\\nmethods, we hope to shed light on the future of GAN-based semi-supervised\\nlearning.'\n",
      " b'Graph deep learning has recently emerged as a powerful ML concept allowing to\\ngeneralize successful deep neural architectures to non-Euclidean structured\\ndata. Such methods have shown promising results on a broad spectrum of\\napplications ranging from social science, biomedicine, and particle physics to\\ncomputer vision, graphics, and chemistry. One of the limitations of the\\nmajority of the current graph neural network architectures is that they are\\noften restricted to the transductive setting and rely on the assumption that\\nthe underlying graph is known and fixed. In many settings, such as those\\narising in medical and healthcare applications, this assumption is not\\nnecessarily true since the graph may be noisy, partially- or even completely\\nunknown, and one is thus interested in inferring it from the data. This is\\nespecially important in inductive settings when dealing with nodes not present\\nin the graph at training time. Furthermore, sometimes such a graph itself may\\nconvey insights that are even more important than the downstream task. In this\\npaper, we introduce Differentiable Graph Module (DGM), a learnable function\\npredicting the edge probability in the graph relevant for the task, that can be\\ncombined with convolutional graph neural network layers and trained in an\\nend-to-end fashion. We provide an extensive evaluation of applications from the\\ndomains of healthcare (disease prediction), brain imaging (gender and age\\nprediction), computer graphics (3D point cloud segmentation), and computer\\nvision (zero-shot learning). We show that our model provides a significant\\nimprovement over baselines both in transductive and inductive settings and\\nachieves state-of-the-art results.'\n",
      " b'Gaze estimation involves predicting where the person is looking at, given\\neither a single input image or a sequence of images. One challenging task, gaze\\nestimation in the wild, concerns data collected in unconstrained environments\\nwith varying camera-person distances, like the Gaze360 dataset. The varying\\ndistances result in varying face sizes in the images, which makes it hard for\\ncurrent CNN backbones to estimate the gaze robustly. Inspired by our natural\\nskill to identify the gaze by taking a focused look at the face area, we\\npropose a novel architecture that similarly zooms in on the face area of the\\nimage at multiple scales to improve prediction accuracy. Another challenging\\ntask, 360-degree gaze estimation (also introduced by the Gaze360 dataset),\\nconsists of estimating not only the forward gazes, but also the backward ones.\\nThe backward gazes introduce discontinuity in the yaw angle values of the gaze,\\nmaking the deep learning models affected by some huge loss around the\\ndiscontinuous points. We propose to convert the angle values by sine-cosine\\ntransform to avoid the discontinuity and represent the physical meaning of the\\nyaw angle better. We conduct ablation studies on both ideas, the novel\\narchitecture and the transform, to validate their effectiveness. The two ideas\\nallow our proposed model to achieve state-of-the-art performance for both the\\nGaze360 dataset and the RT-Gene dataset when using single images. Furthermore,\\nwe extend the model to a sequential version that systematically zooms in on a\\ngiven sequence of images. The sequential version again achieves\\nstate-of-the-art performance on the Gaze360 dataset, which further demonstrates\\nthe usefulness of our proposed ideas.'\n",
      " b'Mammography is one of the most commonly applied tools for early breast cancer\\nscreening. Automatic segmentation of breast masses in mammograms is essential\\nbut challenging due to the low signal-to-noise ratio and the wide variety of\\nmass shapes and sizes. Existing methods deal with these challenges mainly by\\nextracting mass-centered image patches manually or automatically. However,\\nmanual patch extraction is time-consuming and automatic patch extraction brings\\nerrors that could not be compensated in the following segmentation step. In\\nthis study, we propose a novel attention-guided dense-upsampling network\\n(AUNet) for accurate breast mass segmentation in whole mammograms directly. In\\nAUNet, we employ an asymmetrical encoder-decoder structure and propose an\\neffective upsampling block, attention-guided dense-upsampling block (AU block).\\nEspecially, the AU block is designed to have three merits. Firstly, it\\ncompensates the information loss of bilinear upsampling by dense upsampling.\\nSecondly, it designs a more effective method to fuse high- and low-level\\nfeatures. Thirdly, it includes a channel-attention function to highlight\\nrich-information channels. We evaluated the proposed method on two publicly\\navailable datasets, CBIS-DDSM and INbreast. Compared to three state-of-the-art\\nfully convolutional networks, AUNet achieved the best performances with an\\naverage Dice similarity coefficient of 81.8% for CBIS-DDSM and 79.1% for\\nINbreast.'\n",
      " b\"Machine learning requires data, but acquiring and labeling real-world data is\\nchallenging, expensive, and time-consuming. More importantly, it is nearly\\nimpossible to alter real data post-acquisition (e.g., change the illumination\\nof a room), making it very difficult to measure how specific properties of the\\ndata affect performance. In this paper, we present AI Playground (AIP), an\\nopen-source, Unreal Engine-based tool for generating and labeling virtual image\\ndata. With AIP, it is trivial to capture the same image under different\\nconditions (e.g., fidelity, lighting, etc.) and with different ground truths\\n(e.g., depth or surface normal values). AIP is easily extendable and can be\\nused with or without code. To validate our proposed tool, we generated eight\\ndatasets of otherwise identical but varying lighting and fidelity conditions.\\nWe then trained deep neural networks to predict (1) depth values, (2) surface\\nnormals, or (3) object labels and assessed each network's intra- and\\ncross-dataset performance. Among other insights, we verified that sensitivity\\nto different settings is problem-dependent. We confirmed the findings of other\\nstudies that segmentation models are very sensitive to fidelity, but we also\\nfound that they are just as sensitive to lighting. In contrast, depth and\\nnormal estimation models seem to be less sensitive to fidelity or lighting and\\nmore sensitive to the structure of the image. Finally, we tested our trained\\ndepth-estimation networks on two real-world datasets and obtained results\\ncomparable to training on real data alone, confirming that our virtual\\nenvironments are realistic enough for real-world tasks.\"\n",
      " b'Piecewise Aggregate Approximation (PAA) is a competitive basic dimension\\nreduction method for high-dimensional time series mining. When deployed,\\nhowever, the limitations are obvious that some important information will be\\nmissed, especially the trend. In this paper, we propose two new approaches for\\ntime series that utilize approximate trend feature information. Our first\\nmethod is based on relative mean value of each segment to record the trend,\\nwhich divide each segment into two parts and use the numerical average\\nrespectively to represent the trend. We proved that this method satisfies lower\\nbound which guarantee no false dismissals. Our second method uses a binary\\nstring to record the trend which is also relative to mean in each segment. Our\\nmethods are applied on similarity measurement in classification and anomaly\\ndetection, the experimental results show the improvement of accuracy and\\neffectiveness by extracting the trend feature suitably.'\n",
      " b'Many attempts have been made towards combining RGB and 3D poses for the\\nrecognition of Activities of Daily Living (ADL). ADL may look very similar and\\noften necessitate to model fine-grained details to distinguish them. Because\\nthe recent 3D ConvNets are too rigid to capture the subtle visual patterns\\nacross an action, this research direction is dominated by methods combining RGB\\nand 3D Poses. But the cost of computing 3D poses from RGB stream is high in the\\nabsence of appropriate sensors. This limits the usage of aforementioned\\napproaches in real-world applications requiring low latency. Then, how to best\\ntake advantage of 3D Poses for recognizing ADL? To this end, we propose an\\nextension of a pose driven attention mechanism: Video-Pose Network (VPN),\\nexploring two distinct directions. One is to transfer the Pose knowledge into\\nRGB through a feature-level distillation and the other towards mimicking pose\\ndriven attention through an attention-level distillation. Finally, these two\\napproaches are integrated into a single model, we call VPN++. We show that\\nVPN++ is not only effective but also provides a high speed up and high\\nresilience to noisy Poses. VPN++, with or without 3D Poses, outperforms the\\nrepresentative baselines on 4 public datasets. Code is available at\\nhttps://github.com/srijandas07/vpnplusplus.'\n",
      " b'Recent work in adversarial machine learning started to focus on the visual\\nperception in autonomous driving and studied Adversarial Examples (AEs) for\\nobject detection models. However, in such visual perception pipeline the\\ndetected objects must also be tracked, in a process called Multiple Object\\nTracking (MOT), to build the moving trajectories of surrounding obstacles.\\nSince MOT is designed to be robust against errors in object detection, it poses\\na general challenge to existing attack techniques that blindly target objection\\ndetection: we find that a success rate of over 98% is needed for them to\\nactually affect the tracking results, a requirement that no existing attack\\ntechnique can satisfy. In this paper, we are the first to study adversarial\\nmachine learning attacks against the complete visual perception pipeline in\\nautonomous driving, and discover a novel attack technique, tracker hijacking,\\nthat can effectively fool MOT using AEs on object detection. Using our\\ntechnique, successful AEs on as few as one single frame can move an existing\\nobject in to or out of the headway of an autonomous vehicle to cause potential\\nsafety hazards. We perform evaluation using the Berkeley Deep Drive dataset and\\nfind that on average when 3 frames are attacked, our attack can have a nearly\\n100% success rate while attacks that blindly target object detection only have\\nup to 25%.'\n",
      " b'The science of solving clinical problems by analyzing images generated in\\nclinical practice is known as medical image analysis. The aim is to extract\\ninformation in an effective and efficient manner for improved clinical\\ndiagnosis. The recent advances in the field of biomedical engineering has made\\nmedical image analysis one of the top research and development area. One of the\\nreason for this advancement is the application of machine learning techniques\\nfor the analysis of medical images. Deep learning is successfully used as a\\ntool for machine learning, where a neural network is capable of automatically\\nlearning features. This is in contrast to those methods where traditionally\\nhand crafted features are used. The selection and calculation of these features\\nis a challenging task. Among deep learning techniques, deep convolutional\\nnetworks are actively used for the purpose of medical image analysis. This\\ninclude application areas such as segmentation, abnormality detection, disease\\nclassification, computer aided diagnosis and retrieval. In this study, a\\ncomprehensive review of the current state-of-the-art in medical image analysis\\nusing deep convolutional networks is presented. The challenges and potential of\\nthese techniques are also highlighted.'\n",
      " b\"Federated learning is emerging as a machine learning technique that trains a\\nmodel across multiple decentralized parties. It is renowned for preserving\\nprivacy as the data never leaves the computational devices, and recent\\napproaches further enhance its privacy by hiding messages transferred in\\nencryption. However, we found that despite the efforts, federated learning\\nremains privacy-threatening, due to its interactive nature across different\\nparties. In this paper, we analyze the privacy threats in industrial-level\\nfederated learning frameworks with secure computation, and reveal such threats\\nwidely exist in typical machine learning models such as linear regression,\\nlogistic regression and decision tree. For the linear and logistic regression,\\nwe show through theoretical analysis that it is possible for the attacker to\\ninvert the entire private input of the victim, given very few information. For\\nthe decision tree model, we launch an attack to infer the range of victim's\\nprivate inputs. All attacks are evaluated on popular federated learning\\nframeworks and real-world datasets.\"\n",
      " b'Deep neural networks have achieved great success both in computer vision and\\nnatural language processing tasks. However, mostly state-of-art methods highly\\nrely on external training or computing to improve the performance. To alleviate\\nthe external reliance, we proposed a gradient enhancement approach, conducted\\nby the short circuit neural connections, to improve the gradient learning of\\ndeep neural networks. The proposed short circuit is a unidirectional connection\\nthat single back propagates the sensitive from the deep layer to the shallows.\\nMoreover, the short circuit formulates to be a gradient truncation of its\\ncrossing layers which can plug into the backbone deep neural networks without\\nintroducing external training parameters. Extensive experiments demonstrate\\ndeep neural networks with our short circuit gain a large margin over the\\nbaselines on both computer vision and natural language processing tasks.'\n",
      " b'Q-learning with neural network function approximation (neural Q-learning for\\nshort) is among the most prevalent deep reinforcement learning algorithms.\\nDespite its empirical success, the non-asymptotic convergence rate of neural\\nQ-learning remains virtually unknown. In this paper, we present a finite-time\\nanalysis of a neural Q-learning algorithm, where the data are generated from a\\nMarkov decision process and the action-value function is approximated by a deep\\nReLU neural network. We prove that neural Q-learning finds the optimal policy\\nwith $O(1/\\\\sqrt{T})$ convergence rate if the neural function approximator is\\nsufficiently overparameterized, where $T$ is the number of iterations. To our\\nbest knowledge, our result is the first finite-time analysis of neural\\nQ-learning under non-i.i.d. data assumption.'\n",
      " b'In this paper, we address the scene segmentation task by capturing rich\\ncontextual dependencies based on the selfattention mechanism. Unlike previous\\nworks that capture contexts by multi-scale features fusion, we propose a Dual\\nAttention Networks (DANet) to adaptively integrate local features with their\\nglobal dependencies. Specifically, we append two types of attention modules on\\ntop of traditional dilated FCN, which model the semantic interdependencies in\\nspatial and channel dimensions respectively. The position attention module\\nselectively aggregates the features at each position by a weighted sum of the\\nfeatures at all positions. Similar features would be related to each other\\nregardless of their distances. Meanwhile, the channel attention module\\nselectively emphasizes interdependent channel maps by integrating associated\\nfeatures among all channel maps. We sum the outputs of the two attention\\nmodules to further improve feature representation which contributes to more\\nprecise segmentation results. We achieve new state-of-the-art segmentation\\nperformance on three challenging scene segmentation datasets, i.e., Cityscapes,\\nPASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5%\\non Cityscapes test set is achieved without using coarse data. We make the code\\nand trained model publicly available at https://github.com/junfu1115/DANet'\n",
      " b'The discovery of time series motifs has emerged as one of the most useful\\nprimitives in time series data mining. Researchers have shown its utility for\\nexploratory data mining, summarization, visualization, segmentation,\\nclassification, clustering, and rule discovery. Although there has been more\\nthan a decade of extensive research, there is still no technique to allow the\\ndiscovery of time series motifs in the presence of missing data, despite the\\nwell-documented ubiquity of missing data in scientific, industrial, and medical\\ndatasets. In this work, we introduce a technique for motif discovery in the\\npresence of missing data. We formally prove that our method is admissible,\\nproducing no false negatives. We also show that our method can piggy-back off\\nthe fastest known motif discovery method with a small constant factor\\ntime/space overhead. We will demonstrate our approach on diverse datasets with\\nvarying amounts of missing data'\n",
      " b'State of the art methods for semantic image segmentation are trained in a\\nsupervised fashion using a large corpus of fully labeled training images.\\nHowever, gathering such a corpus is expensive, due to human annotation effort,\\nin contrast to gathering unlabeled data. We propose an active learning-based\\nstrategy, called CEREALS, in which a human only has to hand-label a few,\\nautomatically selected, regions within an unlabeled image corpus. This\\nminimizes human annotation effort while maximizing the performance of a\\nsemantic image segmentation method. The automatic selection procedure is\\nachieved by: a) using a suitable information measure combined with an estimate\\nabout human annotation effort, which is inferred from a learned cost model, and\\nb) exploiting the spatial coherency of an image. The performance of CEREALS is\\ndemonstrated on Cityscapes, where we are able to reduce the annotation effort\\nto 17%, while keeping 95% of the mean Intersection over Union (mIoU) of a model\\nthat was trained with the fully annotated training set of Cityscapes.'\n",
      " b\"With the rapid development of measurement technology, LiDAR and depth cameras\\nare widely used in the perception of the 3D environment. Recent learning based\\nmethods for robot perception most focus on the image or video, but deep\\nlearning methods for dynamic 3D point cloud sequences are underexplored.\\nTherefore, developing efficient and accurate perception method compatible with\\nthese advanced instruments is pivotal to autonomous driving and service robots.\\nAn Anchor-based Spatio-Temporal Attention 3D Convolution operation (ASTA3DConv)\\nis proposed in this paper to process dynamic 3D point cloud sequences. The\\nproposed convolution operation builds a regular receptive field around each\\npoint by setting several virtual anchors around each point. The features of\\nneighborhood points are firstly aggregated to each anchor based on the\\nspatio-temporal attention mechanism. Then, anchor-based 3D convolution is\\nadopted to aggregate these anchors' features to the core points. The proposed\\nmethod makes better use of the structured information within the local region\\nand learns spatio-temporal embedding features from dynamic 3D point cloud\\nsequences. Anchor-based Spatio-Temporal Attention 3D Convolutional Neural\\nNetworks (ASTA3DCNNs) are built for classification and segmentation tasks based\\non the proposed ASTA3DConv and evaluated on action recognition and semantic\\nsegmentation tasks. The experiments and ablation studies on MSRAction3D and\\nSynthia datasets demonstrate the superior performance and effectiveness of our\\nmethod for dynamic 3D point cloud sequences. Our method achieves the\\nstate-of-the-art performance among the methods with dynamic 3D point cloud\\nsequences as input on MSRAction3D and Synthia datasets.\"\n",
      " b\"With the ubiquity of sensors in the IoT era, statistical observations are\\nbecoming increasingly available in the form of massive (multivariate)\\ntime-series. Formulated as unsupervised anomaly detection tasks, an abundance\\nof applications like aviation safety management, the health monitoring of\\ncomplex infrastructures or fraud detection can now rely on such functional\\ndata, acquired and stored with an ever finer granularity. The concept of\\nstatistical depth, which reflects centrality of an arbitrary observation w.r.t.\\na statistical population may play a crucial role in this regard, anomalies\\ncorresponding to observations with 'small' depth. Supported by sound\\ntheoretical and computational developments in the recent decades, it has proven\\nto be extremely useful, in particular in functional spaces. However, most\\napproaches documented in the literature consist in evaluating independently the\\ncentrality of each point forming the time series and consequently exhibit a\\ncertain insensitivity to possible shape changes. In this paper, we propose a\\nnovel notion of functional depth based on the area of the convex hull of\\nsampled curves, capturing gradual departures from centrality, even beyond the\\nenvelope of the data, in a natural fashion. We discuss practical relevance of\\ncommonly imposed axioms on functional depths and investigate which of them are\\nsatisfied by the notion of depth we promote here. Estimation and computational\\nissues are also addressed and various numerical experiments provide empirical\\nevidence of the relevance of the approach proposed.\"\n",
      " b'Referring object detection and referring image segmentation are important\\ntasks that require joint understanding of visual information and natural\\nlanguage. Yet there has been evidence that current benchmark datasets suffer\\nfrom bias, and current state-of-the-art models cannot be easily evaluated on\\ntheir intermediate reasoning process. To address these issues and complement\\nsimilar efforts in visual question answering, we build CLEVR-Ref+, a synthetic\\ndiagnostic dataset for referring expression comprehension. The precise\\nlocations and attributes of the objects are readily available, and the\\nreferring expressions are automatically associated with functional programs.\\nThe synthetic nature allows control over dataset bias (through sampling\\nstrategy), and the modular programs enable intermediate reasoning ground truth\\nwithout human annotators.\\n  In addition to evaluating several state-of-the-art models on CLEVR-Ref+, we\\nalso propose IEP-Ref, a module network approach that significantly outperforms\\nother models on our dataset. In particular, we present two interesting and\\nimportant findings using IEP-Ref: (1) the module trained to transform feature\\nmaps into segmentation masks can be attached to any intermediate module to\\nreveal the entire reasoning process step-by-step; (2) even if all training data\\nhas at least one object referred, IEP-Ref can correctly predict no-foreground\\nwhen presented with false-premise referring expressions. To the best of our\\nknowledge, this is the first direct and quantitative proof that neural modules\\nbehave in the way they are intended.'\n",
      " b'Well-established methods for the solution of stochastic partial differential\\nequations (SPDEs) typically struggle in problems with high-dimensional\\ninputs/outputs. Such difficulties are only amplified in large-scale\\napplications where even a few tens of full-order model runs are impracticable.\\nWhile dimensionality reduction can alleviate some of these issues, it is not\\nknown which and how many features of the (high-dimensional) input are actually\\npredictive of the (high-dimensional) output. In this paper, we advocate a\\nBayesian formulation that is capable of performing simultaneous dimension and\\nmodel-order reduction. It consists of a component that encodes the\\nhigh-dimensional input into a low-dimensional set of feature functions by\\nemploying sparsity-enforcing priors and a decoding component that makes use of\\nthe solution of a coarse-grained model in order to reconstruct that of the\\nfull-order model. Both components are represented with latent variables in a\\nprobabilistic graphical model and are simultaneously trained using Stochastic\\nVariational Inference methods. The model is capable of quantifying the\\npredictive uncertainty due to the information loss that unavoidably takes place\\nin any model-order/dimension reduction as well as the uncertainty arising from\\nfinite-sized training datasets. We demonstrate its capabilities in the context\\nof random media where fine-scale fluctuations can give rise to random inputs\\nwith tens of thousands of variables. With a few tens of full-order model\\nsimulations, the proposed model is capable of identifying salient physical\\nfeatures and produce sharp predictions under different boundary conditions of\\nthe full output which itself consists of thousands of components.'\n",
      " b'This paper proposes a general multi-modal data learning method, which\\nincludes Global Homogeneous Transformation, Local Homogeneous Transformation\\nand their combination. During ReID model training, on the one hand, it randomly\\nselected a rectangular area in the RGB image and replace its color with the\\nsame rectangular area in corresponding homogeneous image, thus it generate a\\ntraining image with different homogeneous areas; On the other hand, it convert\\nan image into a homogeneous image. These two methods help the model to directly\\nlearn the relationship between different modalities in the Special ReID task.\\nIn single-modal ReID tasks, it can be used as an effective data augmentation.\\nThe experimental results show that our method achieves a performance\\nimprovement of up to 3.3% in single modal ReID task, and performance\\nimprovement in the Sketch Re-identification more than 8%. In addition, our\\nexperiments also show that this method is also very useful in adversarial\\ntraining for adversarial defense. It can help the model learn faster and better\\nfrom adversarial examples.'\n",
      " b\"Validating the safety of autonomous systems generally requires the use of\\nhigh-fidelity simulators that adequately capture the variability of real-world\\nscenarios. However, it is generally not feasible to exhaustively search the\\nspace of simulation scenarios for failures. Adaptive stress testing (AST) is a\\nmethod that uses reinforcement learning to find the most likely failure of a\\nsystem. AST with a deep reinforcement learning solver has been shown to be\\neffective in finding failures across a range of different systems. This\\napproach generally involves running many simulations, which can be very\\nexpensive when using a high-fidelity simulator. To improve efficiency, we\\npresent a method that first finds failures in a low-fidelity simulator. It then\\nuses the backward algorithm, which trains a deep neural network policy using a\\nsingle expert demonstration, to adapt the low-fidelity failures to\\nhigh-fidelity. We have created a series of autonomous vehicle validation case\\nstudies that represent some of the ways low-fidelity and high-fidelity\\nsimulators can differ, such as time discretization. We demonstrate in a variety\\nof case studies that this new AST approach is able to find failures with\\nsignificantly fewer high-fidelity simulation steps than are needed when just\\nrunning AST directly in high-fidelity. As a proof of concept, we also\\ndemonstrate AST on NVIDIA's DriveSim simulator, an industry state-of-the-art\\nhigh-fidelity simulator for finding failures in autonomous vehicles.\"\n",
      " b'High quality standard cell layout automation in advanced technology nodes is\\nstill challenging in the industry today because of complex design rules. In\\nthis paper we introduce an automatic standard cell layout generator called\\nNVCell that can generate layouts with equal or smaller area for over 90% of\\nsingle row cells in an industry standard cell library on an advanced technology\\nnode. NVCell leverages reinforcement learning (RL) to fix design rule\\nviolations during routing and to generate efficient placements.'\n",
      " b'Automatic analysis of retinal blood images is of vital importance in\\ndiagnosis tasks of retinopathy. Segmenting vessels accurately is a fundamental\\nstep in analysing retinal images. However, it is usually difficult due to\\nvarious imaging conditions, low image contrast and the appearance of\\npathologies such as micro-aneurysms. In this paper, we propose a novel method\\nwith deep neural networks to solve this problem. We utilize U-net with residual\\nconnection to detect vessels. To achieve better accuracy, we introduce an\\nedge-aware mechanism, in which we convert the original task into a multi-class\\ntask by adding additional labels on boundary areas. In this way, the network\\nwill pay more attention to the boundary areas of vessels and achieve a better\\nperformance, especially in tiny vessels detecting. Besides, side output layers\\nare applied in order to give deep supervision and therefore help convergence.\\nWe train and evaluate our model on three databases: DRIVE, STARE, and CHASEDB1.\\nExperimental results show that our method has a comparable performance with AUC\\nof 97.99% on DRIVE and an efficient running time compared to the\\nstate-of-the-art methods.'\n",
      " b'Benefit from the quick development of deep learning techniques, salient\\nobject detection has achieved remarkable progresses recently. However, there\\nstill exists following two major challenges that hinder its application in\\nembedded devices, low resolution output and heavy model weight. To this end,\\nthis paper presents an accurate yet compact deep network for efficient salient\\nobject detection. More specifically, given a coarse saliency prediction in the\\ndeepest layer, we first employ residual learning to learn side-output residual\\nfeatures for saliency refinement, which can be achieved with very limited\\nconvolutional parameters while keep accuracy. Secondly, we further propose\\nreverse attention to guide such side-output residual learning in a top-down\\nmanner. By erasing the current predicted salient regions from side-output\\nfeatures, the network can eventually explore the missing object parts and\\ndetails which results in high resolution and accuracy. Experiments on six\\nbenchmark datasets demonstrate that the proposed approach compares favorably\\nagainst state-of-the-art methods, and with advantages in terms of simplicity,\\nefficiency (45 FPS) and model size (81 MB).'\n",
      " b'We consider the composition optimization with two expected-value functions in\\nthe form of $\\\\frac{1}{n}\\\\sum\\\\nolimits_{i = 1}^n F_i(\\\\frac{1}{m}\\\\sum\\\\nolimits_{j\\n= 1}^m G_j(x))+R(x)$, { which formulates many important problems in statistical\\nlearning and machine learning such as solving Bellman equations in\\nreinforcement learning and nonlinear embedding}. Full Gradient or classical\\nstochastic gradient descent based optimization algorithms are unsuitable or\\ncomputationally expensive to solve this problem due to the inner expectation\\n$\\\\frac{1}{m}\\\\sum\\\\nolimits_{j = 1}^m G_j(x)$. We propose a duality-free based\\nstochastic composition method that combines variance reduction methods to\\naddress the stochastic composition problem. We apply SVRG and SAGA based\\nmethods to estimate the inner function, and duality-free method to estimate the\\nouter function. We prove the linear convergence rate not only for the convex\\ncomposition problem, but also for the case that the individual outer functions\\nare non-convex while the objective function is strongly-convex. We also provide\\nthe results of experiments that show the effectiveness of our proposed methods.'\n",
      " b'Traditional methods for motion estimation estimate the motion field F between\\na pair of images as the one that minimizes a predesigned cost function. In this\\npaper, we propose a direct method and train a Convolutional Neural Network\\n(CNN) that when, at test time, is given a pair of images as input it produces a\\ndense motion field F at its output layer. In the absence of large datasets with\\nground truth motion that would allow classical supervised training, we propose\\nto train the network in an unsupervised manner. The proposed cost function that\\nis optimized during training, is based on the classical optical flow\\nconstraint. The latter is differentiable with respect to the motion field and,\\ntherefore, allows backpropagation of the error to previous layers of the\\nnetwork. Our method is tested on both synthetic and real image sequences and\\nperforms similarly to the state-of-the-art methods.'\n",
      " b'In this paper we propose a new approach to person re-identification using\\nimages and natural language descriptions. We propose a joint vision and\\nlanguage model based on CCA and CNN architectures to match across the two\\nmodalities as well as to enrich visual examples for which there are no language\\ndescriptions. We also introduce new annotations in the form of natural language\\ndescriptions for two standard Re-ID benchmarks, namely CUHK03 and VIPeR. We\\nperform experiments on these two datasets with techniques based on CNN,\\nhand-crafted features as well as LSTM for analysing visual and natural\\ndescription data. We investigate and demonstrate the advantages of using\\nnatural language descriptions compared to attributes as well as CNN compared to\\nLSTM in the context of Re-ID. We show that the joint use of language and vision\\ncan significantly improve the state-of-the-art performance on standard Re-ID\\nbenchmarks.'\n",
      " b'We propose a general-purpose approach to discovering active learning (AL)\\nstrategies from data. These strategies are transferable from one domain to\\nanother and can be used in conjunction with many machine learning models. To\\nthis end, we formalize the annotation process as a Markov decision process,\\ndesign universal state and action spaces and introduce a new reward function\\nthat precisely model the AL objective of minimizing the annotation cost. We\\nseek to find an optimal (non-myopic) AL strategy using reinforcement learning.\\nWe evaluate the learned strategies on multiple unrelated domains and show that\\nthey consistently outperform state-of-the-art baselines.'\n",
      " b'Detection of pedestrians on embedded devices, such as those on-board of\\nrobots and drones, has many applications including road intersection\\nmonitoring, security, crowd monitoring and surveillance, to name a few.\\nHowever, the problem can be challenging due to continuously-changing camera\\nviewpoint and varying object appearances as well as the need for lightweight\\nalgorithms suitable for embedded systems. This paper proposes a robust\\nframework for pedestrian detection in many footages. The framework performs\\nfine and coarse detections on different image regions and exploits temporal and\\nspatial characteristics to attain enhanced accuracy and real time performance\\non embedded boards. The framework uses the Yolo-v3 object detection [1] as its\\nbackbone detector and runs on the Nvidia Jetson TX2 embedded board, however\\nother detectors and/or boards can be used as well. The performance of the\\nframework is demonstrated on two established datasets and its achievement of\\nthe second place in CVPR 2019 Embedded Real-Time Inference (ERTI) Challenge.'], shape=(128,), dtype=string) tf.Tensor(\n",
      "[[0 1 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]], shape=(128, 165), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for sample, label in train_dataset.take(1):\n",
    "    print(sample, label)  # Ensure sample is a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text dtype: <dtype: 'string'>, Label dtype: <dtype: 'int64'>\n"
     ]
    }
   ],
   "source": [
    "for text, label in train_dataset.take(1):\n",
    "    print(f\"Text dtype: {text.dtype}, Label dtype: {label.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "FrQ05DAp9dB9"
   },
   "outputs": [],
   "source": [
    "# Reinitialize and re-adapt\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=158894,  # Adjust based on your dataset\n",
    "    ngrams=2,\n",
    "    output_mode=\"tf_idf\"\n",
    ")\n",
    "\n",
    "# Adapt to the training dataset\n",
    "text_vectorizer.adapt(train_dataset.map(lambda text, label: text))\n",
    "\n",
    "# Save the updated vocabulary and IDF weights\n",
    "vocabulary = text_vectorizer.get_vocabulary()\n",
    "idf_weights = text_vectorizer.get_weights()\n",
    "\n",
    "with open(\"vocabulary.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocabulary, f)\n",
    "\n",
    "with open(\"idf_weights.pkl\", \"wb\") as f:\n",
    "    pickle.dump(idf_weights, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(idf_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_abstract = \"This is a sample abstract for testing.\"\n",
    "# preprocessed_test = text_vectorizer([test_abstract])\n",
    "# prediction = loaded_model(preprocessed_test)\n",
    "# print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Xqlrr0uN9c_d"
   },
   "outputs": [],
   "source": [
    "train_dataset= train_dataset.map(lambda text, label: (text_vectorizer(text),label),num_parallel_calls=auto).prefetch(auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llKIcy0rKFlY"
   },
   "outputs": [],
   "source": [
    "# this parallelizes each element coming from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "wxTq4QUO9c8_"
   },
   "outputs": [],
   "source": [
    "val_dataset= val_dataset.map(lambda text, label: (text_vectorizer(text),label),num_parallel_calls=auto).prefetch(auto)\n",
    "test_dataset= test_dataset.map(lambda text, label: (text_vectorizer(text),label),num_parallel_calls=auto).prefetch(auto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXXIVQAdMk0B",
    "outputId": "bfdf9355-add2-4a60-a5ce-2010f00f8e27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 158894), dtype=tf.float32, name=None), TensorSpec(shape=(None, 165), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18SofUcmKeFi"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "KI_gUuy_NvcG"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "IQLhlhHjOEaF"
   },
   "outputs": [],
   "source": [
    "\n",
    "model1=keras.Sequential([\n",
    "    layers.Dense(512,activation='relu') ,#using relu for multiclass classification\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256,activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(lookup.vocabulary_size(),activation='softmax') #this is the output layer\n",
    "])\n",
    "model1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['binary_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "88aPxXZy9c6k",
    "outputId": "0ac41f73-c587-40a5-cceb-daaac7f75a24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 2s/step - binary_accuracy: 0.9908 - loss: 0.1248 - val_binary_accuracy: 0.9936 - val_loss: 0.0187\n",
      "Epoch 2/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 2s/step - binary_accuracy: 0.9936 - loss: 0.0192 - val_binary_accuracy: 0.9935 - val_loss: 0.0182\n",
      "Epoch 3/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m462s\u001b[0m 2s/step - binary_accuracy: 0.9939 - loss: 0.0143 - val_binary_accuracy: 0.9935 - val_loss: 0.0186\n",
      "Epoch 4/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 1s/step - binary_accuracy: 0.9941 - loss: 0.0118 - val_binary_accuracy: 0.9935 - val_loss: 0.0193\n",
      "Epoch 5/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 1s/step - binary_accuracy: 0.9941 - loss: 0.0101 - val_binary_accuracy: 0.9935 - val_loss: 0.0194\n",
      "Epoch 6/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m372s\u001b[0m 1s/step - binary_accuracy: 0.9941 - loss: 0.0089 - val_binary_accuracy: 0.9935 - val_loss: 0.0204\n",
      "Epoch 7/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 1s/step - binary_accuracy: 0.9942 - loss: 0.0081 - val_binary_accuracy: 0.9935 - val_loss: 0.0211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x25a234523a0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "es=EarlyStopping(patience=5,restore_best_weights=True)\n",
    "# when the model reaches the 5th epoch, if it is going towards overfitting, it'll stop after 5 more epochs\n",
    "\n",
    "model1.fit(train_dataset,validation_data=val_dataset,epochs=20,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "LhxzC3Dd9c4A",
    "outputId": "bc2a3293-4bc2-4642-b8b3-c8586a8a596c"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# model1.save('model1.h5')\n",
    "\n",
    "saved_text_vectorizer_config=text_vectorizer.get_config()\n",
    "with open('text_vectorizer_config_new.pkl','wb')as f:\n",
    "    pickle.dump(saved_text_vectorizer_config,f)\n",
    "    \n",
    "with open('vocab_new2.pkl','wb')as f:\n",
    "    pickle.dump(vocabulary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: IDF weights are empty. Ensure the TextVectorization layer was adapted correctly.\n"
     ]
    }
   ],
   "source": [
    "idf_weights = text_vectorizer.get_weights()\n",
    "if len(idf_weights) > 0:\n",
    "    with open(\"text_vectorizer_idf_weights_second.pkl\", \"wb\") as f:\n",
    "        pickle.dump(idf_weights, f)\n",
    "else:\n",
    "    print(\"Warning: IDF weights are empty. Ensure the TextVectorization layer was adapted correctly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-JsQqSp9c1j"
   },
   "source": [
    "## Load Model and Text Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "CWCfQuoU7B__"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "loaded_model=keras.models.load_model('model1.h5')\n",
    "\n",
    "with open('text_vectorizer_config_new.pkl','rb') as f:\n",
    "    saved_text_vectorizer_config=pickle.load(f)\n",
    "\n",
    "loaded_text_vectorizer=text_vectorizer.from_config(saved_text_vectorizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "QXFdRC6jQEvv"
   },
   "outputs": [],
   "source": [
    "# Load the saved weights into the new TextVectorization layer\n",
    "# with open(\"text_vectorizer_weights.pkl\", \"rb\") as f:\n",
    "#     weights = pickle.load(f)\n",
    "#     loaded_text_vectorizer.set_weights(weights)\n",
    "    \n",
    "with open('vocab_new.pkl','rb')as f:\n",
    "    loaded_vocab=pickle.load(f)\n",
    "    \n",
    "# loaded_text_vectorizer = TextVectorization.from_config(saved_text_vectorizer_config)\n",
    "\n",
    "# Restore vocabulary\n",
    "with open(\"text_vectorizer_idf_weights_new.pkl\", \"rb\") as f:\n",
    "    idf_weights = pickle.load(f)\n",
    "    loaded_text_vectorizer.set_weights(idf_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 165\n",
      "IDF weights size: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size:\", len(loaded_vocab))\n",
    "print(\"IDF weights size:\", len(idf_weights[0]) if len(idf_weights) > 0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`idf_weights` must be set if output_mode is 'tf_idf'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16332\\6151228.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mloaded_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mloaded_text_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\preprocessing\\text_vectorization.py\u001b[0m in \u001b[0;36mset_vocabulary\u001b[1;34m(self, vocabulary, idf_weights)\u001b[0m\n\u001b[0;32m    517\u001b[0m                 \u001b[0mShould\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mset\u001b[0m \u001b[0motherwise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \"\"\"\n\u001b[1;32m--> 519\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lookup_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midf_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midf_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\preprocessing\\index_lookup.py\u001b[0m in \u001b[0;36mset_vocabulary\u001b[1;34m(self, vocabulary, idf_weights)\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"tf_idf\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0midf_weights\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    375\u001b[0m                     \u001b[1;34m\"`idf_weights` must be set if output_mode is 'tf_idf'.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m                 )\n",
      "\u001b[1;31mValueError\u001b[0m: `idf_weights` must be set if output_mode is 'tf_idf'."
     ]
    }
   ],
   "source": [
    "with open(\"vocab_new.pkl\", \"rb\") as f:\n",
    "    loaded_vocab = pickle.load(f)\n",
    "    \n",
    "    loaded_text_vectorizer.set_vocabulary(loaded_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Restore idf_weights\n",
    "# with open(\"text_vectorizer_idf_weights.pkl\", \"rb\") as f:\n",
    "#     idf_weights = pickle.load(f)\n",
    "\n",
    "# # Ensure idf_weights match vocabulary size\n",
    "# if len(idf_weights) != len(loaded_vocab):\n",
    "#     raise ValueError(\"The size of idf_weights does not match the vocabulary size.\")\n",
    "\n",
    "# # Set the idf_weights\n",
    "# loaded_text_vectorizer.set_weights([idf_weights])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# # Load the configuration of the text vectorizer\n",
    "# with open(\"text_vectorizer_config.pkl\", \"rb\") as f:\n",
    "#     saved_text_vectorizer_config = pickle.load(f)\n",
    "\n",
    "# # Create a new TextVectorization layer with the saved configuration\n",
    "# loaded_text_vectorizer = TextVectorization.from_config(saved_text_vectorizer_config)\n",
    "\n",
    "# # Load the saved weights into the new TextVectorization layer\n",
    "# with open(\"text_vectorizer_weights.pkl\", \"rb\") as f:\n",
    "#     weights = pickle.load(f)\n",
    "#     loaded_text_vectorizer.set_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 205ms/step - binary_accuracy: 0.9935 - loss: 0.0188\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 213ms/step - binary_accuracy: 0.9934 - loss: 0.0182\n"
     ]
    }
   ],
   "source": [
    "_, acc1=model1.evaluate(test_dataset)\n",
    "_, acc2=model1.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_multi_hot(encoded_labels):\n",
    "    \"\"\"Reverse a single multi-hot encoded label to a tuple of vocab terms.\"\"\"\n",
    "    hot_indices = np.argwhere(encoded_labels == 1.0)[..., 0]\n",
    "    return np.take(loaded_vocab, hot_indices)\n",
    "\n",
    "def predict_category(abstract, model, vectorizer, label_lookup):\n",
    "    # Preprocess the abstract using the loaded text vectorizer\n",
    "    preprocessed_abstract = vectorizer([abstract])\n",
    "\n",
    "    # Make predictions using the loaded model\n",
    "    predictions = model.predict(preprocessed_abstract)\n",
    "\n",
    "    # Convert predictions to human-readable labels\n",
    "    predicted_labels = label_lookup(np.round(predictions).astype(int)[0])\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Exception encountered when calling TextVectorization.call().\n\n\u001b[1m{{function_node __wrapped__LookupTableFindV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Table not initialized. [Op:LookupTableFindV2] name: \u001b[0m\n\nArguments received by TextVectorization.call():\n  • inputs=['\"Graph neural networks (GNNs) have been widely used to learn vector\\\\nrepresentation of graph-structured data and achieved better task performance\\\\nthan conventional methods. The foundation of GNNs is the message passing\\\\nprocedure, which propagates the information in a node to its neighbors. Since\\\\nthis procedure proceeds one step per layer, the range of the information\\\\npropagation among nodes is small in the lower layers, and it expands toward the\\\\nhigher layers. Therefore, a GNN model has to be deep enough to capture global\\\\nstructural information in a graph. On the other hand, it is known that deep GNN\\\\nmodels suffer from performance degradation because they lose nodes\\' local\\\\ninformation, which would be essential for good model performance, through many\\\\nmessage passing steps. In this study, we propose multi-level attention pooling\\\\n(MLAP) for graph-level classification tasks, which can adapt to both local and\\\\nglobal structural information in a graph. It has an attention pooling layer for\\\\neach message passing step and computes the final graph representation by\\\\nunifying the layer-wise graph representations. The MLAP architecture allows\\\\nmodels to utilize the structural information of graphs with multiple levels of\\\\nlocalities because it preserves layer-wise information before losing them due\\\\nto oversmoothing. Results of our experiments show that the MLAP architecture\\\\nimproves the graph classification performance compared to the baseline\\\\narchitectures. In addition, analyses on the layer-wise graph representations\\\\nsuggest that aggregating information from multiple levels of localities indeed\\\\nhas the potential to improve the discriminability of learned graph\\\\nrepresentations.\"']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14364\\459880968.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnew_abstract\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Graph neural networks (GNNs) have been widely used to learn vector\\nrepresentation of graph-structured data and achieved better task performance\\nthan conventional methods. The foundation of GNNs is the message passing\\nprocedure, which propagates the information in a node to its neighbors. Since\\nthis procedure proceeds one step per layer, the range of the information\\npropagation among nodes is small in the lower layers, and it expands toward the\\nhigher layers. Therefore, a GNN model has to be deep enough to capture global\\nstructural information in a graph. On the other hand, it is known that deep GNN\\nmodels suffer from performance degradation because they lose nodes' local\\ninformation, which would be essential for good model performance, through many\\nmessage passing steps. In this study, we propose multi-level attention pooling\\n(MLAP) for graph-level classification tasks, which can adapt to both local and\\nglobal structural information in a graph. It has an attention pooling layer for\\neach message passing step and computes the final graph representation by\\nunifying the layer-wise graph representations. The MLAP architecture allows\\nmodels to utilize the structural information of graphs with multiple levels of\\nlocalities because it preserves layer-wise information before losing them due\\nto oversmoothing. Results of our experiments show that the MLAP architecture\\nimproves the graph classification performance compared to the baseline\\narchitectures. In addition, analyses on the layer-wise graph representations\\nsuggest that aggregating information from multiple levels of localities indeed\\nhas the potential to improve the discriminability of learned graph\\nrepresentations.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredicted_categories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_category\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_abstract\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloaded_text_vectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minvert_multi_hot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Predicted Categories:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_categories\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14364\\3486012855.py\u001b[0m in \u001b[0;36mpredict_category\u001b[1;34m(abstract, model, vectorizer, label_lookup)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict_category\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabstract\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_lookup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Preprocess the abstract using the loaded text vectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mpreprocessed_abstract\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mabstract\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Make predictions using the loaded model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\preprocessing\\index_lookup.py\u001b[0m in \u001b[0;36m_lookup_dense\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[0mlookups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 769\u001b[1;33m             \u001b[0mlookups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask_token\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Exception encountered when calling TextVectorization.call().\n\n\u001b[1m{{function_node __wrapped__LookupTableFindV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Table not initialized. [Op:LookupTableFindV2] name: \u001b[0m\n\nArguments received by TextVectorization.call():\n  • inputs=['\"Graph neural networks (GNNs) have been widely used to learn vector\\\\nrepresentation of graph-structured data and achieved better task performance\\\\nthan conventional methods. The foundation of GNNs is the message passing\\\\nprocedure, which propagates the information in a node to its neighbors. Since\\\\nthis procedure proceeds one step per layer, the range of the information\\\\npropagation among nodes is small in the lower layers, and it expands toward the\\\\nhigher layers. Therefore, a GNN model has to be deep enough to capture global\\\\nstructural information in a graph. On the other hand, it is known that deep GNN\\\\nmodels suffer from performance degradation because they lose nodes\\' local\\\\ninformation, which would be essential for good model performance, through many\\\\nmessage passing steps. In this study, we propose multi-level attention pooling\\\\n(MLAP) for graph-level classification tasks, which can adapt to both local and\\\\nglobal structural information in a graph. It has an attention pooling layer for\\\\neach message passing step and computes the final graph representation by\\\\nunifying the layer-wise graph representations. The MLAP architecture allows\\\\nmodels to utilize the structural information of graphs with multiple levels of\\\\nlocalities because it preserves layer-wise information before losing them due\\\\nto oversmoothing. Results of our experiments show that the MLAP architecture\\\\nimproves the graph classification performance compared to the baseline\\\\narchitectures. In addition, analyses on the layer-wise graph representations\\\\nsuggest that aggregating information from multiple levels of localities indeed\\\\nhas the potential to improve the discriminability of learned graph\\\\nrepresentations.\"']"
     ]
    }
   ],
   "source": [
    "new_abstract = \"Graph neural networks (GNNs) have been widely used to learn vector\\nrepresentation of graph-structured data and achieved better task performance\\nthan conventional methods. The foundation of GNNs is the message passing\\nprocedure, which propagates the information in a node to its neighbors. Since\\nthis procedure proceeds one step per layer, the range of the information\\npropagation among nodes is small in the lower layers, and it expands toward the\\nhigher layers. Therefore, a GNN model has to be deep enough to capture global\\nstructural information in a graph. On the other hand, it is known that deep GNN\\nmodels suffer from performance degradation because they lose nodes' local\\ninformation, which would be essential for good model performance, through many\\nmessage passing steps. In this study, we propose multi-level attention pooling\\n(MLAP) for graph-level classification tasks, which can adapt to both local and\\nglobal structural information in a graph. It has an attention pooling layer for\\neach message passing step and computes the final graph representation by\\nunifying the layer-wise graph representations. The MLAP architecture allows\\nmodels to utilize the structural information of graphs with multiple levels of\\nlocalities because it preserves layer-wise information before losing them due\\nto oversmoothing. Results of our experiments show that the MLAP architecture\\nimproves the graph classification performance compared to the baseline\\narchitectures. In addition, analyses on the layer-wise graph representations\\nsuggest that aggregating information from multiple levels of localities indeed\\nhas the potential to improve the discriminability of learned graph\\nrepresentations.\"\n",
    "predicted_categories = predict_category(new_abstract, loaded_model, loaded_text_vectorizer, invert_multi_hot)\n",
    "print(\"Predicted Categories:\", predicted_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0. 0. 0. ... 0. 0. 0.]], shape=(1, 158894), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_input = tf.constant([\"This is a test input.\"])\n",
    "print(text_vectorizer(test_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dummy_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14364\\2174329063.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"TextVectorization Output Shape: {text_vectorizer(dummy_input).shape}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Model Input Shape: {modified_model.input_shape}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dummy_input' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"TextVectorization Output Shape: {text_vectorizer(dummy_input).shape}\")\n",
    "print(f\"Model Input Shape: {modified_model.input_shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Graph neural networks (GNNs) have been widely used to learn vector\\nrepresentation of graph-structured data and achieved better task performance\\nthan conventional methods. The foundation of GNNs is the message passing\\nprocedure, which propagates the information in a node to its neighbors. Since\\nthis procedure proceeds one step per layer, the range of the information\\npropagation among nodes is small in the lower layers, and it expands toward the\\nhigher layers. Therefore, a GNN model has to be deep enough to capture global\\nstructural information in a graph. On the other hand, it is known that deep GNN\\nmodels suffer from performance degradation because they lose nodes' local\\ninformation, which would be essential for good model performance, through many\\nmessage passing steps. In this study, we propose multi-level attention pooling\\n(MLAP) for graph-level classification tasks, which can adapt to both local and\\nglobal structural information in a graph. It has an attention pooling layer for\\neach message passing step and computes the final graph representation by\\nunifying the layer-wise graph representations. The MLAP architecture allows\\nmodels to utilize the structural information of graphs with multiple levels of\\nlocalities because it preserves layer-wise information before losing them due\\nto oversmoothing. Results of our experiments show that the MLAP architecture\\nimproves the graph classification performance compared to the baseline\\narchitectures. In addition, analyses on the layer-wise graph representations\\nsuggest that aggregating information from multiple levels of localities indeed\\nhas the potential to improve the discriminability of learned graph\\nrepresentations.\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['abstracts'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKC2A6ffQh-i"
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['terms','abstracts'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "0C29TG90Q4NK",
    "outputId": "67edb022-cab1-4d59-f59c-bf2fa0dea48e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 56181,\n  \"fields\": [\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41105,\n        \"samples\": [\n          \"Semi-supervised Federated Learning for Activity Recognition\",\n          \"SATR-DL: Improving Surgical Skill Assessment and Task Recognition in Robot-assisted Surgery with Deep Neural Networks\",\n          \"A Hybrid Stochastic Policy Gradient Algorithm for Reinforcement Learning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-119e5f72-00df-47bd-b708-13832c4f60dd\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multi-Level Attention Pooling for Graph Neural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Forests vs. Deep Networks: Conceptual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Power up! Robust Graph Convolutional Network v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Releasing Graph Neural Networks with Different...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Recurrence-Aware Long-Term Cognitive Network f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56176</th>\n",
       "      <td>Mining Spatio-temporal Data on Industrializati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56177</th>\n",
       "      <td>Wav2Letter: an End-to-End ConvNet-based Speech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56178</th>\n",
       "      <td>Deep Reinforcement Learning with Double Q-lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56179</th>\n",
       "      <td>Generalized Low Rank Models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56180</th>\n",
       "      <td>Chi-square Tests Driven Method for Learning th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56181 rows × 1 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-119e5f72-00df-47bd-b708-13832c4f60dd')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-119e5f72-00df-47bd-b708-13832c4f60dd button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-119e5f72-00df-47bd-b708-13832c4f60dd');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-4b3cf96b-724e-4aa6-bc81-effc73a64856\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4b3cf96b-724e-4aa6-bc81-effc73a64856')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-4b3cf96b-724e-4aa6-bc81-effc73a64856 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "  <div id=\"id_f1aba86b-c10c-4acc-af03-c1cc2e9c54f7\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_f1aba86b-c10c-4acc-af03-c1cc2e9c54f7 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                  titles\n",
       "0      Multi-Level Attention Pooling for Graph Neural...\n",
       "1      Decision Forests vs. Deep Networks: Conceptual...\n",
       "2      Power up! Robust Graph Convolutional Network v...\n",
       "3      Releasing Graph Neural Networks with Different...\n",
       "4      Recurrence-Aware Long-Term Cognitive Network f...\n",
       "...                                                  ...\n",
       "56176  Mining Spatio-temporal Data on Industrializati...\n",
       "56177  Wav2Letter: an End-to-End ConvNet-based Speech...\n",
       "56178  Deep Reinforcement Learning with Double Q-lear...\n",
       "56179                        Generalized Low Rank Models\n",
       "56180  Chi-square Tests Driven Method for Learning th...\n",
       "\n",
       "[56181 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AruYHZWAREon"
   },
   "outputs": [],
   "source": [
    "# !pip install -U -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsYQeJHORgwm"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "d9701ebca5c241c0a0c618395a1d1a26",
      "9ddebab4f2404215a1d0c9200b972c59",
      "c713a95ce6cc4a9f9fe764830ef1c530",
      "02e6d43fdbdc4120be8ac47c2aae45eb",
      "35a28e996912404d87d42948d80d0cbc",
      "80921d0fa7bf44f9bd1b4d57ab4ab6ac",
      "1838545ac8764c088886c909ea69077a",
      "5f99d3b2377e4569b2404b240a4fb5c2",
      "abf5a99a05fa4ee88b15ef35803b9c2e",
      "deb3f0f1bb87431e89b33194e82c35da",
      "1c94ecec173146e6bce1edb03f174e7b",
      "0143ec9c89e345738b76f8a733f4f93a",
      "3a007c5b1abb4243bbb650baa5d69a86",
      "f38498fcc37e45d592d28e5d554269ba",
      "1e732456bfa9438182c68673d39e04e0",
      "8cc82f07ac3741459da535bee6cf55a3",
      "56ea5f6cd9234aa2891a3b8d428a70f3",
      "d14182a1535a43d4a34e51ff45deec42",
      "8b8d339f863543aba4652c934e03b144",
      "b3bae6e72448409886a95135a00cec32",
      "73ad0994be8447c183f4af32d25fcfa1",
      "f42aa4013faa46b29cf18b179e7d3b24",
      "f7e5486375a34a45bf1875218d5dca8a",
      "7a2e0da72e0f4e38a7cb65182ca679b8",
      "bbf356a23b9a45698dcfb3001a5295f3",
      "9a1463b70135451f87ce43edd1c69a54",
      "ce7d20edda12402cab1eefe9644ab397",
      "d72896c9b5224795bbc03bb430d1bf66",
      "28ba2fbf1e784c8bb04c2795e2215cdd",
      "a807406e002d4bb783dffb1498c1fee2",
      "e0b783311cb74df694434127494a8172",
      "28fd5253be194cd4b43509f8f20cd98b",
      "b0cda68f9c874534b6746041c303563f",
      "f0c5978e42e74f6186888ec6f3ed0772",
      "73e8b192fb234a60a41161ac6bfdf971",
      "b8eb435df37b47a9b2ef613a4ca6f3a9",
      "d31293713d0d4638a7c46e3ea995f848",
      "ecf3072760554ca8a9833454ab7bfacf",
      "8340cd023f84441fb37960e66bb9810b",
      "69b1cd795af040a590f06fbfe81e75d2",
      "039acb1152164c1c84d2180df3890a91",
      "7e7662e3ae02496ead7837541db8ca8a",
      "ec755d08bed14999a60cab3d55842ede",
      "94a65d6ab1134723807d024bb3ac2b64",
      "6c85301acc994abbb720a90bccb911e3",
      "3b64cf918b73411489b0ca024c2110cc",
      "c4b1716fe5084ed69b59edfcf1ea9c01",
      "13d908b025c24f2b922136a9c57eb81f",
      "fc094154ff4a410a8d18098c7b9a24c3",
      "abd5fe35bee34f1a938f19babb52ea82",
      "7f7b32b2fb274fca80b2c13cca18c935",
      "8d1595773def4d84bffc3c574b32f246",
      "40dd19c5748e45a89af660afd344fdf9",
      "20086288b12f4c8687a845fe506d69cb",
      "079e2d42a4974853a68d9028f34239d8",
      "79405eb439d040db8a469906d85a3ef7",
      "81a5c1d3e7fc437682715f833705e8ad",
      "b28ca6ea5c06407a8dd801ae6aeab5df",
      "158412d969fb44d1bfe507315d39eb02",
      "8b6dfe99931440f8bc00ea993ae77e19",
      "fab9071500b040dc8235d040862b49cb",
      "c0f4bdd6e6e04fc19f33e2ed03e83ce7",
      "048656be3d914950aa7eb8f72fc46490",
      "c39a25168cb840778ac2132226ea1c2e",
      "be92601ae6534d13bb1ecb75a40b739b",
      "95290639bfac4b02a4d31dcf33c8cc7c",
      "af2872dbef174d8c9262869c04d21aaf",
      "302b31ae249640e798f51ca8c9145f0c",
      "d3f751a1d9574e5790ff1e90fdaec69d",
      "415e35ed3ea846868efddba5e2f777ef",
      "2dc039ad1a1147c6b854a2b4429c2fc8",
      "0bf703def7774fc1ae494fed6b07dc18",
      "4c16f1838d124f94bec38b37583880a0",
      "c7c7b26c91b34cc09968c733fbb8b458",
      "261a92c84dfa430daa0d79724cc733e3",
      "6ae3385f544f461eba4ff37e5fe682d0",
      "331a506f896a4ea994eeb055015d1ba2",
      "013eff89837e407b83b6a48610e151aa",
      "b06d899cfbd94d42ab1c63d4879ded2a",
      "b36eee4033d849a188796626ac4680b8",
      "d4fe4fa9e3954e5181ee3ae5c3e4d9e6",
      "4d89c8b5f9294581a182da5dc417134a",
      "e29a3195d52f456b9825d45376746dc8",
      "1af30249634041eb95724bbb150faeb6",
      "9bdaeb8303a24c2fb976f150e56d73d2",
      "908d2eb4e4ac4ba2b2fc35d3e48e28ed",
      "d241155d47094462a585644e72053ce8",
      "5ee8614511c14d1cb84b542d5e48959a",
      "d20368aa539d489fbca954739d71663c",
      "0ec51d38ba8c4bc5a6542f2e3f31a0ac",
      "c726855055244b7f958776925ce29998",
      "1dae591c37334cff896827f752ed82b6",
      "2d3d2c6335b74da6adafbe5088cd01b2",
      "9d10209df80a4112a6605cb8ae6e5c6b",
      "5d9acd0190014962b04bb4924310efc5",
      "37332e6adaff4bb5a8820954d68455f2",
      "22507414e1904aba8b17e418929b0215",
      "becc4350bc1f45e4aa40aa8cefba896e",
      "29d99c8647c74e6ca01af80c7bd68fb9",
      "275e5155124448778e6393c6f871359c",
      "f383d5f771c5478793cc6b4661a898f2",
      "02e618b289144b869e3bf32a15b9d253",
      "ead12077e9db4b7fa5216a88a711228a",
      "273966a87d5f4fb58cfaa72f6eac5a71",
      "971369ad3a8243ae9d588e7e6fca1aef",
      "aa8e611f8a834f6bbd0363419f888736",
      "0851a8a10f2c4a4f9bf26e9c6b38fa69",
      "299ef1278a8f4f2c8e7c71a64a4da6ca",
      "bc8c84956d494bc78f90efc81f7e5fbf",
      "20547f9c281b4a34adb09fb6cbe0ed79",
      "ac3e7fb75c444d178b531848f037ed08",
      "75649d053fe04366b628d43c8ed645e1",
      "b1d63f8845b443f18caa27ff46c1486d",
      "f301618d7f414e00ba1a05b843aa0dac",
      "66b63efe69ad4bb0ad4cffc3ff0e578b",
      "1b46bd698f02405c85e11266f53e6bda",
      "c70a3f23feaf432ea6b34257234c156b",
      "c11b4436eedb4dc3a06c82540b1261df",
      "205e3ffc28f541acaa3b013e41665614",
      "3540f319e5d34880b4f72423fc2d5687",
      "afa7706c79c441d09aa61c59ce1f916c"
     ]
    },
    "id": "3KO2nPPJRrq1",
    "outputId": "79f4191d-651e-4e7e-e457-ace7dc12fb7f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9701ebca5c241c0a0c618395a1d1a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0143ec9c89e345738b76f8a733f4f93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e5486375a34a45bf1875218d5dca8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c5978e42e74f6186888ec6f3ed0772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c85301acc994abbb720a90bccb911e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79405eb439d040db8a469906d85a3ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2872dbef174d8c9262869c04d21aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013eff89837e407b83b6a48610e151aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20368aa539d489fbca954739d71663c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275e5155124448778e6393c6f871359c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3e7fb75c444d178b531848f037ed08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model=SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjWeTFgZR14p"
   },
   "outputs": [],
   "source": [
    "sentences=df['titles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "QWMA3jPMSdQl",
    "outputId": "e03249a8-adec-4fba-f884-dfa496441710"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multi-Level Attention Pooling for Graph Neural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Forests vs. Deep Networks: Conceptual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Power up! Robust Graph Convolutional Network v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Releasing Graph Neural Networks with Different...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Recurrence-Aware Long-Term Cognitive Network f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56176</th>\n",
       "      <td>Mining Spatio-temporal Data on Industrializati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56177</th>\n",
       "      <td>Wav2Letter: an End-to-End ConvNet-based Speech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56178</th>\n",
       "      <td>Deep Reinforcement Learning with Double Q-lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56179</th>\n",
       "      <td>Generalized Low Rank Models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56180</th>\n",
       "      <td>Chi-square Tests Driven Method for Learning th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56181 rows × 1 columns</p>\n",
       "</div><br><label><b>dtype:</b> object</label>"
      ],
      "text/plain": [
       "0        Multi-Level Attention Pooling for Graph Neural...\n",
       "1        Decision Forests vs. Deep Networks: Conceptual...\n",
       "2        Power up! Robust Graph Convolutional Network v...\n",
       "3        Releasing Graph Neural Networks with Different...\n",
       "4        Recurrence-Aware Long-Term Cognitive Network f...\n",
       "                               ...                        \n",
       "56176    Mining Spatio-temporal Data on Industrializati...\n",
       "56177    Wav2Letter: an End-to-End ConvNet-based Speech...\n",
       "56178    Deep Reinforcement Learning with Double Q-lear...\n",
       "56179                          Generalized Low Rank Models\n",
       "56180    Chi-square Tests Driven Method for Learning th...\n",
       "Name: titles, Length: 56181, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmc2x3F7SeeV"
   },
   "outputs": [],
   "source": [
    "embeddings=model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zo0QXIBCSkla",
    "outputId": "842e2613-0650-4981-8cd2-045d16ad066b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56181, 384)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3U1QCOs5X861",
    "outputId": "402d90e1-8a88-4a1d-dce8-963da20d2da9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities\n",
      "Embedding: 384\n",
      "\n",
      "Sentence: Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes\n",
      "Embedding: 384\n",
      "\n",
      "Sentence: Power up! Robust Graph Convolutional Network via Graph Powering\n",
      "Embedding: 384\n",
      "\n",
      "Sentence: Releasing Graph Neural Networks with Differential Privacy Guarantees\n",
      "Embedding: 384\n",
      "\n",
      "Sentence: Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification\n",
      "Embedding: 384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the embeddings\n",
    "c=0\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", len(embedding))\n",
    "    print(\"\")\n",
    "    c+=1\n",
    "    if c==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBXgsMfmZCTW"
   },
   "outputs": [],
   "source": [
    "#  Save files\n",
    "import pickle\n",
    "\n",
    "with open('models/embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7u7S1S8OZf3V"
   },
   "outputs": [],
   "source": [
    "with open('models/sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(sentences, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xrch1d3mZp2r"
   },
   "outputs": [],
   "source": [
    "with open('models/model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aaug2NFQZx5Y"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRBafaoVaefL"
   },
   "outputs": [],
   "source": [
    "def recommendation(input_paper):\n",
    "    # Calculate cosine similarity scores between the embeddings of input_paper and all papers in the dataset.\n",
    "    cosine_scores = util.cos_sim(embeddings, model.encode(input_paper))\n",
    "\n",
    "    # Get the indices of the top-k most similar papers based on cosine similarity.\n",
    "    top_similar_papers = torch.topk(cosine_scores, dim=0, k=20, sorted=True)\n",
    "\n",
    "    # Retrieve the titles of the top similar papers.\n",
    "    papers_set = set()\n",
    "    papers_list = []\n",
    "    for i in top_similar_papers.indices:\n",
    "        title = sentences[i.item()]\n",
    "        if title not in papers_set:\n",
    "            papers_list.append(title)\n",
    "            papers_set.add(title)\n",
    "        # if len(papers_list) == 5:  # Stop once we have 5 unique papers\n",
    "        #     break\n",
    "    return papers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fofT4TQVbBcp",
    "outputId": "411931b4-de6a-4f25-ed97-47d08ada14be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2157]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a='Attention is all you need'\n",
    "myemb=model.encode(a)\n",
    "util.cos_sim(embedding,myemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xaipcoPnbHEy",
    "outputId": "107b6156-d4bf-4575-b41e-1fd399f6bdbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter titleCNN Architectures\n",
      "Automated Architecture Design for Deep Neural Networks\n",
      "Analysis and Optimization of Convolutional Neural Network Architectures\n",
      "Analytical Characterization and Design Space Exploration for Optimization of CNNs\n",
      "Analyzing Modular CNN Architectures for Joint Depth Prediction and Semantic Segmentation\n",
      "Neural Architecture Optimization\n",
      "Fine-Grained Neural Architecture Search\n",
      "Explanatory Graphs for CNNs\n",
      "Finding the Needle in the Haystack with Convolutions: on the benefits of architectural bias\n",
      "Should You Go Deeper? Optimizing Convolutional Neural Network Architectures without Training by Receptive Field Analysis\n",
      "An Analysis of the Connections Between Layers of Deep Neural Networks\n",
      "Reversible Architectures for Arbitrarily Deep Residual Neural Networks\n",
      "A Survey of the Recent Architectures of Deep Convolutional Neural Networks\n",
      "Joint Neural Architecture Search and Quantization\n",
      "Automatically Designing CNN Architectures for Medical Image Segmentation\n",
      "Efficient Deep Neural Networks\n",
      "Neural Architecture Transfer\n"
     ]
    }
   ],
   "source": [
    "input_paper=input('Enter title')\n",
    "recommend_paper=recommendation(input_paper)\n",
    "\n",
    "for paper in recommend_paper:\n",
    "  print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M1iXcCXwcOYP",
    "outputId": "e9c1b4c0-0ca6-4226-d87e-9efe743e72f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "3.3.1\n",
      "2.17.1\n",
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import sentence_transformers\n",
    "import torch\n",
    "import tensorflow\n",
    "import numpy\n",
    "print(torch.__version__)\n",
    "print(sentence_transformers.__version__)\n",
    "print(tensorflow.__version__)\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAn8hU_TkFPz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "013eff89837e407b83b6a48610e151aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b06d899cfbd94d42ab1c63d4879ded2a",
       "IPY_MODEL_b36eee4033d849a188796626ac4680b8",
       "IPY_MODEL_d4fe4fa9e3954e5181ee3ae5c3e4d9e6"
      ],
      "layout": "IPY_MODEL_4d89c8b5f9294581a182da5dc417134a"
     }
    },
    "0143ec9c89e345738b76f8a733f4f93a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3a007c5b1abb4243bbb650baa5d69a86",
       "IPY_MODEL_f38498fcc37e45d592d28e5d554269ba",
       "IPY_MODEL_1e732456bfa9438182c68673d39e04e0"
      ],
      "layout": "IPY_MODEL_8cc82f07ac3741459da535bee6cf55a3"
     }
    },
    "02e618b289144b869e3bf32a15b9d253": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0851a8a10f2c4a4f9bf26e9c6b38fa69",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_299ef1278a8f4f2c8e7c71a64a4da6ca",
      "value": 112
     }
    },
    "02e6d43fdbdc4120be8ac47c2aae45eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_deb3f0f1bb87431e89b33194e82c35da",
      "placeholder": "​",
      "style": "IPY_MODEL_1c94ecec173146e6bce1edb03f174e7b",
      "value": " 349/349 [00:00&lt;00:00, 9.23kB/s]"
     }
    },
    "039acb1152164c1c84d2180df3890a91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "048656be3d914950aa7eb8f72fc46490": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "079e2d42a4974853a68d9028f34239d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0851a8a10f2c4a4f9bf26e9c6b38fa69": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0bf703def7774fc1ae494fed6b07dc18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ec51d38ba8c4bc5a6542f2e3f31a0ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d10209df80a4112a6605cb8ae6e5c6b",
      "placeholder": "​",
      "style": "IPY_MODEL_5d9acd0190014962b04bb4924310efc5",
      "value": "tokenizer.json: 100%"
     }
    },
    "13d908b025c24f2b922136a9c57eb81f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20086288b12f4c8687a845fe506d69cb",
      "placeholder": "​",
      "style": "IPY_MODEL_079e2d42a4974853a68d9028f34239d8",
      "value": " 612/612 [00:00&lt;00:00, 23.1kB/s]"
     }
    },
    "158412d969fb44d1bfe507315d39eb02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be92601ae6534d13bb1ecb75a40b739b",
      "placeholder": "​",
      "style": "IPY_MODEL_95290639bfac4b02a4d31dcf33c8cc7c",
      "value": " 90.9M/90.9M [00:00&lt;00:00, 209MB/s]"
     }
    },
    "1838545ac8764c088886c909ea69077a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1af30249634041eb95724bbb150faeb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1b46bd698f02405c85e11266f53e6bda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c94ecec173146e6bce1edb03f174e7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1dae591c37334cff896827f752ed82b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_becc4350bc1f45e4aa40aa8cefba896e",
      "placeholder": "​",
      "style": "IPY_MODEL_29d99c8647c74e6ca01af80c7bd68fb9",
      "value": " 466k/466k [00:00&lt;00:00, 1.04MB/s]"
     }
    },
    "1e732456bfa9438182c68673d39e04e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73ad0994be8447c183f4af32d25fcfa1",
      "placeholder": "​",
      "style": "IPY_MODEL_f42aa4013faa46b29cf18b179e7d3b24",
      "value": " 116/116 [00:00&lt;00:00, 5.17kB/s]"
     }
    },
    "20086288b12f4c8687a845fe506d69cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20547f9c281b4a34adb09fb6cbe0ed79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "205e3ffc28f541acaa3b013e41665614": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "22507414e1904aba8b17e418929b0215": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "261a92c84dfa430daa0d79724cc733e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "273966a87d5f4fb58cfaa72f6eac5a71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "275e5155124448778e6393c6f871359c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f383d5f771c5478793cc6b4661a898f2",
       "IPY_MODEL_02e618b289144b869e3bf32a15b9d253",
       "IPY_MODEL_ead12077e9db4b7fa5216a88a711228a"
      ],
      "layout": "IPY_MODEL_273966a87d5f4fb58cfaa72f6eac5a71"
     }
    },
    "28ba2fbf1e784c8bb04c2795e2215cdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "28fd5253be194cd4b43509f8f20cd98b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "299ef1278a8f4f2c8e7c71a64a4da6ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "29d99c8647c74e6ca01af80c7bd68fb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2d3d2c6335b74da6adafbe5088cd01b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2dc039ad1a1147c6b854a2b4429c2fc8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "302b31ae249640e798f51ca8c9145f0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0bf703def7774fc1ae494fed6b07dc18",
      "placeholder": "​",
      "style": "IPY_MODEL_4c16f1838d124f94bec38b37583880a0",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "331a506f896a4ea994eeb055015d1ba2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3540f319e5d34880b4f72423fc2d5687": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35a28e996912404d87d42948d80d0cbc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37332e6adaff4bb5a8820954d68455f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a007c5b1abb4243bbb650baa5d69a86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56ea5f6cd9234aa2891a3b8d428a70f3",
      "placeholder": "​",
      "style": "IPY_MODEL_d14182a1535a43d4a34e51ff45deec42",
      "value": "config_sentence_transformers.json: 100%"
     }
    },
    "3b64cf918b73411489b0ca024c2110cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_abd5fe35bee34f1a938f19babb52ea82",
      "placeholder": "​",
      "style": "IPY_MODEL_7f7b32b2fb274fca80b2c13cca18c935",
      "value": "config.json: 100%"
     }
    },
    "40dd19c5748e45a89af660afd344fdf9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "415e35ed3ea846868efddba5e2f777ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ae3385f544f461eba4ff37e5fe682d0",
      "placeholder": "​",
      "style": "IPY_MODEL_331a506f896a4ea994eeb055015d1ba2",
      "value": " 350/350 [00:00&lt;00:00, 17.9kB/s]"
     }
    },
    "4c16f1838d124f94bec38b37583880a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4d89c8b5f9294581a182da5dc417134a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56ea5f6cd9234aa2891a3b8d428a70f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d9acd0190014962b04bb4924310efc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5ee8614511c14d1cb84b542d5e48959a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5f99d3b2377e4569b2404b240a4fb5c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66b63efe69ad4bb0ad4cffc3ff0e578b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69b1cd795af040a590f06fbfe81e75d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6ae3385f544f461eba4ff37e5fe682d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c85301acc994abbb720a90bccb911e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3b64cf918b73411489b0ca024c2110cc",
       "IPY_MODEL_c4b1716fe5084ed69b59edfcf1ea9c01",
       "IPY_MODEL_13d908b025c24f2b922136a9c57eb81f"
      ],
      "layout": "IPY_MODEL_fc094154ff4a410a8d18098c7b9a24c3"
     }
    },
    "73ad0994be8447c183f4af32d25fcfa1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73e8b192fb234a60a41161ac6bfdf971": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8340cd023f84441fb37960e66bb9810b",
      "placeholder": "​",
      "style": "IPY_MODEL_69b1cd795af040a590f06fbfe81e75d2",
      "value": "sentence_bert_config.json: 100%"
     }
    },
    "75649d053fe04366b628d43c8ed645e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b46bd698f02405c85e11266f53e6bda",
      "placeholder": "​",
      "style": "IPY_MODEL_c70a3f23feaf432ea6b34257234c156b",
      "value": "1_Pooling/config.json: 100%"
     }
    },
    "79405eb439d040db8a469906d85a3ef7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_81a5c1d3e7fc437682715f833705e8ad",
       "IPY_MODEL_b28ca6ea5c06407a8dd801ae6aeab5df",
       "IPY_MODEL_158412d969fb44d1bfe507315d39eb02"
      ],
      "layout": "IPY_MODEL_8b6dfe99931440f8bc00ea993ae77e19"
     }
    },
    "7a2e0da72e0f4e38a7cb65182ca679b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d72896c9b5224795bbc03bb430d1bf66",
      "placeholder": "​",
      "style": "IPY_MODEL_28ba2fbf1e784c8bb04c2795e2215cdd",
      "value": "README.md: 100%"
     }
    },
    "7e7662e3ae02496ead7837541db8ca8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7f7b32b2fb274fca80b2c13cca18c935": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "80921d0fa7bf44f9bd1b4d57ab4ab6ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81a5c1d3e7fc437682715f833705e8ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fab9071500b040dc8235d040862b49cb",
      "placeholder": "​",
      "style": "IPY_MODEL_c0f4bdd6e6e04fc19f33e2ed03e83ce7",
      "value": "model.safetensors: 100%"
     }
    },
    "8340cd023f84441fb37960e66bb9810b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b6dfe99931440f8bc00ea993ae77e19": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b8d339f863543aba4652c934e03b144": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8cc82f07ac3741459da535bee6cf55a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d1595773def4d84bffc3c574b32f246": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "908d2eb4e4ac4ba2b2fc35d3e48e28ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "94a65d6ab1134723807d024bb3ac2b64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "95290639bfac4b02a4d31dcf33c8cc7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "971369ad3a8243ae9d588e7e6fca1aef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a1463b70135451f87ce43edd1c69a54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28fd5253be194cd4b43509f8f20cd98b",
      "placeholder": "​",
      "style": "IPY_MODEL_b0cda68f9c874534b6746041c303563f",
      "value": " 10.7k/10.7k [00:00&lt;00:00, 677kB/s]"
     }
    },
    "9bdaeb8303a24c2fb976f150e56d73d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d10209df80a4112a6605cb8ae6e5c6b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ddebab4f2404215a1d0c9200b972c59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_80921d0fa7bf44f9bd1b4d57ab4ab6ac",
      "placeholder": "​",
      "style": "IPY_MODEL_1838545ac8764c088886c909ea69077a",
      "value": "modules.json: 100%"
     }
    },
    "a807406e002d4bb783dffb1498c1fee2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa8e611f8a834f6bbd0363419f888736": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "abd5fe35bee34f1a938f19babb52ea82": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abf5a99a05fa4ee88b15ef35803b9c2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ac3e7fb75c444d178b531848f037ed08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_75649d053fe04366b628d43c8ed645e1",
       "IPY_MODEL_b1d63f8845b443f18caa27ff46c1486d",
       "IPY_MODEL_f301618d7f414e00ba1a05b843aa0dac"
      ],
      "layout": "IPY_MODEL_66b63efe69ad4bb0ad4cffc3ff0e578b"
     }
    },
    "af2872dbef174d8c9262869c04d21aaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_302b31ae249640e798f51ca8c9145f0c",
       "IPY_MODEL_d3f751a1d9574e5790ff1e90fdaec69d",
       "IPY_MODEL_415e35ed3ea846868efddba5e2f777ef"
      ],
      "layout": "IPY_MODEL_2dc039ad1a1147c6b854a2b4429c2fc8"
     }
    },
    "afa7706c79c441d09aa61c59ce1f916c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b06d899cfbd94d42ab1c63d4879ded2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e29a3195d52f456b9825d45376746dc8",
      "placeholder": "​",
      "style": "IPY_MODEL_1af30249634041eb95724bbb150faeb6",
      "value": "vocab.txt: 100%"
     }
    },
    "b0cda68f9c874534b6746041c303563f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1d63f8845b443f18caa27ff46c1486d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c11b4436eedb4dc3a06c82540b1261df",
      "max": 190,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_205e3ffc28f541acaa3b013e41665614",
      "value": 190
     }
    },
    "b28ca6ea5c06407a8dd801ae6aeab5df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_048656be3d914950aa7eb8f72fc46490",
      "max": 90868376,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c39a25168cb840778ac2132226ea1c2e",
      "value": 90868376
     }
    },
    "b36eee4033d849a188796626ac4680b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9bdaeb8303a24c2fb976f150e56d73d2",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_908d2eb4e4ac4ba2b2fc35d3e48e28ed",
      "value": 231508
     }
    },
    "b3bae6e72448409886a95135a00cec32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b8eb435df37b47a9b2ef613a4ca6f3a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_039acb1152164c1c84d2180df3890a91",
      "max": 53,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7e7662e3ae02496ead7837541db8ca8a",
      "value": 53
     }
    },
    "bbf356a23b9a45698dcfb3001a5295f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a807406e002d4bb783dffb1498c1fee2",
      "max": 10659,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e0b783311cb74df694434127494a8172",
      "value": 10659
     }
    },
    "bc8c84956d494bc78f90efc81f7e5fbf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be92601ae6534d13bb1ecb75a40b739b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "becc4350bc1f45e4aa40aa8cefba896e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0f4bdd6e6e04fc19f33e2ed03e83ce7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c11b4436eedb4dc3a06c82540b1261df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c39a25168cb840778ac2132226ea1c2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c4b1716fe5084ed69b59edfcf1ea9c01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d1595773def4d84bffc3c574b32f246",
      "max": 612,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_40dd19c5748e45a89af660afd344fdf9",
      "value": 612
     }
    },
    "c70a3f23feaf432ea6b34257234c156b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c713a95ce6cc4a9f9fe764830ef1c530": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f99d3b2377e4569b2404b240a4fb5c2",
      "max": 349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_abf5a99a05fa4ee88b15ef35803b9c2e",
      "value": 349
     }
    },
    "c726855055244b7f958776925ce29998": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37332e6adaff4bb5a8820954d68455f2",
      "max": 466247,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_22507414e1904aba8b17e418929b0215",
      "value": 466247
     }
    },
    "c7c7b26c91b34cc09968c733fbb8b458": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce7d20edda12402cab1eefe9644ab397": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d14182a1535a43d4a34e51ff45deec42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d20368aa539d489fbca954739d71663c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0ec51d38ba8c4bc5a6542f2e3f31a0ac",
       "IPY_MODEL_c726855055244b7f958776925ce29998",
       "IPY_MODEL_1dae591c37334cff896827f752ed82b6"
      ],
      "layout": "IPY_MODEL_2d3d2c6335b74da6adafbe5088cd01b2"
     }
    },
    "d241155d47094462a585644e72053ce8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d31293713d0d4638a7c46e3ea995f848": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec755d08bed14999a60cab3d55842ede",
      "placeholder": "​",
      "style": "IPY_MODEL_94a65d6ab1134723807d024bb3ac2b64",
      "value": " 53.0/53.0 [00:00&lt;00:00, 3.41kB/s]"
     }
    },
    "d3f751a1d9574e5790ff1e90fdaec69d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7c7b26c91b34cc09968c733fbb8b458",
      "max": 350,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_261a92c84dfa430daa0d79724cc733e3",
      "value": 350
     }
    },
    "d4fe4fa9e3954e5181ee3ae5c3e4d9e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d241155d47094462a585644e72053ce8",
      "placeholder": "​",
      "style": "IPY_MODEL_5ee8614511c14d1cb84b542d5e48959a",
      "value": " 232k/232k [00:00&lt;00:00, 8.33MB/s]"
     }
    },
    "d72896c9b5224795bbc03bb430d1bf66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9701ebca5c241c0a0c618395a1d1a26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ddebab4f2404215a1d0c9200b972c59",
       "IPY_MODEL_c713a95ce6cc4a9f9fe764830ef1c530",
       "IPY_MODEL_02e6d43fdbdc4120be8ac47c2aae45eb"
      ],
      "layout": "IPY_MODEL_35a28e996912404d87d42948d80d0cbc"
     }
    },
    "deb3f0f1bb87431e89b33194e82c35da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0b783311cb74df694434127494a8172": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e29a3195d52f456b9825d45376746dc8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ead12077e9db4b7fa5216a88a711228a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc8c84956d494bc78f90efc81f7e5fbf",
      "placeholder": "​",
      "style": "IPY_MODEL_20547f9c281b4a34adb09fb6cbe0ed79",
      "value": " 112/112 [00:00&lt;00:00, 6.15kB/s]"
     }
    },
    "ec755d08bed14999a60cab3d55842ede": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ecf3072760554ca8a9833454ab7bfacf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0c5978e42e74f6186888ec6f3ed0772": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_73e8b192fb234a60a41161ac6bfdf971",
       "IPY_MODEL_b8eb435df37b47a9b2ef613a4ca6f3a9",
       "IPY_MODEL_d31293713d0d4638a7c46e3ea995f848"
      ],
      "layout": "IPY_MODEL_ecf3072760554ca8a9833454ab7bfacf"
     }
    },
    "f301618d7f414e00ba1a05b843aa0dac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3540f319e5d34880b4f72423fc2d5687",
      "placeholder": "​",
      "style": "IPY_MODEL_afa7706c79c441d09aa61c59ce1f916c",
      "value": " 190/190 [00:00&lt;00:00, 9.89kB/s]"
     }
    },
    "f383d5f771c5478793cc6b4661a898f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_971369ad3a8243ae9d588e7e6fca1aef",
      "placeholder": "​",
      "style": "IPY_MODEL_aa8e611f8a834f6bbd0363419f888736",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "f38498fcc37e45d592d28e5d554269ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b8d339f863543aba4652c934e03b144",
      "max": 116,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b3bae6e72448409886a95135a00cec32",
      "value": 116
     }
    },
    "f42aa4013faa46b29cf18b179e7d3b24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f7e5486375a34a45bf1875218d5dca8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7a2e0da72e0f4e38a7cb65182ca679b8",
       "IPY_MODEL_bbf356a23b9a45698dcfb3001a5295f3",
       "IPY_MODEL_9a1463b70135451f87ce43edd1c69a54"
      ],
      "layout": "IPY_MODEL_ce7d20edda12402cab1eefe9644ab397"
     }
    },
    "fab9071500b040dc8235d040862b49cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc094154ff4a410a8d18098c7b9a24c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
